[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ENVS543 Narratives",
    "section": "",
    "text": "Preface\nThis is a compilation of all the narrative and slide components that we covered in this course."
  },
  {
    "objectID": "welcome.html",
    "href": "welcome.html",
    "title": "1  Environmental Data Literacy",
    "section": "",
    "text": "2 Logistics"
  },
  {
    "objectID": "welcome.html#envs-543-environmental-data-literacy-fall-2023",
    "href": "welcome.html#envs-543-environmental-data-literacy-fall-2023",
    "title": "1  Environmental Data Literacy",
    "section": "1.1 ENVS 543: Environmental Data Literacy Fall 2023",
    "text": "1.1 ENVS 543: Environmental Data Literacy Fall 2023\n \n\nSemester course; 3 lecture hours. 3 credits. Enrollment is restricted to students with graduate standing or those with one course in statistics and permission of instructor. Develop quantitative skills for the visualization, manipulation, analysis, and communication of environmental ‘big data.’ This course focuses on spatial environmental data analysis, data interpretation, manipulation, & analysis of real-world sized data sets, and developing methods and practices that aid in scientific communication using the R statistical analysis environment."
  },
  {
    "objectID": "welcome.html#sources-of-truth",
    "href": "welcome.html#sources-of-truth",
    "title": "1  Environmental Data Literacy",
    "section": "2.1 Sources of Truth",
    "text": "2.1 Sources of Truth\n \nCanvas will be used as the official grade repository as well as where I post announcements about course content. - Communication with be through your @vcu.edu email address."
  },
  {
    "objectID": "welcome.html#philosophy-of-backwards-design",
    "href": "welcome.html#philosophy-of-backwards-design",
    "title": "1  Environmental Data Literacy",
    "section": "2.2 Philosophy of Backwards Design",
    "text": "2.2 Philosophy of Backwards Design"
  },
  {
    "objectID": "welcome.html#stage-1-identify-desired-results",
    "href": "welcome.html#stage-1-identify-desired-results",
    "title": "1  Environmental Data Literacy",
    "section": "2.3 Stage 1: Identify Desired Results",
    "text": "2.3 Stage 1: Identify Desired Results\n \n\nDetermining the overarching learning goals for graduate-level classes\nWriting clear and measurable learning outcomes\nEnsuring alignment with program objectives and standards"
  },
  {
    "objectID": "welcome.html#stage-2-determine-acceptable-evidence",
    "href": "welcome.html#stage-2-determine-acceptable-evidence",
    "title": "1  Environmental Data Literacy",
    "section": "2.4 Stage 2: Determine Acceptable Evidence",
    "text": "2.4 Stage 2: Determine Acceptable Evidence\n \n\nSelecting appropriate assessment methods for graduate-level courses\nDesigning authentic assessments that demonstrate mastery of advanced concepts\nIncorporating formative and summative assessments"
  },
  {
    "objectID": "welcome.html#stage-3-plan-learning-experiences-and-instruction",
    "href": "welcome.html#stage-3-plan-learning-experiences-and-instruction",
    "title": "1  Environmental Data Literacy",
    "section": "2.5 Stage 3: Plan Learning Experiences and Instruction",
    "text": "2.5 Stage 3: Plan Learning Experiences and Instruction\n \n\nDesigning engaging and meaningful learning experiences for graduate students\nIdentifying instructional strategies that promote deep learning\nIncorporating technology and resources to enhance instruction"
  },
  {
    "objectID": "welcome.html#stage-4-implement-and-reflect",
    "href": "welcome.html#stage-4-implement-and-reflect",
    "title": "1  Environmental Data Literacy",
    "section": "2.6 Stage 4: Implement and Reflect",
    "text": "2.6 Stage 4: Implement and Reflect\n \n\nCollecting and analyzing data to inform instructional decisions\nReflecting on the effectiveness of the Backwards Design process\nEvaluate and promote additive content for the participants"
  },
  {
    "objectID": "welcome.html#mapping-content-context",
    "href": "welcome.html#mapping-content-context",
    "title": "1  Environmental Data Literacy",
    "section": "2.7 Mapping Content Context",
    "text": "2.7 Mapping Content Context"
  },
  {
    "objectID": "welcome.html#importance-of-data-literacy",
    "href": "welcome.html#importance-of-data-literacy",
    "title": "1  Environmental Data Literacy",
    "section": "2.8 Importance of Data Literacy",
    "text": "2.8 Importance of Data Literacy\n \n\n\n\nGIF with a hard G"
  },
  {
    "objectID": "welcome.html#workflow-in-data-analysis",
    "href": "welcome.html#workflow-in-data-analysis",
    "title": "1  Environmental Data Literacy",
    "section": "2.9 Workflow in Data Analysis",
    "text": "2.9 Workflow in Data Analysis\n\n\nCommon Workflow:\n\nCollect\n\nVisualize\nTransform\nModel\n\nCommunicate"
  },
  {
    "objectID": "welcome.html#student-learning-objectives",
    "href": "welcome.html#student-learning-objectives",
    "title": "1  Environmental Data Literacy",
    "section": "2.10 Student Learning Objectives",
    "text": "2.10 Student Learning Objectives\n\nStudent Learning Objectives (SLO) are statements that directly specify what participants will know, be able to do, or be able to demonstrate when they have completed or participated in the specific learning activity.\n\n\nThese define the active verbs of your learning experiences. At the end of a course, you should be able to implement or demonstrate mastery from each SLO."
  },
  {
    "objectID": "welcome.html#evs543-student-learning-objectives",
    "href": "welcome.html#evs543-student-learning-objectives",
    "title": "1  Environmental Data Literacy",
    "section": "2.11 EVS543 Student Learning Objectives",
    "text": "2.11 EVS543 Student Learning Objectives\nThese all map onto MS/MEnvs Program-Learning Outcomes.\n\nSLO 1: Identity, manipulate, and summarize numerical, categorical, ordinal, logical, date, string, and spatial data types.\nSLO 2: Create habits and took the knowledge to support reproducible research.\n\nSLO 3: Create an informative and effective graphical display of various data types of suitable quality for inclusion in published manuscripts.\nSLO 4: Effectively choose appropriate statistical models based on the types of data at hand and the questions being addressed.\nSLO 5: Demonstrate a general understanding of spatial data types and the creation of both appropriate static and dynamic maps."
  },
  {
    "objectID": "welcome.html#content-part-1-analytical-background",
    "href": "welcome.html#content-part-1-analytical-background",
    "title": "1  Environmental Data Literacy",
    "section": "3.1 Content Part 1: Analytical Background",
    "text": "3.1 Content Part 1: Analytical Background\n\n\n\n\n\n\n\n\nDeliverable\nDetails\nSLO\n\n\n\n\nWelcome & Logistics\nSetting up the computational environment for the class.\nNA\n\n\nGit, Github & Markdown\nEstablish a functional working knowledge of git, github, and begin learning Markdown\n2\n\n\nData Types & Containers\nUnderstanding the fundamental grammar and objects in R.\n1,2\n\n\nTidyverse\nData manipulation. Like a boss.\n1, 2\n\n\nGraphics that DON’T suck\nHello publication quality graphics, using the grammar of graphics approach\n2,3\n\n\nAI & Data Analytics\nLeveraging large language models to aid in scientific communication\n1,2,3,4"
  },
  {
    "objectID": "welcome.html#content-part-2-statistical-inferences",
    "href": "welcome.html#content-part-2-statistical-inferences",
    "title": "1  Environmental Data Literacy",
    "section": "3.2 Content Part 2: Statistical Inferences",
    "text": "3.2 Content Part 2: Statistical Inferences\n\n\n\n\n\n\n\n\n\nDeliverable\nDetails\nSLO\n\n\n\n\nStatistical Confidence\nBase understanding of statistical inferences and the properties of sampled data\n1,2,4\n\n\nBinomial Inferences\nAnalyses based upon counts and expectations.\n4\n\n\nCategorical~f(Categorical)\nContingency table and categorical count data\n4\n\n\nContinuous~f(Categorical)\nAnalysis of Variance (or equality of means)\n4\n\n\nContinuous~f(Continuous)\nCorrelation & Regression approaches\n4"
  },
  {
    "objectID": "welcome.html#topic-modularity",
    "href": "welcome.html#topic-modularity",
    "title": "1  Environmental Data Literacy",
    "section": "3.3 Topic Modularity",
    "text": "3.3 Topic Modularity\nEach of the topics listed are entirely self-contained (example). They will each have:\n\nStudent Learning Objectives relevant to the Topic\nPreparatory Content & Resources\nIn-Person Content & Resources\nAssessment Tools"
  },
  {
    "objectID": "welcome.html#prepatory-content-resources",
    "href": "welcome.html#prepatory-content-resources",
    "title": "1  Environmental Data Literacy",
    "section": "3.4 Prepatory Content & Resources",
    "text": "3.4 Prepatory Content & Resources\nThese items are intended to provide everyone a background foundation understanding of the topic and should be gone through prior to class where we are working on this topic.\n\nBackground Readings\nVideo Lecture\nSlides For Video\nData Sets\nBackground Assessment Quiz\nLonger Background Narrative"
  },
  {
    "objectID": "welcome.html#in-person-content-resources",
    "href": "welcome.html#in-person-content-resources",
    "title": "1  Environmental Data Literacy",
    "section": "3.5 In-Person Content & Resources",
    "text": "3.5 In-Person Content & Resources\nThis content is intended for in-person activities. In addition to presentation content, these items may include activities for individual work, group project work, reflection activities, and other assessment metrics.\n\nSlides\nIn Class\nAdditional External Resources"
  },
  {
    "objectID": "welcome.html#assessment-tools",
    "href": "welcome.html#assessment-tools",
    "title": "1  Environmental Data Literacy",
    "section": "3.6 Assessment Tools",
    "text": "3.6 Assessment Tools\nThe ability of participants to understand, practice, and demonstrate mastery of a topic can be evaluated using both direct and indirect methods. These will come in a variety of formats.\n\nHomework Assigmnent\nTopic Reflection"
  },
  {
    "objectID": "welcome.html#topic-duration",
    "href": "welcome.html#topic-duration",
    "title": "1  Environmental Data Literacy",
    "section": "3.7 Topic Duration",
    "text": "3.7 Topic Duration\nWhile each self-contained learning module is self-contained and will take as long as it takes to master, there are some limitations to how work is distributed and evaluated in this course.\n\n\n\nData analytics are never…\n\n\ndone in 50 minute increments\n\nperformed in isolation\ncompleted from memory\n\n\n\n\n\nAll evaluatory content will be due 1 week (7 days) later."
  },
  {
    "objectID": "welcome.html#grading-policy",
    "href": "welcome.html#grading-policy",
    "title": "1  Environmental Data Literacy",
    "section": "3.8 Grading Policy",
    "text": "3.8 Grading Policy\nThe grade for this course is based upon the totality of the points gained for all assignments as well as a single large data analysis project that will be due at the end of the semester. Grades will be determined using the normal 10% scale:\n\nA (&gt;= 90%),\n\nB (&gt;= 80% & &lt; 90%),\n\nC (&gt;= 70% & &lt; 80%),\n\nD (&gt;= 60% & &lt; 70%), and\n\nF (&lt; 60%)."
  },
  {
    "objectID": "welcome.html#section",
    "href": "welcome.html#section",
    "title": "1  Environmental Data Literacy",
    "section": "3.9 ",
    "text": "3.9 \n rjdyer@vcu.edu dyerlab.org @dyerlab"
  },
  {
    "objectID": "welcome.html#additional-information",
    "href": "welcome.html#additional-information",
    "title": "1  Environmental Data Literacy",
    "section": "3.10 Additional Information",
    "text": "3.10 Additional Information\n \nThe Office of the Provost has a list of additional information relevant for syllabi. Please go visit that page and look over it to familiarize yourself with University-wide regulations and guidelines."
  },
  {
    "objectID": "welcome.html#questions-on-logistics",
    "href": "welcome.html#questions-on-logistics",
    "title": "1  Environmental Data Literacy",
    "section": "3.11 Questions on Logistics?",
    "text": "3.11 Questions on Logistics?\n \nAny questions?1\nReturn to the course index page."
  },
  {
    "objectID": "welcome.html#footnotes",
    "href": "welcome.html#footnotes",
    "title": "1  Environmental Data Literacy",
    "section": "",
    "text": "Here are the next slides.↩︎"
  },
  {
    "objectID": "analysis_environment.html",
    "href": "analysis_environment.html",
    "title": "2  The R Environment",
    "section": "",
    "text": "3 Today’s Schedule\nReturn to the course index page."
  },
  {
    "objectID": "analysis_environment.html#what-is-this-class",
    "href": "analysis_environment.html#what-is-this-class",
    "title": "2  The R Environment",
    "section": "2.1 What is this class",
    "text": "2.1 What is this class\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nIt is not a pipe but oil paint on canvas.\nRené Magritte was a surrealist painter that encouraged the viewer to use their free mind to explore the logical shortcuts that human take when viewing items and instead to encourage alternative thinking about what is in front of you."
  },
  {
    "objectID": "analysis_environment.html#data-analysis",
    "href": "analysis_environment.html#data-analysis",
    "title": "2  The R Environment",
    "section": "2.2 Data Analysis",
    "text": "2.2 Data Analysis"
  },
  {
    "objectID": "analysis_environment.html#why-r",
    "href": "analysis_environment.html#why-r",
    "title": "2  The R Environment",
    "section": "2.3 Why R?",
    "text": "2.3 Why R?"
  },
  {
    "objectID": "analysis_environment.html#what-is-r",
    "href": "analysis_environment.html#what-is-r",
    "title": "2  The R Environment",
    "section": "2.4 What is R?",
    "text": "2.4 What is R?\nR is a language for data analysis and manipulation.\n\n\nBased upon S-Plus (Bell Laboratories).\nOpen Source.\nCollaborative.\nCommunity Supported."
  },
  {
    "objectID": "analysis_environment.html#challenges",
    "href": "analysis_environment.html#challenges",
    "title": "2  The R Environment",
    "section": "2.5 Challenges",
    "text": "2.5 Challenges\nSome of the challenges that I’ve seen individuals face when learning something like R.\n\n\nThis is not a point-and-click experience.\nYou will have to learn a grammar.\nCASE SENSITIVITY MATTERS!"
  },
  {
    "objectID": "analysis_environment.html#benefits",
    "href": "analysis_environment.html#benefits",
    "title": "2  The R Environment",
    "section": "2.6 Benefits",
    "text": "2.6 Benefits\nThere are several benefits to larning a general purpose analysis framework.\n\n\nNo vendor lock-in.\nData are data.\nTransferable skills to other disciplines/problems/issues."
  },
  {
    "objectID": "analysis_environment.html#and-ultimately",
    "href": "analysis_environment.html#and-ultimately",
    "title": "2  The R Environment",
    "section": "2.7 And Ultimately…",
    "text": "2.7 And Ultimately…"
  },
  {
    "objectID": "analysis_environment.html#content-live-here",
    "href": "analysis_environment.html#content-live-here",
    "title": "2  The R Environment",
    "section": "3.1 Content Live Here",
    "text": "3.1 Content Live Here\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nDon’t know GitHub? Start panicking! JK. We’ll get there together."
  },
  {
    "objectID": "analysis_environment.html#to-do-right-now",
    "href": "analysis_environment.html#to-do-right-now",
    "title": "2  The R Environment",
    "section": "4.1 To Do Right Now!",
    "text": "4.1 To Do Right Now!\n \nFirst things first! We need to get everyone configured to have the most up-to-date versions of the software and libraries we are going to start with on this class.\n\nInstall R onto your laptop (version 4.2+).\n \nIf you have already installed R on your computer, you need to update it to the latest version. Please see this blog post before you start installing stuff."
  },
  {
    "objectID": "analysis_environment.html#to-do-right-now-1",
    "href": "analysis_environment.html#to-do-right-now-1",
    "title": "2  The R Environment",
    "section": "4.2 To Do Right Now!",
    "text": "4.2 To Do Right Now!\n \nNext, let’s install an integrative developer environment (IDE).\nInstall RStudio—the “One True Interface” (2023.06.1+524 or later)."
  },
  {
    "objectID": "analysis_environment.html#yet-another-right-now-activity",
    "href": "analysis_environment.html#yet-another-right-now-activity",
    "title": "2  The R Environment",
    "section": "4.3 Yet Another “Right Now” activity",
    "text": "4.3 Yet Another “Right Now” activity\n \nInstall Quarto to help make dynamic documents, webpages, sites, and reports (1.0.38 or later)"
  },
  {
    "objectID": "analysis_environment.html#finally-right-now",
    "href": "analysis_environment.html#finally-right-now",
    "title": "2  The R Environment",
    "section": "4.4 Finally, “Right Now”",
    "text": "4.4 Finally, “Right Now”\n \nInstall Packages that we’ll start with. These are located in my teaching GitHub repository but can be accessed directly from within R. This will configure some basic packages for you to start with (we will come back to this later).\n \n\nsource(\"https://raw.githubusercontent.com/DyerlabTeaching/Welcome/main/StartingPackages.R\")"
  },
  {
    "objectID": "analysis_environment.html#getting-additional-integration",
    "href": "analysis_environment.html#getting-additional-integration",
    "title": "2  The R Environment",
    "section": "4.5 Getting Additional Integration",
    "text": "4.5 Getting Additional Integration\nLet’s let quarto install a few additional things on your machine. Switch over to the terminal in RStudio and type the following (for TeX)\nquarto install tool tinytex\n\nAnd if you do not have a version of Chrome on your laptop, also hit this one to aid in rendering images for pdf and docx files.\nquarto install tool chromium"
  },
  {
    "objectID": "markdown.html",
    "href": "markdown.html",
    "title": "3  Markdown",
    "section": "",
    "text": "4 Impetus\nIn current data analytics and communication, there are a wide variety of platforms on which we can provide summaries and insights regarding our work. Each of these end points requires a non-insignificant amount of effort to learn these systems. Moreover, they all are cul de sacs in that all the effort you exert to learn one will not allow you to get the benefits of any other platform than the one you just learned.\nEnter Pandoc, the universal document converter. Some really smart programmers have put together a set of software that allows you to convert from or two (and hence between) different document types given that most documents are regularly structured. With Pandoc, it does not matter if you do or do not have Word or Powerpoint or EPub or LaTeX or whatever, as long as you can create one of the supported types, you can convert that input into a huge variety of output types.\nThe first step is to go to quarto and download the quarto engine for your particular laptop.\nFor maximum usability, the document that we embed our code into should be as widely available as possible—unhindered by the necessity of having a particular program just to view the content. For this, R uses Markdown, created by John Gruber & Aaron Swartz in 2004. Markdown was created so that people are enabled “…to write using an easy-to-read and easy-to-write plain text format…”\nBecause everything is text, it is easy share and collaborate using Markdown, and for R, it is how we can make a wide array of output document types including (but not limited to):"
  },
  {
    "objectID": "markdown.html#pandoc-supported-conversions",
    "href": "markdown.html#pandoc-supported-conversions",
    "title": "3  Markdown",
    "section": "4.1 Pandoc Supported Conversions",
    "text": "4.1 Pandoc Supported Conversions\n\n\n\nConversion Formats\n\n\nThis is critical for us because Code is just text. Once it is evaluated, it can replaced with:\n\nNumerical values from one or more calculations,\nTextual content from analyses or manipulation, or\nGraphical content from plots.\n\nAs such, we can embed R code within raw text to create our analyses and documents."
  },
  {
    "objectID": "markdown.html#text-markup",
    "href": "markdown.html#text-markup",
    "title": "3  Markdown",
    "section": "6.1 Text Markup",
    "text": "6.1 Text Markup\nWhen we make a document, presentation, or any other output, there are only a finite set of different text components we can put into the document. The document itself does not need to be heavy or bloated, it is just text (though surprisingly, a blank Word document on my laptop with nothing in the document itself is still 12KB in size!). Common elements include:\n\nHeaders & Titles\nTypography such as italic, bold, underline, strike through\nLists (numbered or as bullets)\nPictures and links\nPage numbers, tables of contents, etc.\n\nWhat Markdown does is allows you to type these components and use ‘marking’ around the elements to make them different from regular text. It is really, amazingly simple.\nTitle and headers are created by prepending a hashtag\n# Header 1\n## Header 2\n### Header 3\n#### Header 4\nare knit into the following headers styles."
  },
  {
    "objectID": "markdown.html#header-2",
    "href": "markdown.html#header-2",
    "title": "3  Markdown",
    "section": "7.1 Header 2",
    "text": "7.1 Header 2\n\n7.1.1 Header 3\n\n7.1.1.1 Header 4\nThe actual appearance of the headers are determined by where it is being presented (e.g., in Word it will take the default typography and font attributes, etc.).\nIn text markdown examples are shown below and are contained within paragraphs of text. Individual paragraphs are delimited by either a blank line between them or two spaces at the end of the sentence.\n\n\n\nMarkdown\nRendered As\n\n\n\n\nplain text\nplain text\n\n\n*italic*\nitalic\n\n\n**bold**\nbold\n\n\n~~strike through~~\nstrike through\n\n\n\nYou can also embed links and images. Both of these are configured in two parts. For links, you need to specify the text that can be clicked upon and it must be surrounded in square brackets. The link to the web or file or image is right next to the square brackets and is contained within parentheses. The difference between link and image is that images have alternative text (or at times captions) and the whole thing has an exclamation mark in front of it. Here are some examples.\n\n\n\nMarkdown\nRendered As\n\n\n\n\n[CES](https://ces.vcu.edu)\nCES\n\n\n![Goat](https://bit.ly/3fRVlfl)\n\n\n\n\nLists (both numbered and unordered) are created using dashes or asterisks.\n\nBullet 1\n\nBullet 2\n\nBullet 3\n\nWill be turned into an unordered list as:\n\nBullet 1\n\nBullet 2\n\nBullet 3\n\nWhereas the following raw text.1\n1. First\n1. Second\n1. Third\nWill be rendered in list format as:\n\nFirst\n\nSecond\n\nThird\n\nActually, you can just use 1. in front of every line if you like, it will auto-number them for you when it makes a list. I tend to do this because it makes it a bit easier in case I want to reorder the list later and I don’t have to go back and change the numbers."
  },
  {
    "objectID": "markdown.html#code-text",
    "href": "markdown.html#code-text",
    "title": "3  Markdown",
    "section": "7.2 Code & Text",
    "text": "7.2 Code & Text\nOn of the strengths of RMarkdown is the ability to mix code and text together in one place. This allows us to bring all of our analyses and data as close to one another as possible, helping with reproducibility and error reduction.\n\n7.2.1 Inline Code\nYou can easily integrate code, into the text, either to be displayed OR to be evaluated. For example, in R you get the value of \\(\\pi\\) by the constant pi. Type that into the console and it will return 3.1415927.\nIf you look at the RMarkdown for that paragraph above, it looks like the following before knitting:\nYou can easily integrate code, into the text, either to be displayed *OR* to be evaluated.  For example, in `R` you get the value of $\\pi$ by the constant `pi`. Type that into the console and it will return 3.1415927.\nNotice the following parts:\n\nSymbols: The \\(\\pi\\) symbol is created by the name of the symbol surrounded by dollar signs. $ $. There are a ton of symbols and equations you can use, all borrowed from LaTeX, so if you need complicated equations or symbols, this is not a problem.\nText rendered as code (in typography) but not evaluated: Both the `R` and the `pi` are examples here. Nothing is evaluated, but it looks like code.\nEvaluated R Code: Any code between \\rand`will be evaluated as R code within the text. When you knit the document, it will be run and the contents between the`rand`are replaced by the output of theRcode. The example here was \\pi` at the end of the last sentence.\n\n\n\n7.2.2 Code Chunks\nIn addition to code within the text, RMarkdown supports code chunks, which can be one or many lines of raw R code. This code is executed and the results are merged into the markdown in the document (text, graphical, interactive widgets, whatever) before knitting.\nEach chunk is enclosed within boundary rows, the top row must contain three acute accents (back ticks - `) followed by the letter r in curly brackets ```{r}. The end of the chunk is indicated by three back ticks on their own line such as ```. Everything between these two enclosing lines is treated as R code and is subject to evaluation when you re-knit the document.\nHere is what a chunk looks like in markdown that prints out a simple message “This is text from a chunk.\n```{r}\nprint(“This is text from a chunk”)\n```\nWhen it is evaluated, the R interpreter removes the first and last rows, and executes the code within them. By default, the code is presented as a box in the output as well as any output that is produced from the code.\n\nprint(\"This is text from a chunk\")\n\n[1] \"This is text from a chunk\"\n\n\nThe first line in the chunk can also be used to modify the behavior of the code. There are several options that you can place within the curly brackets, including:\n\n{r eval=FALSE}: Will not evaluate (e.g., run) the code. The default value is TRUE.\n\n{r echo=FALSE}: Will not show the code in the document. This is great for our final version of our analyses, we want the output but not the code chunks showing. The default value is TRUE.\n\n{r message=FALSE, warning=FALSE, error=FALSE}: These suppress the messages that R prints out on occasion.\n\nSee the reference guide for several more options you can put into the header of each chunk."
  },
  {
    "objectID": "markdown.html#code-chunks-in-document",
    "href": "markdown.html#code-chunks-in-document",
    "title": "3  Markdown",
    "section": "7.3 Code Chunks in Document",
    "text": "7.3 Code Chunks in Document\nThere are some very fundamental issues regarding chunks, the R environment, and documents that should be pointed out here.\n\nThe R environment (see tab labeled Environment in the RStudio interface) has all the variables and new functions that you have created listed and available for use.\nAn R Markdown document is not a ‘living’ environment. If you make a change in a chunk, you must rerun that chunk to have the output available and inserted into the Environment. It does not do it automagically.\nWhen you knit a document, the only data it has is what is actually in the document itself. It does not look to the general Environment for variables and functions. This means that if you create a variable or load data using the Console and then reference it in the Document, it will fail when you try to knit the document.\nAll the code and variables in a document (if they are not within a chunk with eval=FALSE) is visible to everything in the document below where it was defined.\n\nChunks are evaluated from the top of the document to the bottom of the document.\n\nThe options for each chunk are available from the setup menu on the top right of the chunk itself (the gear icon). Additional options include a button to run all the chunks prior to this one as well as running this particular chunk (see image).\n\n\n\n\nOption buttons for each chunk include a quick menu for optoins (gear), the ability to run all the chunks above this one (triangle and line button in the middle), and run this particular chunk (play button).\n\n\nReturn to the course index page."
  },
  {
    "objectID": "markdown.html#footnotes",
    "href": "markdown.html#footnotes",
    "title": "3  Markdown",
    "section": "",
    "text": "This is a footnote and is defined by enclosing square brackets and a carat symbol (^) where you want to put the footnote in the text (e.g., [^1]) and then at the bottom of the document add the text (this part) prepended by [^1]:. The linking to the footnote and back to the place you put it will be automagically inserted.↩︎"
  },
  {
    "objectID": "quarto.html",
    "href": "quarto.html",
    "title": "4  Quarto",
    "section": "",
    "text": "5 How Quarto Works"
  },
  {
    "objectID": "quarto.html#so-what-is-quarto-really",
    "href": "quarto.html#so-what-is-quarto-really",
    "title": "4  Quarto",
    "section": "4.1 So what is Quarto really?",
    "text": "4.1 So what is Quarto really?\nQuarto is an open source scientific and technical publishing system makes the process of workign with data, analyzing it, and writing it up suck just a little bit less by streamlining how we create content for:\n\nHTML reports, blogs, and websites.\nPDF, RTF, & Word documents.\nPresentations (PowerPoint, Beamer, revealjs).\nTechnical Books & Journal Articles.\nDashboards.\nProjects."
  },
  {
    "objectID": "quarto.html#implementation-interface",
    "href": "quarto.html#implementation-interface",
    "title": "4  Quarto",
    "section": "5.1 Implementation != Interface",
    "text": "5.1 Implementation != Interface"
  },
  {
    "objectID": "quarto.html#language-agnostic-python-or-julia",
    "href": "quarto.html#language-agnostic-python-or-julia",
    "title": "4  Quarto",
    "section": "5.2 Language Agnostic (Python or Julia)",
    "text": "5.2 Language Agnostic (Python or Julia)"
  },
  {
    "objectID": "quarto.html#interface-agnostic",
    "href": "quarto.html#interface-agnostic",
    "title": "4  Quarto",
    "section": "5.3 Interface Agnostic",
    "text": "5.3 Interface Agnostic"
  },
  {
    "objectID": "quarto.html#downloading",
    "href": "quarto.html#downloading",
    "title": "4  Quarto",
    "section": "6.1 Downloading",
    "text": "6.1 Downloading\nQuarto is a document processing system and must be installed on your computer to be accessed from inside RStudio, VSCode, etc.\nhttps://quarto.org has the latest version of the executable. Visit the site and download the proper version for your computer.\n\n\n\n\n\n\nNote\n\n\n\nThe version used for this document was r system(\"quarto --version\", intern=TRUE)"
  },
  {
    "objectID": "quarto.html#quarto-document-creation-in-rstudio",
    "href": "quarto.html#quarto-document-creation-in-rstudio",
    "title": "4  Quarto",
    "section": "6.2 Quarto Document Creation in RStudio",
    "text": "6.2 Quarto Document Creation in RStudio"
  },
  {
    "objectID": "quarto.html#visual-vs.-source-editor",
    "href": "quarto.html#visual-vs.-source-editor",
    "title": "4  Quarto",
    "section": "6.3 Visual vs. Source Editor",
    "text": "6.3 Visual vs. Source Editor\nThe latest versions of RStudio have a source and visual editor for quarto documents."
  },
  {
    "objectID": "quarto.html#just-say-no-to-visual-editors",
    "href": "quarto.html#just-say-no-to-visual-editors",
    "title": "4  Quarto",
    "section": "6.4 “Just Say No” To Visual Editors",
    "text": "6.4 “Just Say No” To Visual Editors"
  },
  {
    "objectID": "quarto.html#rendering-a-quarto-document",
    "href": "quarto.html#rendering-a-quarto-document",
    "title": "4  Quarto",
    "section": "6.5 Rendering A Quarto Document",
    "text": "6.5 Rendering A Quarto Document\nTo create a document from your markdown, use the Render button in the toolbar.\n\n\nDepending on your configuration and the document type, the output will be displayed within either RStudio or in an external viewer (PDF, Word, etc.)."
  },
  {
    "objectID": "quarto.html#questions",
    "href": "quarto.html#questions",
    "title": "4  Quarto",
    "section": "6.6 Questions",
    "text": "6.6 Questions\n\n\n\nIf you have any questions, please feel free to either post them as an “Issue” on your copy of this GitHub Repository, post to the Canvas discussion board for the class, or drop me an email."
  },
  {
    "objectID": "github.html#look-familiar",
    "href": "github.html#look-familiar",
    "title": "5  Git & GitHub",
    "section": "5.1 Look Familiar?",
    "text": "5.1 Look Familiar?"
  },
  {
    "objectID": "github.html#what-is-git",
    "href": "github.html#what-is-git",
    "title": "5  Git & GitHub",
    "section": "5.2 What is Git",
    "text": "5.2 What is Git\nGit is a distributed versioning system\n\n\nEasy for collaboration.\n\nEasy track entire history of changes in your projects, documents, and code.\n\nComplete history of all changes.\nEasily revert to previous instances.\nProject Based.\nBranching"
  },
  {
    "objectID": "github.html#lifetime-of-a-document",
    "href": "github.html#lifetime-of-a-document",
    "title": "5  Git & GitHub",
    "section": "5.3 Lifetime of a Document",
    "text": "5.3 Lifetime of a Document\nConsider a single file. After the first edit, you now have two differents ones.\n\n\n\n\ngraph LR\n  A[File v1] --&gt;|Edited| B[File v2]\n\n\n\n\n\n\nIn Google Docs, you can see the version history (and revert) while in Word you can use track changes to see edits but revisions are more complicated…"
  },
  {
    "objectID": "github.html#persistence-through-diffs",
    "href": "github.html#persistence-through-diffs",
    "title": "5  Git & GitHub",
    "section": "5.4 Persistence through Diffs",
    "text": "5.4 Persistence through Diffs\nWhat we do not do is keep multiple versions of the file around as complete files. What is done is that the original document and only the changes made to the file—called diffs—are saved."
  },
  {
    "objectID": "github.html#lifetime-of-a-document-1",
    "href": "github.html#lifetime-of-a-document-1",
    "title": "5  Git & GitHub",
    "section": "5.5 Lifetime of a Document",
    "text": "5.5 Lifetime of a Document\nAdd a new figure to the file and now you have three different entities.\n\n\n\n\ngraph LR\n  A[File v1] --&gt;|Edited| B[File v2]\n  B --&gt;|New Figure| C[File v3]"
  },
  {
    "objectID": "github.html#lifetime-of-a-document-2",
    "href": "github.html#lifetime-of-a-document-2",
    "title": "5  Git & GitHub",
    "section": "5.6 Lifetime of a Document",
    "text": "5.6 Lifetime of a Document\nPerhaps the figure did not work out, and you decided to replace it with an image.\n\n\n\n\n\ngraph LR\n  A[File v1] --&gt;|Edited| B[File v2]\n  B --&gt;|Figure| C[File v3]\n  B --&gt;|Table| D[File v4]\n\n\n\n\n\nFor each new “revision”, you create a separate instance of the file itself."
  },
  {
    "objectID": "github.html#lifetime-of-a-document-3",
    "href": "github.html#lifetime-of-a-document-3",
    "title": "5  Git & GitHub",
    "section": "5.7 Lifetime of a Document",
    "text": "5.7 Lifetime of a Document\nContinue editing and making revisions.\n\n\n\n\n\ngraph LR\n  A[File v1] --&gt;|Edited| B[File v2]\n  B --&gt;|Figure| C[File v3]\n  B --&gt;|Table| D[File v4]\n  D --&gt;|Edited| E[File v5]"
  },
  {
    "objectID": "github.html#lifetime-of-a-document-4",
    "href": "github.html#lifetime-of-a-document-4",
    "title": "5  Git & GitHub",
    "section": "5.8 Lifetime of a Document",
    "text": "5.8 Lifetime of a Document\nEventually, you finish making revisions to the file.\n\n\n\n\n\ngraph LR\n  A[File v1] --&gt;|Edited| B[File v2]\n  B --&gt;|Figure| C[File v3]\n  B --&gt;|Table| D[File v4]\n  D --&gt;|Edited| E[File v5]\n  E --&gt;|Submitted| F[Thesis!]\n\n\n\n\n\n\nGit allows you to have access to the entire history of the file and can revert to any of the previous version you want to."
  },
  {
    "objectID": "github.html#lifetime-of-a-document-5",
    "href": "github.html#lifetime-of-a-document-5",
    "title": "5  Git & GitHub",
    "section": "5.9 Lifetime of a Document",
    "text": "5.9 Lifetime of a Document\nNot all documents are incremental. Submitting for publication requires a different layout, citation format, etc. than for your thesis. Both of these are derived from File v5 as different branches.\n\n\n\n\n\ngraph LR\n  A[File v1] --&gt;|Edited| B[File v2]\n  B --&gt;|Figure| C[File v3]\n  B --&gt;|Table| D[File v4]\n  D --&gt;|Edited| E[File v5]\n  E --&gt;|Submitted| F[Thesis!]\n  E --&gt;|Submitted| G[Publication]"
  },
  {
    "objectID": "github.html#git-in-projects",
    "href": "github.html#git-in-projects",
    "title": "5  Git & GitHub",
    "section": "5.10 Git in Projects",
    "text": "5.10 Git in Projects\nThis is a process that can include hundreds of files, data sets, images, shapefiles, etc. It is a Project Based approach.\nAnd the entire project is referred to as a Repository."
  },
  {
    "objectID": "github.html#remote-locations",
    "href": "github.html#remote-locations",
    "title": "5  Git & GitHub",
    "section": "5.11 Remote Locations",
    "text": "5.11 Remote Locations\nA git repository can live in many places.\n\n\nOn your laptop\n\nOn the lab computer\n\nOn a thumb drive (please do not use these any more)\n\nOn your home computer\nOn you iPhone"
  },
  {
    "objectID": "github.html#synchronizing-repositories",
    "href": "github.html#synchronizing-repositories",
    "title": "5  Git & GitHub",
    "section": "5.12 Synchronizing Repositories",
    "text": "5.12 Synchronizing Repositories\nHaving a single remote Repository allows you to keep all the remote repositories in sync.\n\n\nEstablish Repository on GitHub.\n\nClone that repository to your local computer.\nMake changes.\nCommit changes when you are at a logical stoping point.\nPUSH the one or more committed changes to GitHub.\nOn other computers with local copies, you PULL changes from GitHub to retrieve any changes you’ve made."
  },
  {
    "objectID": "github.html#pulling-pushing.",
    "href": "github.html#pulling-pushing.",
    "title": "5  Git & GitHub",
    "section": "5.13 Pulling & Pushing.",
    "text": "5.13 Pulling & Pushing.\nFor all these computers, you need to make sure you are pushing changes to GitHub from your current machine so that when you get to the next computer you can pull down these changes."
  },
  {
    "objectID": "github.html#collaborations",
    "href": "github.html#collaborations",
    "title": "5  Git & GitHub",
    "section": "5.14 Collaborations",
    "text": "5.14 Collaborations\nWith a GitHub repository, several people can contribute to the same project and have access to the same content.\n\n\nCan lead to conflicts\nMerging files"
  },
  {
    "objectID": "github.html#section",
    "href": "github.html#section",
    "title": "5  Git & GitHub",
    "section": "5.15 ",
    "text": "5.15"
  },
  {
    "objectID": "github.html#the-profile-page",
    "href": "github.html#the-profile-page",
    "title": "5  Git & GitHub",
    "section": "5.16 The Profile Page",
    "text": "5.16 The Profile Page"
  },
  {
    "objectID": "github.html#github-repositories",
    "href": "github.html#github-repositories",
    "title": "5  Git & GitHub",
    "section": "5.17 Github Repositories",
    "text": "5.17 Github Repositories\n\n\n\nhistory\n\nfiles\n\ndescription\n\nsettings\n\nvisibility\n\naccess"
  },
  {
    "objectID": "github.html#access-controls",
    "href": "github.html#access-controls",
    "title": "5  Git & GitHub",
    "section": "5.18 Access Controls",
    "text": "5.18 Access Controls\nGitHub has a fine granularity of control over who can access content and how much access they can have.\n\n\nUsername + Password: Allows you to log into GitHub.\nAccess Control: Dictates who can see and check out repositories.\nUsername + Personal Access Token: Allow you to push content back to GitHub."
  },
  {
    "objectID": "github.html#user-names-passwords",
    "href": "github.html#user-names-passwords",
    "title": "5  Git & GitHub",
    "section": "5.19 User Names & Passwords",
    "text": "5.19 User Names & Passwords\nIf you are not using a password manager, PLEASE START TODAY.\n\n1Password\nLastPass\nmCrypt"
  },
  {
    "objectID": "github.html#personal-access-tokens",
    "href": "github.html#personal-access-tokens",
    "title": "5  Git & GitHub",
    "section": "5.20 Personal Access Tokens",
    "text": "5.20 Personal Access Tokens\nSettings \\(\\to\\) Developer Settings \\(\\to\\) Personal Access Tokens"
  },
  {
    "objectID": "github.html#scoped-tokens",
    "href": "github.html#scoped-tokens",
    "title": "5  Git & GitHub",
    "section": "5.21 Scoped Tokens",
    "text": "5.21 Scoped Tokens\n\nSelect custom length and allow for enough time to not be a problem. Save long token securely."
  },
  {
    "objectID": "github.html#creating-a-repository",
    "href": "github.html#creating-a-repository",
    "title": "5  Git & GitHub",
    "section": "5.22 Creating a Repository",
    "text": "5.22 Creating a Repository\nFrom your profile page, select the Repositories menu and follow the instructions.\n- Names & Descriptions.\n- Private vs Public.\n- The README.md file as your ‘front page’.\n- The .gitignore file.\n- Licenses.\n- Websites."
  },
  {
    "objectID": "github.html#section-1",
    "href": "github.html#section-1",
    "title": "5  Git & GitHub",
    "section": "5.23 ",
    "text": "5.23"
  },
  {
    "objectID": "github.html#online-editing",
    "href": "github.html#online-editing",
    "title": "5  Git & GitHub",
    "section": "5.24 Online Editing",
    "text": "5.24 Online Editing\nThe repository interface allows:\n\nAdding and deleting of files.\n\nEditing file content.\nDeploying to websites.\nWiki/Comments/Feedback."
  },
  {
    "objectID": "github.html#going-forward",
    "href": "github.html#going-forward",
    "title": "5  Git & GitHub",
    "section": "5.25 Going Forward",
    "text": "5.25 Going Forward"
  },
  {
    "objectID": "github.html#questions",
    "href": "github.html#questions",
    "title": "5  Git & GitHub",
    "section": "5.26 Questions",
    "text": "5.26 Questions\n \n\n\n\nIf you have any questions, please feel free to either post to the Canvas discussion board for the class or drop me an email.\n\n\n\n\n\n\n\n\n\n\nReturn to course table of contents."
  },
  {
    "objectID": "data_types.html#missing-data",
    "href": "data_types.html#missing-data",
    "title": "6  Basic Data Types",
    "section": "6.1 Missing Data",
    "text": "6.1 Missing Data\n\nThe Absence of Data\n\nThe most fundamental type of data in R is data that does not exist! Missing data! It is represented as NA\n\nx &lt;- NA\n\nand can be in"
  },
  {
    "objectID": "data_types.html#numerical-data",
    "href": "data_types.html#numerical-data",
    "title": "6  Basic Data Types",
    "section": "6.2 Numerical Data",
    "text": "6.2 Numerical Data\n\nNumerical data contains all numerical represenations.\n\nBy far, the most common kind of data we use in our analyses is numerical data. This may represent measured things like height, snout-vent length (whatever that is), depth, age, etc. In data analysis, we commonly take (or obtain) measurements from several items and then try to characterize them using summaries and visualization.\nIn R, the numerical data type can be defined as:\n\nX &lt;- 42\n\nNotice how the numerical value of 42 is assigned to the variable named X. To have R print out the value of a particular variable, you can type its name in the console and it will give it to you.\n\nX\n\n[1] 42\n\n\n\n6.2.1 Operators\nNumeric types have a ton of normal operators that can be used. Some examples include:\nThe usual arithmatic operators:\n\nx &lt;- 10\ny &lt;- 23\n\nx + y\n\n[1] 33\n\nx - y\n\n[1] -13\n\nx * y\n\n[1] 230\n\nx / y\n\n[1] 0.4347826\n\n\nYou have the exponentials:\n\n## x raised to the y\nx^y\n\n[1] 1e+23\n\n## the inverse of an exponent is a root, here is the 23rd root of 10\nx^(1/y)\n\n[1] 1.105295\n\n\nThe logrithmics:\n\n## the natural log\nlog(x)\n\n[1] 2.302585\n\n## Base 10 log\nlog(x,base=10)\n\n[1] 1\n\n\nAnd the modulus operator:\n\ny %% x\n\n[1] 3\n\n\nIf you didn’t know what this one is, don’t worry. The modulus is just the remainder after division like you did in grade school. The above code means that 23 divided by 10 has a remainder of 3. I include it here just to highlight the fact that many of the operators that we will be working with in R are created by more than just a single symbol residing at the top row of your computer keyboard. There are just too few symbos on the normal keyboard to represent the breath of operators. The authors of R have decided that using combinations of symbols to handle these and you will get used to them in not time at all.\n\n\n6.2.2 Introspection & Coercion\nThe class() of a numeric type is (wait for it)… numeric (those R programmers are sure clever).\n\nclass( 42 )\n\n[1] \"numeric\"\n\n\n\nIn this case class is the name of the function and there are one or more things we pass to that function. These must be enclosed in the parenthesis associated with class. The parantheses must be right next to the name of the function. If you put a space betwen the word class and the parentheses, it may not work the way you would like it to. You’ve been warned.\nThe stuff inside the parenthesis are called arguments and are the data that we pass to the function itself. In this case we pass a value or varible to the class function and it does its magic and tells us what kind of data type it is. Many functions have several arguements that can be passed to them, some optional, some not. We will get more into that on the lecture covering Functions.\n\nIt is also possible to inquire if a particular variable is of a certain class. This is done by using the is.* set of functions.\n\nis.numeric( 42 )\n\n[1] TRUE\n\nis.numeric( \"dr dyer\" )\n\n[1] FALSE\n\n\nSometimes we may need to turn one kind of class into another kind. Consider the following:\n\nx &lt;- \"42\"\nis.numeric( x )\n\n[1] FALSE\n\nclass(x)\n\n[1] \"character\"\n\n\nIt is a character data type because it is enclosed within a set of quotes. However, we can coerce it into a numeric type by:\n\ny &lt;- as.numeric( x )\nis.numeric( y )\n\n[1] TRUE\n\ny\n\n[1] 42"
  },
  {
    "objectID": "data_types.html#character-data",
    "href": "data_types.html#character-data",
    "title": "6  Basic Data Types",
    "section": "6.3 Character Data",
    "text": "6.3 Character Data\n\nCharacter data represents textual content.\n\nThe data type character is intended to represent textual data such as actual texts, names of objects, and other contnet that is intended to help both you and the audience you are trying to reach better understand your data.\n\nname &lt;- \"Dyer\"\nsport &lt;- \"Frolf\"\n\nThe two variables above have a sequence of characters enclosed by a double quote. You can use a single quote instead, however the enclosing quoting characters must be the same (e.g., you cannot start with a single quote and end with a double).\n\n6.3.1 Lengths\nThe length of a string is a measure of how many varibles there are, not the number of characters within it. For example, the length of dyer is\n\nlength(name)\n\n[1] 1\n\n\nbecause it only has one character but the number of characters within it is:\n\nnchar(name)\n\n[1] 4\n\n\nLength is defined specifically on the number of elements in a vector, and technically the variable dyer is a vector of length one. If we concatinate them into a vector (go see the vector content)\n\nphrase &lt;- c( name, sport )\n\nwe find that it has a length of 2\n\nlength(phrase)\n\n[1] 2\n\n\nAnd if we ask the vector how many characters are in the elements it contains, it gives us a vector of numeric types representing the number of letters in each of the elements.\n\nnchar(phrase)\n\n[1] 4 5\n\n\n\n\n6.3.2 Putting Character Objects Together\nThe binary + operator has not been defined for objects of class character, which is understandable once we consider all the different ways we may want to put the values contained in the variables together. If you try it, R will complain.\n\nname + sport\n\nError in name + sport: non-numeric argument to binary operator\n\n\nThe paste() function is designed to take a collection of character variables and smush them togethers. By default, it inserts a space between each of the variables and/or values passed to it.\n\npaste( name, \"plays\", sport )\n\n[1] \"Dyer plays Frolf\"\n\n\nAlthough, you can have any kind of separator you like:\n\npaste(name, sport, sep=\" is no good at \")\n\n[1] \"Dyer is no good at Frolf\"\n\n\nThe elements you pass to paste() do not need to be held in variables, you can put quoted character values in there as well.\n\npaste( name, \" the \", sport, \"er\", sep=\"\") \n\n[1] \"Dyer the Frolfer\"\n\n\nIf you have a vector of character types, by default, it considers the pasting operation to be applied to every element of the vector.\n\npaste( phrase , \"!\")\n\n[1] \"Dyer !\"  \"Frolf !\"\n\n\nHowever if you intention is to take the elements of the vector and paste them together, then you need to specify that using the collapse optional argument. By default, it is set to NULL, and that state tells the function to apply the paste()-ing to each element. However, if you set collapse to something other than NULL, it will use that to take all the elements and put them into a single response.\n\npaste( phrase, collapse = \" is not good at \") \n\n[1] \"Dyer is not good at Frolf\"\n\n\n\n\n6.3.3 String Operations\nMany times, we need to extract components from within a longer character element. Here is a longer bit of text as an example.\n\ncorpus &lt;- \"An environmental impact statement (EIS), under United States environmental law, is a document required by the 1969 National Environmental Policy Act (NEPA) for certain actions 'significantly affecting the quality of the human environment'.[1] An EIS is a tool for decision making. It describes the positive and negative environmental effects of a proposed action, and it usually also lists one or more alternative actions that may be chosen instead of the action described in the EIS. Several U.S. state governments require that a document similar to an EIS be submitted to the state for certain actions. For example, in California, an Environmental Impact Report (EIR) must be submitted to the state for certain actions, as described in the California Environmental Quality Act (CEQA). One of the primary authors of the act is Lynton K. Caldwell.\"\n\n\n\n6.3.4 Splits\nWe can split the original string into several components by specifying which particular character or set of characters we wish to use to break it apart.\nAs we start working with increasingly more complicated string operations, I like to use a higher-level library (part of tidyverse) called stringr. If you do not have this library already installed, you can install it using install.packages(\"stringr\").\n\nlibrary( stringr )\n\nHere is an example using the space character to pull it apart into words.\n\nstr_split( corpus, pattern=\" \", simplify=TRUE)\n\n     [,1] [,2]            [,3]     [,4]        [,5]     [,6]    [,7]    \n[1,] \"An\" \"environmental\" \"impact\" \"statement\" \"(EIS),\" \"under\" \"United\"\n     [,8]     [,9]            [,10]  [,11] [,12] [,13]      [,14]      [,15]\n[1,] \"States\" \"environmental\" \"law,\" \"is\"  \"a\"   \"document\" \"required\" \"by\" \n     [,16] [,17]  [,18]      [,19]           [,20]    [,21] [,22]    [,23]\n[1,] \"the\" \"1969\" \"National\" \"Environmental\" \"Policy\" \"Act\" \"(NEPA)\" \"for\"\n     [,24]     [,25]     [,26]            [,27]       [,28] [,29]     [,30]\n[1,] \"certain\" \"actions\" \"'significantly\" \"affecting\" \"the\" \"quality\" \"of\" \n     [,31] [,32]   [,33]              [,34] [,35] [,36] [,37] [,38]  [,39]\n[1,] \"the\" \"human\" \"environment'.[1]\" \"An\"  \"EIS\" \"is\"  \"a\"   \"tool\" \"for\"\n     [,40]      [,41]     [,42] [,43]       [,44] [,45]      [,46] [,47]     \n[1,] \"decision\" \"making.\" \"It\"  \"describes\" \"the\" \"positive\" \"and\" \"negative\"\n     [,48]           [,49]     [,50] [,51] [,52]      [,53]     [,54] [,55]\n[1,] \"environmental\" \"effects\" \"of\"  \"a\"   \"proposed\" \"action,\" \"and\" \"it\" \n     [,56]     [,57]  [,58]   [,59] [,60] [,61]  [,62]         [,63]     [,64] \n[1,] \"usually\" \"also\" \"lists\" \"one\" \"or\"  \"more\" \"alternative\" \"actions\" \"that\"\n     [,65] [,66] [,67]    [,68]     [,69] [,70] [,71]    [,72]       [,73]\n[1,] \"may\" \"be\"  \"chosen\" \"instead\" \"of\"  \"the\" \"action\" \"described\" \"in\" \n     [,74] [,75]  [,76]     [,77]  [,78]   [,79]         [,80]     [,81]  [,82]\n[1,] \"the\" \"EIS.\" \"Several\" \"U.S.\" \"state\" \"governments\" \"require\" \"that\" \"a\"  \n     [,83]      [,84]     [,85] [,86] [,87] [,88] [,89]       [,90] [,91]\n[1,] \"document\" \"similar\" \"to\"  \"an\"  \"EIS\" \"be\"  \"submitted\" \"to\"  \"the\"\n     [,92]   [,93] [,94]     [,95]      [,96] [,97]      [,98] [,99]        \n[1,] \"state\" \"for\" \"certain\" \"actions.\" \"For\" \"example,\" \"in\"  \"California,\"\n     [,100] [,101]          [,102]   [,103]   [,104]  [,105] [,106] [,107]     \n[1,] \"an\"   \"Environmental\" \"Impact\" \"Report\" \"(EIR)\" \"must\" \"be\"   \"submitted\"\n     [,108] [,109] [,110]  [,111] [,112]    [,113]     [,114] [,115]     \n[1,] \"to\"   \"the\"  \"state\" \"for\"  \"certain\" \"actions,\" \"as\"   \"described\"\n     [,116] [,117] [,118]       [,119]          [,120]    [,121] [,122]   \n[1,] \"in\"   \"the\"  \"California\" \"Environmental\" \"Quality\" \"Act\"  \"(CEQA).\"\n     [,123] [,124] [,125] [,126]    [,127]    [,128] [,129] [,130] [,131]\n[1,] \"One\"  \"of\"   \"the\"  \"primary\" \"authors\" \"of\"   \"the\"  \"act\"  \"is\"  \n     [,132]   [,133] [,134]     \n[1,] \"Lynton\" \"K.\"   \"Caldwell.\"\n\n\nwhich shows 134 words in the text.\nI need to point out that I added the simplify=TRUE option to str_split. Had I not done that, it would have returned a list object that contained the individual vector of words. There are various reasons that it returns a list, none of which I can frankly understand, that is just the way the authors of the function made it.\n\n\n6.3.5 Substrings\nThere are two different things you may want to do with substrings; find them and replace them. Here are some ways to figure out where they are.\n\nstr_detect(corpus, \"Environment\")\n\n[1] TRUE\n\n\n\nstr_count( corpus, \"Environment\")\n\n[1] 3\n\n\n\nstr_locate_all( corpus, \"Environment\")\n\n[[1]]\n     start end\n[1,]   125 135\n[2,]   637 647\n[3,]   754 764\n\n\nWe can also replace instances of one substring with another.\n\nstr_replace_all(corpus, \"California\", \"Virginia\")\n\n[1] \"An environmental impact statement (EIS), under United States environmental law, is a document required by the 1969 National Environmental Policy Act (NEPA) for certain actions 'significantly affecting the quality of the human environment'.[1] An EIS is a tool for decision making. It describes the positive and negative environmental effects of a proposed action, and it usually also lists one or more alternative actions that may be chosen instead of the action described in the EIS. Several U.S. state governments require that a document similar to an EIS be submitted to the state for certain actions. For example, in Virginia, an Environmental Impact Report (EIR) must be submitted to the state for certain actions, as described in the Virginia Environmental Quality Act (CEQA). One of the primary authors of the act is Lynton K. Caldwell.\"\n\n\nThere is a lot more fun stuff to do with string based data."
  },
  {
    "objectID": "data_types.html#logical-data",
    "href": "data_types.html#logical-data",
    "title": "6  Basic Data Types",
    "section": "6.4 Logical Data",
    "text": "6.4 Logical Data\nLogical data consists of two mutually exclusive states: TRUE or FALSE\n \n\ndyer_has_good_jokes &lt;- TRUE\ndyer_has_good_jokes\n\n[1] TRUE\n\n\n\n6.4.1 Operators on Logical Types\nThere are 3 primary logical operators that can be used on logical types; one unary and two binary.\n \n\n6.4.1.1 Unary Operator\nThe negation operator\n\n!dyer_has_good_jokes\n\n[1] FALSE\n\n\n \n\n\n\n6.4.2 The Binary Operators\n\n6.4.2.1 The OR operator\n\nTRUE | FALSE\n\n[1] TRUE\n\n\n\n\n6.4.2.2 The AND operator\n\nTRUE & FALSE\n\n[1] FALSE\n\n\n\n\n\n6.4.3 Introspection\nLogical types have an introspection operator.\n \n\nis.logical( dyer_has_good_jokes )\n\n[1] TRUE\n\n\nCoercion of something else to a Logical is more case-specific.\nFrom character data.\n\nas.logical( \"TRUE\" )\n\n[1] TRUE\n\n\n \n\nas.logical( \"FALSE\" )\n\n[1] FALSE\n\n\nOther character types result in NA (missing data).\n\nas.logical( \"Bob\" )\n\n[1] NA\n\n\n\n\n6.4.4 Coercion\nCoercion of something else to a Logical is more case-specific.\n \nFrom numeric data:\n- Values of 0 are FALSE\n- Non-zero values are TRUE\n\nas.logical(0)\n\n[1] FALSE\n\n\n\nas.logical( 323 )\n\n[1] TRUE"
  },
  {
    "objectID": "data_types.html#dates",
    "href": "data_types.html#dates",
    "title": "6  Basic Data Types",
    "section": "6.5 Dates",
    "text": "6.5 Dates\n\nTime is the next dimension.\n\nThis topic covers the basics of how we put together data based upone date and time objects. For this, we will use the following data frame with a single column of data representing dates as they are written in the US.\nThese are several challenges associated with working with date and time objects. To those of us who are reading this with a background of how US time and date formats are read, we can easily interpret data objects as Month/Day/Year formats (e.g., “2/14/2018”), and is commonly represented in the kind of input data we work in R with as with a string of characters. Dates and times are sticky things in data analysis because they do not work the way we think they should. Here are some wrinkles:\n\nThere are many types of calendars, we use the Julian calendar. However, there are many other calendars that are in use that we may run into. Each of these calendars has a different starting year (e.g., in the Assyrian calendar it is year 6770, it is 4718 in the Chinese calendar, 2020 in the Gregorian, and 1442 in the Islamic calendar).\nWestern calendar has leap years (+1 day in February) as well as leap seconds because it is based on the rotation around the sun, others are based upon the lunar cycle and have other corrections.\nOn this planet, we have 24 different time zones. Some states (looking at you Arizona) don’t feel it necessary to follow the other states around so they may be the same as PST some of the year and the same as MST the rest of the year. The provence of Newfoundland decided to be half-way between time zones so they are GMT-2:30. Some states have more than one time zone even if they are not large in size (hello Indiana).\nDates and time are made up of odd units, 60-seconds a minute, 60-minutes an hour, 24-hours a day, 7-days a week, 2-weeks a fortnight, 28,29,30,or 31-days in a month, 365 or 366 days in a year, 100 years in a century, etc.\n\nFortunately, some smart programmers have figured this out for us already. What they did is made the second as the base unit of time and designated 00:00:00 on 1 January 1970 as the unix epoch. Time on most modern computers is measured from that starting point. It is much easier to measure the difference between two points in time using the seconds since unix epich and then translate it into one or more of these calendars than to deal with all the different calendars each time. So under the hood, much of the date and time issues are kept in terms of epoch seconds.\n\nunclass( Sys.time() )\n\n[1] 1701788349\n\n\n\n6.5.1 Basic Date Objects\nR has some basic date functionality built into it. One of the easiest says to get a date object created is to specify the a date as a character string and then coerce it into a data object. By default, this requires us to represent the date objects as “YEAR-MONTH-DAY” with padding 0 values for any integer of month or date below 9 (e.g., must be two-digits).\nSo for example, we can specify a date object as:\n\nclass_start &lt;- as.Date(\"2021-01-15\")\nclass_start\n\n[1] \"2021-01-15\"\n\n\nAnd it is of type:\n\nclass( class_start )\n\n[1] \"Date\"\n\n\nIf you want to make a the date from a different format, you need to specify what elements within the string representation using format codes. These codes (and many more) can be found by looking at ?strptime.\n\nclass_end &lt;- as.Date( \"5/10/21\", format = \"%m/%d/%y\")\nclass_end\n\n[1] \"2021-05-10\"\n\n\nI like to use some higher-level date functions from the lubridate library. If you don’t have it installed, do so using the normal approach.\n\nlibrary( lubridate )\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\n\nDate objects can be put into vectors and sequences just like other objects.\n\nsemester &lt;- seq( class_start, class_end, by = \"1 day\")\nsemester\n\n  [1] \"2021-01-15\" \"2021-01-16\" \"2021-01-17\" \"2021-01-18\" \"2021-01-19\"\n  [6] \"2021-01-20\" \"2021-01-21\" \"2021-01-22\" \"2021-01-23\" \"2021-01-24\"\n [11] \"2021-01-25\" \"2021-01-26\" \"2021-01-27\" \"2021-01-28\" \"2021-01-29\"\n [16] \"2021-01-30\" \"2021-01-31\" \"2021-02-01\" \"2021-02-02\" \"2021-02-03\"\n [21] \"2021-02-04\" \"2021-02-05\" \"2021-02-06\" \"2021-02-07\" \"2021-02-08\"\n [26] \"2021-02-09\" \"2021-02-10\" \"2021-02-11\" \"2021-02-12\" \"2021-02-13\"\n [31] \"2021-02-14\" \"2021-02-15\" \"2021-02-16\" \"2021-02-17\" \"2021-02-18\"\n [36] \"2021-02-19\" \"2021-02-20\" \"2021-02-21\" \"2021-02-22\" \"2021-02-23\"\n [41] \"2021-02-24\" \"2021-02-25\" \"2021-02-26\" \"2021-02-27\" \"2021-02-28\"\n [46] \"2021-03-01\" \"2021-03-02\" \"2021-03-03\" \"2021-03-04\" \"2021-03-05\"\n [51] \"2021-03-06\" \"2021-03-07\" \"2021-03-08\" \"2021-03-09\" \"2021-03-10\"\n [56] \"2021-03-11\" \"2021-03-12\" \"2021-03-13\" \"2021-03-14\" \"2021-03-15\"\n [61] \"2021-03-16\" \"2021-03-17\" \"2021-03-18\" \"2021-03-19\" \"2021-03-20\"\n [66] \"2021-03-21\" \"2021-03-22\" \"2021-03-23\" \"2021-03-24\" \"2021-03-25\"\n [71] \"2021-03-26\" \"2021-03-27\" \"2021-03-28\" \"2021-03-29\" \"2021-03-30\"\n [76] \"2021-03-31\" \"2021-04-01\" \"2021-04-02\" \"2021-04-03\" \"2021-04-04\"\n [81] \"2021-04-05\" \"2021-04-06\" \"2021-04-07\" \"2021-04-08\" \"2021-04-09\"\n [86] \"2021-04-10\" \"2021-04-11\" \"2021-04-12\" \"2021-04-13\" \"2021-04-14\"\n [91] \"2021-04-15\" \"2021-04-16\" \"2021-04-17\" \"2021-04-18\" \"2021-04-19\"\n [96] \"2021-04-20\" \"2021-04-21\" \"2021-04-22\" \"2021-04-23\" \"2021-04-24\"\n[101] \"2021-04-25\" \"2021-04-26\" \"2021-04-27\" \"2021-04-28\" \"2021-04-29\"\n[106] \"2021-04-30\" \"2021-05-01\" \"2021-05-02\" \"2021-05-03\" \"2021-05-04\"\n[111] \"2021-05-05\" \"2021-05-06\" \"2021-05-07\" \"2021-05-08\" \"2021-05-09\"\n[116] \"2021-05-10\"\n\n\nSome helpful functions include the Julian Ordinal Day (e.g., number of days since the start of the year).\n\nordinal_day &lt;- yday( semester[102] )\nordinal_day\n\n[1] 116\n\n\nThe weekday as an integer (0-6 starting on Sunday), which I use to index the named values.\n\ndays_of_week &lt;- c(\"Sunday\",\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\")\nx &lt;- wday( semester[32] )\ndays_of_week[ x ]\n\n[1] \"Monday\"\n\n\nSince we did not specify a time, things like hour() and minute() do not provide any usable information.\n\n\n6.5.2 Dates & Times\nTo add time to the date objects, we need to specify both date and time specifically. Here are some example data:\n\ndf &lt;- data.frame( Date = c(\"8/21/2004 7:33:51 AM\",\n                           \"7/12/2008 9:23:08 PM\",\n                           \"2/14/2010 8:18:30 AM\",\n                           \"12/23/2018 11:11:45 PM\",\n                           \"2/1/2019 4:42:00 PM\",\n                           \"5/17/2012 1:23:23 AM\",\n                           \"12/11/2020 9:48:02 PM\") )\nsummary( df )\n\n     Date          \n Length:7          \n Class :character  \n Mode  :character  \n\n\nJust like above, if we want to turn these into date and time objects we must be able to tell the parsing algorithm what elements are represented in each entry. There are many ways to make dates and time, 10/14 or 14 Oct or October 14 or Julian day 287, etc. These are designated by a format string were we indicate what element represents a day or month or year or hour or minute or second, etc. These are found by looking at the documentation for?strptime.\nIn our case, we have:\n- Month as 1 or 2 digits\n- Day as 1 or 2 digits\n- Year as 4 digits\n- a space to separate date from time\n- hour (not 24-hour though)\n- minutes in 2 digits\n- seconds in 2 digits\n- a space to separate time from timezone\n- timezone\n- / separating date objects\n- : separating time objects\nTo make the format string, we need to look up how to encode these items. The items in df for a date & time object such as 2/1/2019 4:42:00 PM have the format string:\n\nformat &lt;- \"%m/%d/%Y %I:%M:%S %p\"\n\nNow, we can convert the character string in the data frame to a date and time object.\n\n\n6.5.3 Lubridate\nInstead of using the built-in as.Date() functionality, I like the lubridate library1 as it has a lot of additional functionality that we’ll play with a bit later.\n\ndf$Date &lt;- parse_date_time( df$Date, \n                            orders=format, \n                            tz = \"EST\" )\nsummary( df )\n\n      Date                       \n Min.   :2004-08-21 07:33:51.00  \n 1st Qu.:2009-04-29 14:50:49.00  \n Median :2012-05-17 01:23:23.00  \n Mean   :2013-07-11 07:28:39.85  \n 3rd Qu.:2019-01-12 19:56:52.50  \n Max.   :2020-12-11 21:48:02.00  \n\nclass( df$Date )\n\n[1] \"POSIXct\" \"POSIXt\" \n\n\nNow, we can ask Date-like questions about the data such as what day of the week was the first sample taken?\n\nweekdays( df$Date[1] )\n\n[1] \"Saturday\"\n\n\nWhat is the range of dates?\n\nrange( df$Date )\n\n[1] \"2004-08-21 07:33:51 EST\" \"2020-12-11 21:48:02 EST\"\n\n\nWhat is the median of samples\n\nmedian( df$Date )\n\n[1] \"2012-05-17 01:23:23 EST\"\n\n\nand what julian ordinal day (e.g., how many days since start of the year) is the last record.\n\nyday( df$Date[4] )\n\n[1] 357\n\n\nJust for fun, I’ll add a column to the data that has weekday.\n\ndf$Weekday &lt;- weekdays( df$Date )\ndf\n\n                 Date  Weekday\n1 2004-08-21 07:33:51 Saturday\n2 2008-07-12 21:23:08 Saturday\n3 2010-02-14 08:18:30   Sunday\n4 2018-12-23 23:11:45   Sunday\n5 2019-02-01 16:42:00   Friday\n6 2012-05-17 01:23:23 Thursday\n7 2020-12-11 21:48:02   Friday\n\n\nHowever, we should probably turn it into a factor (e.g., a data type with pre-defined levels—and for us here—an intrinsic order of the levels).\n\ndf$Weekday &lt;- factor( df$Weekday, \n                        ordered = TRUE, \n                        levels = days_of_week\n                        )\nsummary( df$Weekday )\n\n   Sunday    Monday   Tuesday Wednesday  Thursday    Friday  Saturday \n        2         0         0         0         1         2         2 \n\n\n\n\n6.5.4 Filtering on Date Objects\nWe can easily filter the content within a data.frame using some helper functions such as hour(), minute(), weekday(), etc. Here are some examples including pulling out the weekends.\n\nweekends &lt;- df[ df$Weekday %in% c(\"Saturday\",\"Sunday\"), ]\nweekends\n\n                 Date  Weekday\n1 2004-08-21 07:33:51 Saturday\n2 2008-07-12 21:23:08 Saturday\n3 2010-02-14 08:18:30   Sunday\n4 2018-12-23 23:11:45   Sunday\n\n\nfinding items that are in the past (paste being defined as the last time this document was knit).\n\npast &lt;- df$Date[ df$Date &lt; Sys.time() ]\npast\n\n[1] \"2004-08-21 07:33:51 EST\" \"2008-07-12 21:23:08 EST\"\n[3] \"2010-02-14 08:18:30 EST\" \"2018-12-23 23:11:45 EST\"\n[5] \"2019-02-01 16:42:00 EST\" \"2012-05-17 01:23:23 EST\"\n[7] \"2020-12-11 21:48:02 EST\"\n\n\nItems that are during working hours\n\nwork &lt;- df$Date[ hour(df$Date) &gt;= 9 & hour(df$Date) &lt;= 17 ]\nwork\n\n[1] \"2019-02-01 16:42:00 EST\"\n\n\nAnd total range of values in days using normal arithmatic operations such as the minus operator.\n\nmax(df$Date) - min(df$Date)\n\nTime difference of 5956.593 days"
  },
  {
    "objectID": "data_types.html#questions",
    "href": "data_types.html#questions",
    "title": "6  Basic Data Types",
    "section": "6.6 Questions",
    "text": "6.6 Questions\nIf you have any questions for me specifically on this topic, please post as an Issue in your repository, otherwise consider posting to the discussion board on Canvas."
  },
  {
    "objectID": "data_types.html#footnotes",
    "href": "data_types.html#footnotes",
    "title": "6  Basic Data Types",
    "section": "",
    "text": "If you get an error saying something like, “there is no package named lubridate” then use install.packages(\"lubridate\") and install it. You only need to do this once.↩︎"
  },
  {
    "objectID": "data_containers.html#vectors",
    "href": "data_containers.html#vectors",
    "title": "7  Basic Data Containers in R",
    "section": "7.1 Vectors",
    "text": "7.1 Vectors\nVectors are the most basic data container in R. They must contain data of the exact same type and are constructed using the combine() function, which is abbreviated as c() because good programmers are lazy programmers. 1\nHere is an example with some numbers.\n\nx &lt;- c(1,2,3)\nx\n\n[1] 1 2 3\n\n\nVectors can contain any of the base data types.\n\ny &lt;- c(TRUE, TRUE, FALSE, FALSE)\ny\n\n[1]  TRUE  TRUE FALSE FALSE\n\n\n\nz &lt;- c(\"Bob\",\"Alice\",\"Thomas\")\nz\n\n[1] \"Bob\"    \"Alice\"  \"Thomas\"\n\n\nEach vector has an inherent length representing the number of elements it contains.\n\nlength(x)\n\n[1] 3\n\n\n\n7.1.1 Introspection\nWhen asked, a vector reports the class of itself as the type of data contained within it.\n\nclass(x)\n\n[1] \"numeric\"\n\nclass(y)\n\n[1] \"logical\"\n\nclass(z)\n\n[1] \"character\"\n\n\nhowever, a vector is also a data type. As such, it has the is.vector() function. So this x can be both a vector and a numeric.\n\nis.vector( x ) && is.numeric( x )\n\n[1] TRUE\n\n\n\n\n7.1.2 Sequences\nThere are a lot of times when we require a sequnce of values and it would get a bit tedious to type them all out manually. R has several options for creating vectors that are comprised of a sequence of values.\nThe easiest type is the colon operator, that will generate a seqeunce of numerical values from the number on the left to the number on the right\n\n1:10 -&gt; y\ny\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nIt also works in the other direction (descending).\n\n10:1\n\n [1] 10  9  8  7  6  5  4  3  2  1\n\n\nHowever, it is only available to make a sequences where the increment from one value to the next is 1.\n\n3.2:5.7\n\n[1] 3.2 4.2 5.2\n\n\nFor more fine-grained control, we can use the function seq() to iterate across a range of values and specify either the step size (here from 1-10 by 3’s)\n\nseq(1,10,by=3)\n\n[1]  1  4  7 10\n\n\nOR the length of the response and it will figure out the step size to give you the right number of elements.\n\nseq( 119, 121, length.out = 6)\n\n[1] 119.0 119.4 119.8 120.2 120.6 121.0\n\n\n\n\n7.1.3 Indexing & Access\nTo access and change values within a vector, we used square brackets and the number of the entry of interest. It should be noted that in R, the first element of a vector is # 1.\nSo, to get to the third element of the x vector, we would:\n\nx[3]\n\n[1] 3\n\n\nIf you ask for values in the vector off the end (e.g., the index is beyond the length of the vector) it will return missing data.\n\nx[5]\n\n[1] NA\n\n\nIn addition to getting the values from a vector, assignment of individual values proceeds similarily.\n\nx[2] &lt;- 42\nx\n\n[1]  1 42  3\n\n\nIf you assign a value to a vector that is way off the end, it will fill in the intermediate values wtih NA for you.\n\nx[7] &lt;- 43\nx\n\n[1]  1 42  3 NA NA NA 43\n\n\n\n\n7.1.4 Vector Operations\nJust like individual values for each data type, vectors of these data types can also be operated using the same operators. Consider the two vectors x (a sequence) and y (a random selection from a Poisson distribution), both with 5 elements.\n\nx &lt;- 1:5\ny &lt;- rpois(5,2)\nx\n\n[1] 1 2 3 4 5\n\ny\n\n[1] 0 4 2 1 1\n\n\nMathematics operations are done element-wise. Here is an example using addition.\n\nx + y \n\n[1] 1 6 5 5 6\n\n\nas well as exponents.\n\nx^y\n\n[1]  1 16  9  4  5\n\n\nIf the lengths of the vectors are not the same R will implement a recycling rule where the shorter of the vectors is repeated until you fill up the size of the longer vector. Here is an example with the 5-element x and the a new 10-element z. Notice how the values in x are repeated in the addition operaiton.\n\nz &lt;- 1:10\nx + z\n\n [1]  2  4  6  8 10  7  9 11 13 15\n\n\nIf the two vectors are not multiples of each other in length, it will still recycle the shorter one but will also give you a warning that the two vectors are not conformant (just a FYI).\n\nx + 1:8\n\nWarning in x + 1:8: longer object length is not a multiple of shorter object\nlength\n\n\n[1]  2  4  6  8 10  7  9 11\n\n\nThe operations used are dependent upon the base data type. For example, the following character values can be passed along to the paste() function to put each of the elements in the first vectoer with the corresponding values in the second vector (and specifying the separator).\n\na &lt;- c(\"Bob\",\"Alice\",\"Thomas\")\nb &lt;- c(\"Biologist\",\"Chemist\",\"Mathematician\")\npaste( a, b, sep=\" is a \")\n\n[1] \"Bob is a Biologist\"        \"Alice is a Chemist\"       \n[3] \"Thomas is a Mathematician\"\n\n\nSo, in addition to being able to work on individual values, all functions are also vector functions."
  },
  {
    "objectID": "data_containers.html#matrices",
    "href": "data_containers.html#matrices",
    "title": "7  Basic Data Containers in R",
    "section": "7.2 Matrices",
    "text": "7.2 Matrices\nA matrix is a 2-dimensional container for the same kind of data as well. The two dimensions are represented as rows and columns in a rectangular configuration. Here I will make a 3x3 vector consisting of a sequence of numbers from 1 to 9.\n\nX &lt;- matrix( 1:9, nrow=3, ncol=3 )\nX\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\nIt is a bit redundant to have both nrow and ncol with nrow * ncol = length(sequence), you can just specify one of them and it will work out the other dimension.\n\n7.2.1 Indexing\nJust like a vector, matrices use square brackets and the row & column number (in that order) to access indiviudal elements. Also, just like vectors, both rows and columns start at 1 (not zero). So to replace the value in the second row and second column with the number 42, we do this.\n\nX[2,2] &lt;- 42\nX\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2   42    8\n[3,]    3    6    9\n\n\nMatrices are actually structures fundamental to things like linear algebra. As such, there are many operations that can be applied to matrices, both unary and binary.\nA transpose is a translation of a matrix that switches the rows and columns. In R it is done by the function t(). Here I use this to define another matrix.\n\nY &lt;- t(X)\nY\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4   42    6\n[3,]    7    8    9\n\n\nBinary operators using the normal operators in the top row of your keyboard are generally element-wise operations. Here the addition of these two matrices require:\n1. Both matrices have the same number of rows.\n2. Both matrices have the same number of columns.\n3. Both matrices have the same internal data types.\nHere is an example of addition (notic how the resulting [1,1] object is equal to X[1,1] + Y[1,1])\n\nX + Y\n\n     [,1] [,2] [,3]\n[1,]    2    6   10\n[2,]    6   84   14\n[3,]   10   14   18\n\n\nThe same for element-wise multiplication.\n\nX * Y\n\n     [,1] [,2] [,3]\n[1,]    1    8   21\n[2,]    8 1764   48\n[3,]   21   48   81\n\n\nHowever, there is another kind of matrix mutliplication that sums the product or rows and columns. Since this is also a variety of multiplication but is carried out differently, we need to use a different operator. Here the matrix mutliplication operator is denoted as the combination of characters %*%.\n\nX %*% Y\n\n     [,1] [,2] [,3]\n[1,]   66  226   90\n[2,]  226 1832  330\n[3,]   90  330  126\n\n\nThis operation has a few different constraints:\n\nThe number of columns in the left matrix must equal the number of rows in the right one.\nThe resulting matrix will have the number of rows equal to that of the right matrix.\nThe resulting matrix will have the number of columns equal to that of the left matrix.\nThe resulting element at the \\(i\\) \\(j\\) position is the sum of the multipliation of the elements in the \\(i^{th}\\) row of the left matrix and the \\(j^{th}\\) column of the right one.\n\nSo the resulting element in [1,3] position is found by \\(1*3 + 4*6 + 7*9 = 90\\)."
  },
  {
    "objectID": "data_containers.html#lists",
    "href": "data_containers.html#lists",
    "title": "7  Basic Data Containers in R",
    "section": "7.3 Lists",
    "text": "7.3 Lists\nLists are a more flexible container type. Here, lists can contain different types of data in a single list. Here is an example of a list made with a few character vluaes, a numeric, a constant, and a logical value.\n\nlst &lt;- list(\"A\",\"B\",323423.3, pi, TRUE)\n\nWhen you print out a list made like this, it will indicate each element as a numeric value in double square brackets.\n\nlst\n\n[[1]]\n[1] \"A\"\n\n[[2]]\n[1] \"B\"\n\n[[3]]\n[1] 323423.3\n\n[[4]]\n[1] 3.141593\n\n[[5]]\n[1] TRUE\n\n\n\n7.3.1 Indexing\nIndexing values in a list can be done using these numbers. To get and reset the values in the second element of the list, one would:\n\nlst[[2]] &lt;- \"C\"\nlst\n\n[[1]]\n[1] \"A\"\n\n[[2]]\n[1] \"C\"\n\n[[3]]\n[1] 323423.3\n\n[[4]]\n[1] 3.141593\n\n[[5]]\n[1] TRUE\n\n\n\n\n7.3.2 Named Lists\nLists can be more valuable if we use names for the keys instead of just numbers. Here, I make an empty list and then assign values to it using names (as character values) in square brakets.\n\nmyInfo &lt;- list()\nmyInfo[\"First Name\"] &lt;- \"Rodney\"\nmyInfo[\"Second Name\"] &lt;- \"Dyer\"\nmyInfo[\"Favorite Number\"] &lt;- 42\n\nWhen showing named lists, it prints included items as:\n\nmyInfo\n\n$`First Name`\n[1] \"Rodney\"\n\n$`Second Name`\n[1] \"Dyer\"\n\n$`Favorite Number`\n[1] 42\n\n\nIn addition to the square bracket approach, we can also use as $ notation to add elements to the list (like shown above).\n\nmyInfo$Vegitarian &lt;- FALSE\n\nBoth are equivallent.\n\nmyInfo\n\n$`First Name`\n[1] \"Rodney\"\n\n$`Second Name`\n[1] \"Dyer\"\n\n$`Favorite Number`\n[1] 42\n\n$Vegitarian\n[1] FALSE\n\n\nIn addition to having different data types, you can also have different sized data types inside a list. Here I add a vector (a valid data type as shown above) to the list.\n\nmyInfo$Homes &lt;- c(\"RVA\",\"Oly\",\"SEA\")\nmyInfo\n\n$`First Name`\n[1] \"Rodney\"\n\n$`Second Name`\n[1] \"Dyer\"\n\n$`Favorite Number`\n[1] 42\n\n$Vegitarian\n[1] FALSE\n\n$Homes\n[1] \"RVA\" \"Oly\" \"SEA\"\n\n\nTo access these values, we can use a combination of $ notation and [] on the resulting vector.\n\nmyInfo$Homes[2]\n\n[1] \"Oly\"\n\n\nWhen elements in a list are defined using named keys, the list itself can be asked for the keys using names().\n\nnames(myInfo)\n\n[1] \"First Name\"      \"Second Name\"     \"Favorite Number\" \"Vegitarian\"     \n[5] \"Homes\"          \n\n\nThis can be helpful at times when you did not create the list yourself and want to see what is inside of them.\n\n\n7.3.3 Spaces in Names\nAs you see above, this list has keys such as “First Name” and “Vegitarian”. The first one has a space inside of it whereas the second one does not. This is a challenge. If we were to try to use the first key as\n\nmyInfo$First Name\n\nWould give you an error (if I ran the chunck but I cannot because it is an error and won’t let me compile this document if I do). For names that have spaces, we need to enclose them inside back-ticks (as shown in the output above).\n\nmyInfo$`First Name`\n\n[1] \"Rodney\"\n\n\nSo feel free to use names that make sense, but if you do, you’ll need to treat them a bit specially using the backticks.\n\n\n7.3.4 Analysis Output\nBy far, the most common location for lists is when you do some kind of analysis. Almost all analyses return the restuls as a special kind of list.\nHere is an example looking at some data from three species of Iris on the lengths and width of sepal and petals. The data look like:\n\n\n\n\n\nFigure 7.1: The distribution of sepal and petal lengths from three species of Iris.\n\n\n\n\nWe can look at the correlation between two variable using the built-in cor.test() function.\n\niris.test &lt;- cor.test( iris$Sepal.Length, iris$Petal.Length )\n\nWe can print the output and it will format the results in a proper way.\n\niris.test\n\n\n    Pearson's product-moment correlation\n\ndata:  iris$Sepal.Length and iris$Petal.Length\nt = 21.646, df = 148, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8270363 0.9055080\nsample estimates:\n      cor \n0.8717538 \n\n\nHowever, the elements in the iris.test are simply a list.\n\nnames(iris.test)\n\n[1] \"statistic\"   \"parameter\"   \"p.value\"     \"estimate\"    \"null.value\" \n[6] \"alternative\" \"method\"      \"data.name\"   \"conf.int\"   \n\n\nIf fact, the contents of the output are just keys and values, even though when we printed it all out, it was formatted as a much more informative output.\n\n\n\n\n\n\n  \n    \n    \n      \n      Values\n    \n  \n  \n    statistic.t\n21.6460193457598\n    parameter.df\n148\n    p.value\n1.03866741944978e-47\n    estimate.cor\n0.871753775886583\n    null.value.correlation\n0\n    alternative\ntwo.sided\n    method\nPearson's product-moment correlation\n    data.name\niris$Sepal.Length and iris$Petal.Length\n    conf.int1\n0.827036329664362\n    conf.int2\n0.905508048821454\n  \n  \n  \n\n\n\n\nWe will come back to this special kind of printing later when discussing functions but for now, lets just consider how cool this is because we can access the raw values of the analysis directly. We an also easily incorporate the findings of analyses, such as this simple correlation test, and insert the content into the text. All you have to do is address the components of the analysis as in-text r citation. Here is an example where I include the values of:\n\niris.test$estimate\n\n      cor \n0.8717538 \n\niris.test$statistic\n\n       t \n21.64602 \n\niris.test$p.value\n\n[1] 1.038667e-47\n\n\nHere is an example paragraph (see the raw quarto document to see the formatting).\n\nThere was a significant relationship between sepal and petal length (Pearson Correlation, \\(\\rho =\\) 0.872, \\(t =\\) 21.6, P = 1.04e-47)."
  },
  {
    "objectID": "data_containers.html#data-frames",
    "href": "data_containers.html#data-frames",
    "title": "7  Basic Data Containers in R",
    "section": "7.4 Data Frames",
    "text": "7.4 Data Frames\nThe data.frame is the most common container for all the data you’ll be working with in R. It is kind of like a spreadsheet in that each column of data is the same kind of data measured on all objects (e.g., weight, survival, population, etc.) and each row represents one observation that has a bunch of different kinds of measurements associated with it.\nHere is an example with three different data types (the z is a random sample of TRUE/FALSE equal in length to the other elements).\n\nx &lt;- 1:10\ny &lt;- LETTERS[11:20]\nz &lt;- sample( c(TRUE,FALSE), size=10, replace=TRUE )\n\nI can put them into a data.frame object as:\n\ndf &lt;- data.frame( TheNums = x,\n                  TheLetters = y,\n                  TF = z\n                  )\ndf\n\n   TheNums TheLetters    TF\n1        1          K  TRUE\n2        2          L FALSE\n3        3          M FALSE\n4        4          N FALSE\n5        5          O  TRUE\n6        6          P  TRUE\n7        7          Q FALSE\n8        8          R  TRUE\n9        9          S  TRUE\n10      10          T  TRUE\n\n\nSince each column is its own ‘type’ we can easily get a summary of the elements within it using summary().\n\nsummary( df )\n\n    TheNums       TheLetters            TF         \n Min.   : 1.00   Length:10          Mode :logical  \n 1st Qu.: 3.25   Class :character   FALSE:4        \n Median : 5.50   Mode  :character   TRUE :6        \n Mean   : 5.50                                     \n 3rd Qu.: 7.75                                     \n Max.   :10.00                                     \n\n\nAnd depending upon the data type, the output may give numerical, counts, or just description of the contents.\n\n7.4.1 Indexing\nJust like a list, a data.frame can be defined as having named columns. The distinction here is that each column should have the same number of elements in it, whereas a list may have differnet lengths to the elements.\n\nnames( df )\n\n[1] \"TheNums\"    \"TheLetters\" \"TF\"        \n\n\nAnd like the list, we can easily use the $ operator to access the vectors components.\n\ndf$TheLetters\n\n [1] \"K\" \"L\" \"M\" \"N\" \"O\" \"P\" \"Q\" \"R\" \"S\" \"T\"\n\nclass( df$TheLetters )\n\n[1] \"character\"\n\n\nIndexing and grabbing elements can be done by either the column name (with $) and a square bracket OR by the [row,col] indexing like the matrix above.\n\ndf$TheLetters[3]\n\n[1] \"M\"\n\ndf[3,2]\n\n[1] \"M\"\n\n\nJust like a matrix, the dimensions of the data.frame is defined by the number of rows and columns.\n\ndim( df )\n\n[1] 10  3\n\nnrow( df )\n\n[1] 10\n\nncol( df )\n\n[1] 3\n\n\n\n\n7.4.2 Loading Data\nBy far, you will most often NOT be making data by hand but instead will be loading it from external locations. here is an example of how we can load in a CSV file that is located in the GitHub repository for this topic. As this is a public repository, we can get a direct URL to the file. For simplicity, I’ll load in tidyverse and use some helper functions contained therein.\n\nlibrary( tidyverse )\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nThe URL for this repository is\n\nurl &lt;- \"https://raw.githubusercontent.com/DyerlabTeaching/Data-Containers/main/data/arapat.csv\"\n\nAnd we can read it in directly (as long as we have an internet connection) as:\n\nbeetles &lt;- read_csv( url )\n\nRows: 39 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Stratum\ndbl (2): Longitude, Latitude\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNotice how the funtion tells us a few things about the data.\nThe data itself consists of:\n\nsummary( beetles )\n\n   Stratum            Longitude         Latitude    \n Length:39          Min.   :-114.3   Min.   :23.08  \n Class :character   1st Qu.:-112.9   1st Qu.:24.52  \n Mode  :character   Median :-111.5   Median :26.21  \n                    Mean   :-111.7   Mean   :26.14  \n                    3rd Qu.:-110.4   3rd Qu.:27.47  \n                    Max.   :-109.1   Max.   :29.33  \n\n\nwhich looks like:\n\nbeetles\n\n# A tibble: 39 × 3\n   Stratum Longitude Latitude\n   &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n 1 88          -114.     29.3\n 2 9           -114.     29.0\n 3 84          -114.     29.0\n 4 175         -113.     28.7\n 5 177         -114.     28.7\n 6 173         -113.     28.4\n 7 171         -113.     28.2\n 8 89          -113.     28.0\n 9 159         -113.     27.5\n10 SFr         -113.     27.4\n# ℹ 29 more rows\n\n\nWe can quickly use these data and make an interactive labeled map of it in a few lines of code (click on a marker).\n\nlibrary( leaflet )\nbeetles %&gt;%\n  leaflet() %&gt;%\n  addProviderTiles(provider = providers$Esri.WorldTopo) %&gt;%\n  addMarkers( ~Longitude, ~Latitude,popup = ~Stratum )"
  },
  {
    "objectID": "data_containers.html#questions",
    "href": "data_containers.html#questions",
    "title": "7  Basic Data Containers in R",
    "section": "7.5 Questions",
    "text": "7.5 Questions\nIf you have any questions for me specifically on this topic, please post as an Issue in your repository, otherwise consider posting to the discussion board on Canvas."
  },
  {
    "objectID": "data_containers.html#footnotes",
    "href": "data_containers.html#footnotes",
    "title": "7  Basic Data Containers in R",
    "section": "",
    "text": "The more lines of code that you write, the more likely there will be either a grammatical error (easier to find) or a logical one (harder to find).↩︎"
  },
  {
    "objectID": "factors.html#the-forcats-library",
    "href": "factors.html#the-forcats-library",
    "title": "8  Factor Data",
    "section": "8.1 The forcats Library",
    "text": "8.1 The forcats Library\nThe forcats library has a bunch of helper functions for working with factors. This is a relatively small library in tidyverse but a powerful one. I would recommend looking at the cheatsheet for it to get a more broad understanding of what functions in this library can do.\n\nlibrary( forcats )\n\nJust like stringr had the str_ prefix, all the functions here have the fct_ prefix. Here are some examples.\nCounting how many of each factor\n\nfct_count( data )\n\n# A tibble: 7 × 2\n  f             n\n  &lt;ord&gt;     &lt;int&gt;\n1 Monday        6\n2 Tuesday       5\n3 Wednesday     5\n4 Thursday      8\n5 Friday        4\n6 Saturday      7\n7 Sunday        5\n\n\nLumping Rare Factors\n\nlumped &lt;- fct_lump_min( data, min = 5 )\nfct_count( lumped )\n\n# A tibble: 7 × 2\n  f             n\n  &lt;ord&gt;     &lt;int&gt;\n1 Monday        6\n2 Tuesday       5\n3 Wednesday     5\n4 Thursday      8\n5 Saturday      7\n6 Sunday        5\n7 Other         4\n\n\nReordering Factor Levels by Frequency\n\nfreq &lt;- fct_infreq( data )\nlevels( freq )\n\n[1] \"Thursday\"  \"Saturday\"  \"Monday\"    \"Tuesday\"   \"Wednesday\" \"Sunday\"   \n[7] \"Friday\"   \n\n\nReordering by Order of Appearance\n\nordered &lt;- fct_inorder( data )\nlevels( ordered )\n\n[1] \"Thursday\"  \"Saturday\"  \"Monday\"    \"Wednesday\" \"Friday\"    \"Tuesday\"  \n[7] \"Sunday\"   \n\n\nReordering Specific Levels\n\nnewWeek &lt;- fct_relevel( data, \"Sunday\")\nlevels( newWeek )\n\n[1] \"Sunday\"    \"Monday\"    \"Tuesday\"   \"Wednesday\" \"Thursday\"  \"Friday\"   \n[7] \"Saturday\" \n\n\nDropping Unobserved Levels - just like droplevels()\n\ndropped &lt;- fct_drop( workdays )\nsummary( dropped )\n\n   Monday   Tuesday Wednesday  Thursday    Friday \n       10         9         9         4         8"
  },
  {
    "objectID": "factors.html#using-factors",
    "href": "factors.html#using-factors",
    "title": "8  Factor Data",
    "section": "8.2 Using Factors",
    "text": "8.2 Using Factors\nIt is common to use factors as an organizing princple in our data. For example, let’s say we went out and sampled three different species of plants and measured characteristics of their flower size. The iris data set from R.A. Fisher is a classid data set that is include in R and it looks like this (the functions head() and tail() show the top or bottom parts of a data frame).\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nBy default it is a data.frame object.\n\nclass( iris )\n\n[1] \"data.frame\"\n\n\n\n8.2.1 By the by\nOne helpful function in base R is the by() function. It has the following form.\nby( data, index, function)\nThe data is the raw data you are using, the index is a vector that we are using to differentiate among the species (the factor), and the function is what function we want to use.\nSo for example, if I were interesed in the mean length of the Sepal for each species, I could write.\n\nmeanSepalLength &lt;- by( iris$Sepal.Length, iris$Species, mean )\nclass( meanSepalLength )\n\n[1] \"by\"\n\nmeanSepalLength\n\niris$Species: setosa\n[1] 5.006\n------------------------------------------------------------ \niris$Species: versicolor\n[1] 5.936\n------------------------------------------------------------ \niris$Species: virginica\n[1] 6.588\n\n\nI could also do the same thing with the variance in sepal length.\n\nby( iris[,2], iris[,5], var ) -&gt; varSepalLength\nvarSepalLength \n\niris[, 5]: setosa\n[1] 0.1436898\n------------------------------------------------------------ \niris[, 5]: versicolor\n[1] 0.09846939\n------------------------------------------------------------ \niris[, 5]: virginica\n[1] 0.1040041\n\n\nUsing these kinds of functions we can create a summary data frame.\n\nlibrary( tidyverse )\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ ggplot2   3.4.4     ✔ stringr   1.5.0\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndf &lt;- tibble( Species = levels( iris$Species), \n              Average = meanSepalLength,\n              Variance = varSepalLength\n)\ndf\n\n# A tibble: 3 × 3\n  Species    Average  Variance  \n  &lt;chr&gt;      &lt;by[1d]&gt; &lt;by[1d]&gt;  \n1 setosa     5.006    0.14368980\n2 versicolor 5.936    0.09846939\n3 virginica  6.588    0.10400408"
  },
  {
    "objectID": "factors.html#missing-data",
    "href": "factors.html#missing-data",
    "title": "8  Factor Data",
    "section": "8.3 Missing Data",
    "text": "8.3 Missing Data\nMissing data is a .red[fact of life] and R is very opinionated about how it handles missing values. In general, missing data is encoded as NA and is a valid entry for any data type (character, numeric, logical, factor, etc.). Where this becomes tricky is when we are doing operations on data that has missing values. R could take two routes:\n\nIt could ignore the data and give you the answer directly as if the data were not missing, or\n\nIt could let you know that there is missing data and make you do something about it.\n\nFortunately, R took the second route.\nAn example from the iris data, I’m going to add some missing data to it.\n\nmissingIris &lt;- iris[, 4:5]\nmissingIris$Petal.Width[ c(2,6,12) ] &lt;- NA\nsummary( missingIris )\n\n  Petal.Width          Species  \n Min.   :0.100   setosa    :50  \n 1st Qu.:0.300   versicolor:50  \n Median :1.300   virginica :50  \n Mean   :1.218                  \n 3rd Qu.:1.800                  \n Max.   :2.500                  \n NA's   :3                      \n\n\nNotice how the missing data is denoted in the summary.\n\n8.3.1 Indications of Missing Data\nWhen we perform a mathematical or statistical operation on data that has missing elements R will always return NA as the result.\n\nmean( missingIris$Petal.Width )\n\n[1] NA\n\n\nThis warns you that .red[at least one] of the observations in the data is missing.\nSame output for using by(), it will put NA into each level that has at least one missing value.\n\nby( missingIris$Petal.Width, missingIris$Species, mean )\n\nmissingIris$Species: setosa\n[1] NA\n------------------------------------------------------------ \nmissingIris$Species: versicolor\n[1] 1.326\n------------------------------------------------------------ \nmissingIris$Species: virginica\n[1] 2.026\n\n\n\n\n8.3.2 Working with Missing Data\nTo acknowledge that there are missing data and you still want the values, you need to tell the function you are using that data is missing and you are OK with that using the optional argument na.rm=TRUE (na = missing data & rm is remove).\n\nmean( missingIris$Petal.Width, na.rm=TRUE)\n\n[1] 1.218367\n\n\nTo pass this to the by() function, we add the optional argument na.rm=TRUE and by() passes it along to the mean function as “…”\n\nby( missingIris$Petal.Width, missingIris$Species, mean, na.rm=TRUE )\n\nmissingIris$Species: setosa\n[1] 0.2446809\n------------------------------------------------------------ \nmissingIris$Species: versicolor\n[1] 1.326\n------------------------------------------------------------ \nmissingIris$Species: virginica\n[1] 2.026"
  },
  {
    "objectID": "factors.html#fancy-tables",
    "href": "factors.html#fancy-tables",
    "title": "8  Factor Data",
    "section": "8.4 Fancy Tables",
    "text": "8.4 Fancy Tables\nMaking data frames like that above is a classic maneuver in R and I’m going to use this to introduce the use of the knitr library to show you how to take a set of data and turn it into a table for your manuscript.\n\nlibrary( knitr )\n\nNow we can make a table as:\n\nkable( df )\n\n\n\n\nSpecies\nAverage\nVariance\n\n\n\n\nsetosa\n5.006\n0.14368980\n\n\nversicolor\n5.936\n0.09846939\n\n\nvirginica\n6.588\n0.10400408\n\n\n\n\n\nWe can even add a caption to it.\n\nirisTable &lt;- kable( df, caption = \"The mean and variance in measured sepal length (in cm) for three species of Iris.\")\nirisTable\n\n\nThe mean and variance in measured sepal length (in cm) for three species of Iris.\n\n\nSpecies\nAverage\nVariance\n\n\n\n\nsetosa\n5.006\n0.14368980\n\n\nversicolor\n5.936\n0.09846939\n\n\nvirginica\n6.588\n0.10400408\n\n\n\n\n\nIn addition to this basic library, there is an kableExtra one that allows us to get even more fancy. You must go check out this webpage (which is an RMarkdown page by the way) to see all the other ways you can fancy up your tables.\n\nlibrary( kableExtra )\n\n\nAttaching package: 'kableExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\n\n\n8.4.1 Table Themes\nHere are some examples Themes\n\nkable_paper( irisTable )\n\nWarning in kable_styling(kable_input, \"none\", htmltable_class = light_class, :\nPlease specify format in kable. kableExtra can customize either HTML or LaTeX\noutputs. See https://haozhu233.github.io/kableExtra/ for details.\n\n\n\nThe mean and variance in measured sepal length (in cm) for three species of Iris.\n\n\nSpecies\nAverage\nVariance\n\n\n\n\nsetosa\n5.006\n0.14368980\n\n\nversicolor\n5.936\n0.09846939\n\n\nvirginica\n6.588\n0.10400408\n\n\n\n\n\n\nkable_classic( irisTable )\n\nWarning in kable_styling(kable_input, \"none\", htmltable_class = light_class, :\nPlease specify format in kable. kableExtra can customize either HTML or LaTeX\noutputs. See https://haozhu233.github.io/kableExtra/ for details.\n\n\n\nThe mean and variance in measured sepal length (in cm) for three species of Iris.\n\n\nSpecies\nAverage\nVariance\n\n\n\n\nsetosa\n5.006\n0.14368980\n\n\nversicolor\n5.936\n0.09846939\n\n\nvirginica\n6.588\n0.10400408\n\n\n\n\n\n\nkable_classic_2( irisTable )\n\nWarning in kable_styling(kable_input, \"none\", htmltable_class = light_class, :\nPlease specify format in kable. kableExtra can customize either HTML or LaTeX\noutputs. See https://haozhu233.github.io/kableExtra/ for details.\n\n\n\nThe mean and variance in measured sepal length (in cm) for three species of Iris.\n\n\nSpecies\nAverage\nVariance\n\n\n\n\nsetosa\n5.006\n0.14368980\n\n\nversicolor\n5.936\n0.09846939\n\n\nvirginica\n6.588\n0.10400408\n\n\n\n\n\n\nkable_minimal( irisTable )\n\nWarning in kable_styling(kable_input, \"none\", htmltable_class = light_class, :\nPlease specify format in kable. kableExtra can customize either HTML or LaTeX\noutputs. See https://haozhu233.github.io/kableExtra/ for details.\n\n\n\nThe mean and variance in measured sepal length (in cm) for three species of Iris.\n\n\nSpecies\nAverage\nVariance\n\n\n\n\nsetosa\n5.006\n0.14368980\n\n\nversicolor\n5.936\n0.09846939\n\n\nvirginica\n6.588\n0.10400408\n\n\n\n\n\n\nkable_material( irisTable,lightable_options = c(\"striped\", \"hover\") )\n\nWarning in kable_styling(kable_input, \"none\", htmltable_class = light_class, :\nPlease specify format in kable. kableExtra can customize either HTML or LaTeX\noutputs. See https://haozhu233.github.io/kableExtra/ for details.\n\n\n\nThe mean and variance in measured sepal length (in cm) for three species of Iris.\n\n\nSpecies\nAverage\nVariance\n\n\n\n\nsetosa\n5.006\n0.14368980\n\n\nversicolor\n5.936\n0.09846939\n\n\nvirginica\n6.588\n0.10400408\n\n\n\n\n\n\nkable_material_dark( irisTable )\n\nWarning in kable_styling(kable_input, \"none\", htmltable_class = light_class, :\nPlease specify format in kable. kableExtra can customize either HTML or LaTeX\noutputs. See https://haozhu233.github.io/kableExtra/ for details.\n\n\n\nThe mean and variance in measured sepal length (in cm) for three species of Iris.\n\n\nSpecies\nAverage\nVariance\n\n\n\n\nsetosa\n5.006\n0.14368980\n\n\nversicolor\n5.936\n0.09846939\n\n\nvirginica\n6.588\n0.10400408\n\n\n\n\n\n\n\n8.4.2 Table Sizes and Positions\nWe can be specific about the size and location of the whole table.\n\nkable_paper(irisTable, full_width = FALSE )\n\nWarning in kable_styling(kable_input, \"none\", htmltable_class = light_class, :\nPlease specify format in kable. kableExtra can customize either HTML or LaTeX\noutputs. See https://haozhu233.github.io/kableExtra/ for details.\n\n\n\nThe mean and variance in measured sepal length (in cm) for three species of Iris.\n\n\nSpecies\nAverage\nVariance\n\n\n\n\nsetosa\n5.006\n0.14368980\n\n\nversicolor\n5.936\n0.09846939\n\n\nvirginica\n6.588\n0.10400408\n\n\n\n\n\n\nkable_paper( irisTable, full_width=FALSE, position=\"right\")\n\nWarning in kable_styling(kable_input, \"none\", htmltable_class = light_class, :\nPlease specify format in kable. kableExtra can customize either HTML or LaTeX\noutputs. See https://haozhu233.github.io/kableExtra/ for details.\n\n\n\nThe mean and variance in measured sepal length (in cm) for three species of Iris.\n\n\nSpecies\nAverage\nVariance\n\n\n\n\nsetosa\n5.006\n0.14368980\n\n\nversicolor\n5.936\n0.09846939\n\n\nvirginica\n6.588\n0.10400408\n\n\n\n\n\nAnd even embed it in a bunch of text and float it to left or right (I added echo=FALSE to the chunck header so it hides itself).\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Ut blandit libero sit amet porta elementum. In imperdiet tellus non odio porttitor auctor ac sit amet diam. Suspendisse eleifend vel nisi nec efficitur. Ut varius urna lectus, ac iaculis velit bibendum eget. Curabitur dignissim magna eu odio sagittis blandit. Vivamus sed ipsum mi. Etiam est leo, mollis ultrices dolor eget, consectetur euismod augue. In hac habitasse platea dictumst. Integer blandit ante magna, quis volutpat velit varius hendrerit. Vestibulum sit amet lacinia magna. Sed at varius nisl. Donec eu porta tellus, vitae rhoncus velit.\n\n\nWarning in kable_styling(kable_input, \"none\", htmltable_class = light_class, :\nPlease specify format in kable. kableExtra can customize either HTML or LaTeX\noutputs. See https://haozhu233.github.io/kableExtra/ for details.\n\n\n\nThe mean and variance in measured sepal length (in cm) for three species of Iris.\n\n\nSpecies\nAverage\nVariance\n\n\n\n\nsetosa\n5.006\n0.14368980\n\n\nversicolor\n5.936\n0.09846939\n\n\nvirginica\n6.588\n0.10400408\n\n\n\n\n\nMaecenas euismod mattis neque. Ut at sapien lacinia, vehicula felis vitae, laoreet odio. Cras ut magna sed sapien scelerisque auctor maximus tincidunt arcu. Praesent vel accumsan leo. Etiam tempor leo placerat, commodo ante eu, posuere ligula. Sed purus justo, feugiat vel volutpat in, faucibus quis sem. Vivamus enim lacus, ultrices id erat in, posuere fringilla est. Nulla porttitor ac nunc nec efficitur. Duis tincidunt metus leo, at lacinia orci tristique in.\nNulla nec elementum nibh, quis congue augue. Vivamus fermentum nec mauris nec vehicula. Proin laoreet sapien quis orci mollis, et condimentum ante tempor. Vivamus hendrerit ut sem a iaculis. Quisque mauris enim, accumsan sit amet fermentum quis, convallis a nisl. Donec elit orci, consectetur id vestibulum in, elementum nec magna. In lobortis erat velit. Nam sit amet finibus arcu.\n\n\n8.4.3 Heading Judo\nWe can do some really cool stuff on row and column headings. Here is an example where I add another row above the data columns for output.\n\nclassic &lt;- kable_paper( irisTable )\n\nWarning in kable_styling(kable_input, \"none\", htmltable_class = light_class, :\nPlease specify format in kable. kableExtra can customize either HTML or LaTeX\noutputs. See https://haozhu233.github.io/kableExtra/ for details.\n\nadd_header_above( classic, c(\" \" = 1, \"Sepal Length (cm)\" = 2))\n\nWarning in add_header_above(classic, c(` ` = 1, `Sepal Length (cm)` = 2)):\nPlease specify format in kable. kableExtra can customize either HTML or LaTeX\noutputs. See https://haozhu233.github.io/kableExtra/ for details.\n\n\n\nThe mean and variance in measured sepal length (in cm) for three species of Iris.\n\n\nSpecies\nAverage\nVariance\n\n\n\n\nsetosa\n5.006\n0.14368980\n\n\nversicolor\n5.936\n0.09846939\n\n\nvirginica\n6.588\n0.10400408"
  },
  {
    "objectID": "factors.html#questions",
    "href": "factors.html#questions",
    "title": "8  Factor Data",
    "section": "8.5 Questions",
    "text": "8.5 Questions\nIf you have any questions for me specifically on this topic, please post as an Issue in your repository, otherwise consider posting to the discussion board on Canvas."
  },
  {
    "objectID": "classic_graphics.html#the-data",
    "href": "classic_graphics.html#the-data",
    "title": "9  Basic Graphics in R",
    "section": "9.1 The Data",
    "text": "9.1 The Data\nThe iris flower data set (also known as Fisher’s Iris data set) is a multivariate data set introduced by the British statistician, eugenicist, and biologist Ronald Fisher in his 1936 paper entitled, The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis.\nThese data are part of the base R distribution and contain sepal and pedal measurements for three species if congeneric plants, Iris setosa, I. versicolor, and I. virginica.\n\n\n\nThe three species of iris in the default data set.\n\n\nHere is what the data summary looks like.\n\nsummary(iris)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50"
  },
  {
    "objectID": "classic_graphics.html#basic-plotting-in-r",
    "href": "classic_graphics.html#basic-plotting-in-r",
    "title": "9  Basic Graphics in R",
    "section": "9.2 Basic Plotting in R",
    "text": "9.2 Basic Plotting in R\nThe base R comes with several built-in plotting functions, each of which is accessed through a single function with a wide array of optional arguments that modify the overall appearance.\nHistograms - The Density of A Single Data Vector\n\nhist( iris$Sepal.Length)\n\n\n\n\nYou can see that the default values for the hist() function label the x-axis & title on the graph have the names of the variable passed to it, with a y-axis is set to “Frequency”.\n\nxlab & ylab: The names attached to both x- and y-axes.\nmain: The title on top of the graph.\nbreaks: This controls the way in which the original data are partitioned (e.g., the width of the bars along the x-axis).\n\nIf you pass a single number, n to this option, the data will be partitioned into n bins.\nIf you pass a sequence of values to this, it will use this sequence as the boundaries of bins.\n\ncol: The color of the bar (not the border)\nprobability: A flag as either TRUE or FALSE (the default) to have the y-axis scaled by total likelihood of each bins rather than a count of the numbrer of elements in that range.\n\nDensity - Estimating the continuous density of data\n\nd_sepal.length &lt;- density( iris$Sepal.Length )\nd_sepal.length\n\n\nCall:\n    density.default(x = iris$Sepal.Length)\n\nData: iris$Sepal.Length (150 obs.); Bandwidth 'bw' = 0.2736\n\n       x               y            \n Min.   :3.479   Min.   :0.0001495  \n 1st Qu.:4.790   1st Qu.:0.0341599  \n Median :6.100   Median :0.1534105  \n Mean   :6.100   Mean   :0.1905934  \n 3rd Qu.:7.410   3rd Qu.:0.3792237  \n Max.   :8.721   Max.   :0.3968365  \n\n\nThe density() function estimates a continuous probability density function for the data and returns an object that has both x and y values. In fact, it is a special kind of object.\n\nclass(d_sepal.length)\n\n[1] \"density\"\n\n\nBecause of this, the general plot() function knows how to plot these kinds of things.\n\nplot( d_sepal.length )\n\n\n\n\nNow, the general plot() function has A TON of options and is overloaded to be able to plot all kinds of data. In addition to xlab and ylab, we modify the following:\n\ncol: Color of the line.\nlwd: Line width\nbty: This covers the ‘box type’, which is the square box around the plot area. I typically use bty=\"n\" because I hate those square boxes around my plots (compare the following 2 plots to see the differences). But you do you.\nxlim & ylim: These dictate the range on both the x- and y-axes. It takes a pair of values such as c(min,max) and then limits (or extends) that axis to to fill that range.\n\nScatter Plots - Plotting two variables\n\nplot( iris$Sepal.Length, iris$Sepal.Width  )\n\n\n\n\nHere is the most general plot(). The form of the arguments to this function are x-data and then y-data. The visual representation of the data is determined by the optional values you pass (or if you do not pass any optional values, the default is the scatter plot shown above)\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\ntype\nThe kind of plot to show (’p’oint, ’l’ine, ’b’oth, or ’o’ver). A point plot is the default.\n\n\npch\nThe character (or symbol) being used to plot. There 26 recognized general characters to use for plotting. The default is pch=1.\n\n\ncol\nThe color of the symbols/lines that are plot.\n\n\ncex\nThe magnification size of the character being plot. The default is cex=1 and deviation from that will increase (\\(cex &gt; 1\\)) or decrease (\\(0 &lt; cex &lt; 1\\)) the scaling of the symbols.\n\n\nlwd\nThe width of any lines in the plot.\n\n\nlty\nThe type of line to be plot (solid, dashed, etc.)\n\n\n\n\n\n\n\n\nOne of the relevant things you can use the parameter pch for is to differentiate between groups of observations (such as different species for example). Instead of giving it one value, pass it a vector of values whose length is equal to that for x- and y-axis data.\nHere is an example where I coerce the iris$Species data vector into numeric types and use that for symbols.\n\nsymbol &lt;- as.numeric(iris$Species)\nsymbol\n\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n [75] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3\n[112] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[149] 3 3\n\n\n\nplot( iris$Sepal.Length, iris$Sepal.Width, pch=symbol )\n\n\n\n\nWe can use the same technique to use col instead of pch. Here I make a vector of color names and then use the previously defined in the variable symbol.\n\nraw_colors &lt;- c(\"red\",\"gold\",\"forestgreen\")\ncolors &lt;- raw_colors[ symbol ]\ncolors[1:10]\n\n [1] \"red\" \"red\" \"red\" \"red\" \"red\" \"red\" \"red\" \"red\" \"red\" \"red\"\n\n\nIn addition to the general form for the function plot(x,y) we used above, we can use an alternative designation based upon what is called the functional form. The functional form is how we designate functions in R, such as regression anlaysis. This basic syntax for this is y ~ x, that is the response variable (on the y-axis) is a function of the predictor (on the x-axis).\nFor simplicty, I’ll make x and y varibles pointing to the same same data as in the previous graph.\n\ny &lt;- iris$Sepal.Width\nx &lt;- iris$Sepal.Length\n\nThen, the plot() function can be written as (including all the fancy additional stuff we just described):\n\nplot( y ~ x , \n      col=colors, \n      pch=20, \n      bty=\"n\", \n      xlab=\"Sepal Length\", ylab=\"Sepal Width\")\n\n\n\n\nThis is much easier to read (also notice how I used serveral lines to put in all the options to the plot function for legibility).\nBar Plots - Quantifying Counts\nThe barplot function takes a set of heights, one for each bar. Let’s quickly grab the mean length for sepals across all three species. There are many ways to do this, here are two, the first being more pedantic and the second more concise.\nThe iris data is in a data.frame that has a column designating the species. We can see which ones using unique().\n\nunique( iris$Species )\n\n[1] setosa     versicolor virginica \nLevels: setosa versicolor virginica\n\n\nTo estimate the mean for each species, we can take values in iris$Sepal.Length for each level of iris$Species using indices.\n\nmu.Setosa &lt;- mean( iris$Sepal.Length[ iris$Species == \"setosa\" ])\nmu.Versicolor &lt;- mean( iris$Sepal.Length[ iris$Species == \"versicolor\" ])\nmu.Virginica &lt;- mean( iris$Sepal.Length[ iris$Species == \"virginica\" ])\n\nmeanSepalLength &lt;- c( mu.Setosa, mu.Versicolor, mu.Virginica )\nmeanSepalLength\n\n[1] 5.006 5.936 6.588\n\n\nWhen we plot these data using barplot() we pass the values and set the names of the bars us\n\nbarplot( meanSepalLength, \n         names.arg = c(\"setosa\",\"versicolor\",\"virginica\"), \n         xlab=\"Iris Species\",\n         ylab=\"Mean Sepal Length\")\n\n\n\n\nThe second way to do this is to use the by() function (see ?by for the complete help file). The by function takes the following objects:\n\nThe raw data to use as measurements. Here we will use iris$Sepal.Length as the raw data.\nData designating groups to partition the raw data into (we will use iris$Species).\nThe function that you want to use on each group. (here we will ask for the mean).\n\n\nmeanSepalLength &lt;- by( iris$Sepal.Length, iris$Species, mean )\nmeanSepalLength\n\niris$Species: setosa\n[1] 5.006\n------------------------------------------------------------ \niris$Species: versicolor\n[1] 5.936\n------------------------------------------------------------ \niris$Species: virginica\n[1] 6.588\n\n\nThe data returned from this function is both numeric and has a name set for each value.\n\nis.numeric( meanSepalLength )\n\n[1] TRUE\n\nnames( meanSepalLength )\n\n[1] \"setosa\"     \"versicolor\" \"virginica\" \n\n\nThen when we pass that to barplot() the column labels are set automatically (e.g., no need to set names.arg as above).\n\nbarplot( meanSepalLength, \n         xlab = \"Iris Species\",\n         ylab = \"Average Sepal Length\")\n\n\n\n\nBoxplots - High density information\nA boxplot contains a high amount of information content and is appropriate when the groupings on the x-axis are categorical. For each category, the graphical representation includes:\n\nThe median value for the raw data\nA box indicating the area between the first and third quartile (e.g,. the values enclosing the 25% - 75% of the data). The top and bottoms are often referred to as the hinges of the box.\nA notch (if requested), represents confidence around the estimate of the median.\nWhiskers extending out to shows \\(\\pm 1.5 * IQR\\) (the Inner Quartile Range)\nAny points of the data that extend beyond the whiskers are plot as points.\n\nFor legibility, we can use the functional form for the plots as well as separate out the data.frame from the columns using the optional data= argument.\n\nboxplot( Sepal.Length ~ Species, \n         data = iris, \n         notch=TRUE, \n         ylab=\"Sepal Length\" )"
  },
  {
    "objectID": "classic_graphics.html#colors",
    "href": "classic_graphics.html#colors",
    "title": "9  Basic Graphics in R",
    "section": "9.3 Colors",
    "text": "9.3 Colors\nNamed Colors -  There are 657 pre-defined, named colors built into the base R distribution. Here is a random selection of those values.\n\nrandomColors &lt;- sample( colors(), size = nrow(iris) )\nhead(randomColors)\n\n[1] \"grey54\"         \"lavenderblush1\" \"gray32\"         \"peachpuff1\"    \n[5] \"grey35\"         \"springgreen3\"  \n\n\nTo use these colors, you can specify them by name for either all the elements\n\nboxplot( Sepal.Length ~ Species, \n         data = iris, \n         col = randomColors[1],\n         notch=TRUE, \n         ylab=\"Sepal Length\" )\n\n\n\n\nor for each element individually.\n\nboxplot( Sepal.Length ~ Species, \n         data = iris, \n         col = randomColors[1:3],\n         notch=TRUE, \n         ylab=\"Sepal Length\" )\n\n\n\n\nHex Colors:  You can also use hexadecimal representations of colors, which is most commonly used on the internet. A hex representation of colors consists of red, green, and blue values encoded as numbers in base 16 (e.g., the single digits 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, A, B, C, D, E, F). There are a lot of great resources on the internet for color themes that report red, green, blue and hex values. I often use the coolors.co website to look for themes that go well together for slides or presentations.\nColor Brewer Finally, there is an interesting website at colorbrewer2.org that has some interesting built-in palettes. There is an associated library that makes creating palettes for plots really easy and as you get more expreienced with R, you will find this very helpful. For quick visualizations and estimation of built-in color palettes, you can look at the website (below).\n or look at the colors in R\n\nlibrary(RColorBrewer)\ndisplay.brewer.all()\n\n\n\n\nThere are three basic kinds of palettes: divergent, qualitative, and sequential. Each of these built-in palletes has a maximum number of colors available (though as you see below we can use them to interpolate larger sets) as well as indications if the palette is safe for colorblind individuals.\n\nbrewer.pal.info\n\n         maxcolors category colorblind\nBrBG            11      div       TRUE\nPiYG            11      div       TRUE\nPRGn            11      div       TRUE\nPuOr            11      div       TRUE\nRdBu            11      div       TRUE\nRdGy            11      div      FALSE\nRdYlBu          11      div       TRUE\nRdYlGn          11      div      FALSE\nSpectral        11      div      FALSE\nAccent           8     qual      FALSE\nDark2            8     qual       TRUE\nPaired          12     qual       TRUE\nPastel1          9     qual      FALSE\nPastel2          8     qual      FALSE\nSet1             9     qual      FALSE\nSet2             8     qual       TRUE\nSet3            12     qual      FALSE\nBlues            9      seq       TRUE\nBuGn             9      seq       TRUE\nBuPu             9      seq       TRUE\nGnBu             9      seq       TRUE\nGreens           9      seq       TRUE\nGreys            9      seq       TRUE\nOranges          9      seq       TRUE\nOrRd             9      seq       TRUE\nPuBu             9      seq       TRUE\nPuBuGn           9      seq       TRUE\nPuRd             9      seq       TRUE\nPurples          9      seq       TRUE\nRdPu             9      seq       TRUE\nReds             9      seq       TRUE\nYlGn             9      seq       TRUE\nYlGnBu           9      seq       TRUE\nYlOrBr           9      seq       TRUE\nYlOrRd           9      seq       TRUE\n\n\nIt is very helpful to look at the different kinds of data palettes available and I’ll show you how to use them below when we color in the states based upon population size at the end of this document."
  },
  {
    "objectID": "classic_graphics.html#annotations",
    "href": "classic_graphics.html#annotations",
    "title": "9  Basic Graphics in R",
    "section": "9.4 Annotations",
    "text": "9.4 Annotations\nYou can easily add text onto a graph using the text() function. Here is the correlation between the sepal length and width (the function cor.test() does the statistical test).\n\ncor &lt;- cor.test( iris$Sepal.Length, iris$Sepal.Width )\ncor\n\n\n    Pearson's product-moment correlation\n\ndata:  iris$Sepal.Length and iris$Sepal.Width\nt = -1.4403, df = 148, p-value = 0.1519\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.27269325  0.04351158\nsample estimates:\n       cor \n-0.1175698 \n\n\nWe can put the correlation and the p-value on the plot\n\ncor.text &lt;- paste( \"r = \", format( cor$estimate, digits=4), \"; P = \", format( cor$p.value, digits=4 ), sep=\"\" ) \ncor.text\n\n[1] \"r = -0.1176; P = 0.1519\"\n\n\nThe we can the overlay this onto an existing plot. For the text() function, we need to give the x- and y- coordinates where you want it put onto the coordinate space of the existing graph.\n\nplot( y ~ x , \n      col=colors, \n      pch=20, \n      bty=\"n\", \n      xlab=\"Sepal Length\", ylab=\"Sepal Width\")\ntext( 7.4, 4.2, cor.text )"
  },
  {
    "objectID": "ggplot_graphics.html#basic-ggplot",
    "href": "ggplot_graphics.html#basic-ggplot",
    "title": "10  ggplot2 Graphics",
    "section": "10.1 Basic ggplot",
    "text": "10.1 Basic ggplot\nAs outlined above, the basis of this appraoch is an additive (and iterative) process of creating a graphic. This all starts with the data. For our purposes, we will use the same iris data.frame as in the previous section on base graphics.\n\n\n\nThe iris data\n\n\n\nsummary( iris )\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n\n\nWe start building a graphic using the ggplot() function and passing it the data.frame object. This will initialize the graphic, though it will not plot anything.\n\nlibrary(ggplot2)\n\nggplot( iris )\n\n\n\n\nNext, we need to tell the plot which variables it will be using from the data.frame. For simplicity, we do not need to make special data objects with just the variables we want to plot, we can pass around the whole data.frame object and just indicate to ggplot which ones we want to use by specifying the aesthetics to be used.\n\nggplot( iris , aes( x=Sepal.Length ) )\n\n\n\n\nAt this point, there is enough information to make an axis in the graph because the underlying data has been identified. What has not been specified to date is the way in which we want to represent the data. To do this, we add geometries to the graph. In this case, I’m going to add a histogram\n\nggplot( iris, aes(x=Sepal.Length) ) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nNow we have a base graph!"
  },
  {
    "objectID": "ggplot_graphics.html#aestheics-and-scope",
    "href": "ggplot_graphics.html#aestheics-and-scope",
    "title": "10  ggplot2 Graphics",
    "section": "10.2 Aestheics and Scope",
    "text": "10.2 Aestheics and Scope\nThe location of the data and the aes() determines the scope of the assignment. What I mean by this is:\n\nIf the data and aes() is in the the ggplot() function, then everything in the whole plot inherits that assignment.\nIf you put them in one or more of the components you add to ggplot() then the they are localized to only those layers.\n\nSo the following statements are all identical for this most basic of plots.\n\nggplot( iris, aes(x=Sepal.Length) ) + geom_histogram()\nggplot( iris ) + geom_historgram( aes(x=Sepal.Length) )\nggplot() + geom_histogram( aes(x=Sepal.Length), data=iris)\n\n\nIn the first case, the geom_histogram() inherits both data and aesthetics from ggplot().\n\nIn the second one, it inherits only the data but has it’s own specification for aesthetics.\nIn the last one, ggplot() only specifies the presence of a graph and all the data and aesthetics are localized within geom_histogram() function.\n\nWhere this becomes important is when we want to make more complicated graphics like the one above. The data that has the country CDI and HDI also has the names of the countries. However, only a subset of the country names are plot. This is because both the geometric layer and the text layer that has the names are using different data.frame objects.\nHere is a more simplistic example where I overlay a density plot (as a red line) on top of the histogram.\n\nggplot( iris, aes(x=Sepal.Length) ) + geom_histogram() + geom_density( col=\"red\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nBoth the geom_histogram and the geom_density use the same data and same specification for how to deal with the y-axis. However, the density is depicted as a frequency on the y-axis whereas the histogram uses counts. Also notice how the col=\"red\" is localized just for the geom_density() layer.\nWe can override the way in which geom_histogram uses the y-axis by changing the aesthetics for that particular geometric layer. Here, I’m goint to add another aes() just within the geom_histogram() function and have it treat y as the density rather than the count (yes that is two periods before and after the word density).\n\nggplot( iris, aes(x=Sepal.Length) ) + geom_histogram(aes(y=..density..)) + geom_density( col=\"red\" )\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nBy default, everything inside the ggplot() function call is inherited by all the remaining components unless it is specifically overridden. Here is a more pedantic version where only the raw data.frame is in the ggplot and the rest is in each of the geometric layers.\n\nggplot( iris ) + \n  geom_histogram( aes(x=Sepal.Length, y=..density..) ) + \n  geom_density( aes(x=Sepal.Length), col=\"red\", lwd=2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "ggplot_graphics.html#labels-titles",
    "href": "ggplot_graphics.html#labels-titles",
    "title": "10  ggplot2 Graphics",
    "section": "10.3 Labels & Titles",
    "text": "10.3 Labels & Titles\nJust like we added geometric layers to the plot to make histograms and densities, we do the same for labels and titles.\n\nggplot( iris,  aes(x=Sepal.Length) ) + \n  geom_histogram( aes(y=..density..), bins = 10, fill=\"lightgray\", col=\"darkgrey\" ) + \n  geom_density( col=\"red\", lwd=1.5) + \n  xlab(\"Length\") + ylab(\"Density\") + \n  ggtitle(\"Sepal Lengths for Three Iris Species\")"
  },
  {
    "objectID": "ggplot_graphics.html#scatter-plots",
    "href": "ggplot_graphics.html#scatter-plots",
    "title": "10  ggplot2 Graphics",
    "section": "10.4 Scatter Plots",
    "text": "10.4 Scatter Plots\nWith two columns of data, we can make the old scatter plot using the geom_point() function.\n\nggplot( iris, aes(x=Sepal.Length, y=Sepal.Width) ) + geom_point( col=\"purple\") \n\n\n\n\nIn this plot, we are hiding some of the information by having all the points be the same color and shape. We could have a geom_point for each species as follows:\n\nggplot(  ) + \n  geom_point( aes( x = Sepal.Length, y = Sepal.Width), data=iris[ 1:50,], col=\"red\") + \n  geom_point( aes( x = Sepal.Length, y = Sepal.Width), data=iris[ 51:100,], col=\"yellow\" ) + \n  geom_point( aes( x = Sepal.Length, y = Sepal.Width), data=iris[ iris$Species == \"virginica\", ], col=\"darkgreen\" ) \n\n\n\n\nBut that is a lot of typing. In cases like this, where there is a an actual column of data that we want to use to change the appearance (e.g., in this case the Species column), we can put this within the aes() directly and ggplot() will handle the specifics for you. Anything we do to reduce the amount of typing we must do is going to help us be more accurate analysts.\n\nggplot( iris, aes( x = Sepal.Length, y = Sepal.Width, col=Species) ) + geom_point()\n\n\n\n\n\n10.4.1 In or Out of aes()\nNotice in the last graph I put the name of the data column in the aesthetic but have the color (col) within the aes() function call in the graph before that, I put color outside of the aes() in the geom_point() function. What gives? Here is a simple rule.\n\nIf information from within the data.frame is needed to customize the display of data then it must be designated within the aes(), whereas if the display of the data is to be applied to the entire geometric layer, it is specified outside of the aes() call.\n\nHere is an example, where I have the color of the shapes determined by a value in the data.frame but have the shape2 applied to all the points, independent of any data in the data.frame.\n\nggplot( iris ) + geom_point(aes( x = Sepal.Length, y = Sepal.Width, col=Species), shape=5)\n\n\n\n\nWe can build these things in an iterative fashion making things easier to read. In what follows I will use the basic plot from above but assign it to the variable p as I add things to it. It can be as iterative as you like and you can add a bunch of stuff and wait until the end to display it.\n\np &lt;- ggplot( iris ) \np &lt;- p + geom_point(aes( x = Sepal.Length, y = Sepal.Width, col=Species, shape=Species), size=3, alpha=0.75 ) \np &lt;- p + xlab(\"Sepal Length\") \np &lt;- p + ylab(\"Sepal Width\")\n\nThe overall class of the plot varible is\n\nclass(p)\n\n[1] \"gg\"     \"ggplot\"\n\n\nAnd there is no plot output until we display it specifically.\n\np"
  },
  {
    "objectID": "ggplot_graphics.html#themes",
    "href": "ggplot_graphics.html#themes",
    "title": "10  ggplot2 Graphics",
    "section": "10.5 Themes",
    "text": "10.5 Themes\nThe overall coloration of the plot is determined by the theme.\n\np + theme_bw()\n\n\n\n\n\np + theme_dark()\n\n\n\n\n\np + theme_minimal()\n\n\n\n\n\np + theme_linedraw()\n\n\n\n\n\np + theme_void()\n\n\n\n\nYou can even define your own themes to customize all the text and lines.\nOne thing that I like to do is to specify a default theme for all my plots. You can accomplish this using theme_set() and from this point forward, this theme will be used as the default (again, we need to try as hard as possible to minimzie the amount of typing we do to minimize the amount of mistakes we make).\n\ntheme_set( theme_bw() )"
  },
  {
    "objectID": "ggplot_graphics.html#boxplots",
    "href": "ggplot_graphics.html#boxplots",
    "title": "10  ggplot2 Graphics",
    "section": "10.6 Boxplots",
    "text": "10.6 Boxplots\n\nggplot( iris, aes( x = Sepal.Length) ) + geom_boxplot( notch=TRUE )\n\n\n\n\n\nggplot( iris, aes(x=Species, y=Sepal.Length) )  + geom_boxplot( notch=TRUE )"
  },
  {
    "objectID": "ggplot_graphics.html#overlays",
    "href": "ggplot_graphics.html#overlays",
    "title": "10  ggplot2 Graphics",
    "section": "10.7 Overlays",
    "text": "10.7 Overlays\nJust like in the previous\n\np &lt;- ggplot( iris, aes(Sepal.Length, Sepal.Width) ) + \n  geom_point(col=\"red\") + \n  xlab(\"Sepal Length\") + \n  ylab(\"Sepal Width\")\n\nThe order by which you add the components to the ggplot() will determine the order of the layers from bottom to top—the. Layers added earlier will be covered by content in layers that are added later. Compare the following plot that takes the length and width of the sepals and overlays a linear regression line over the top.\n\np + geom_point(col=\"red\") + \n  stat_smooth( formula = y ~ x, method=\"lm\", alpha=1.0)\n\n\n\n\nCompare that plot to the one below. Notice how puting stat_smooth() in front of the call to geom_point() layes the regression smoothing line and error zone underneath the points.\n\np + stat_smooth(formula = y ~ x, method=\"lm\", alpha=1.0) + \n  geom_point(col=\"red\")"
  },
  {
    "objectID": "ggplot_graphics.html#labeling",
    "href": "ggplot_graphics.html#labeling",
    "title": "10  ggplot2 Graphics",
    "section": "10.8 Labeling",
    "text": "10.8 Labeling\nWe can create two kinds of annotations, text on the raw graph and text associated with some of the points. Labels of the first kind can be added direclty by placing raw data inside the aes() function.\nI’ll start by taking the correlation between sepal width and length.\n\ncor &lt;- cor.test( iris$Sepal.Length, iris$Sepal.Width )\ncor\n\n\n    Pearson's product-moment correlation\n\ndata:  iris$Sepal.Length and iris$Sepal.Width\nt = -1.4403, df = 148, p-value = 0.1519\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.27269325  0.04351158\nsample estimates:\n       cor \n-0.1175698 \n\n\nAnd then grab the raw data from it and make a message.\n\ncor.text &lt;- paste( \"r = \", format( cor$estimate, digits=4), \"; P = \", format( cor$p.value, digits=4 ), sep=\"\" ) \ncor.text\n\n[1] \"r = -0.1176; P = 0.1519\"\n\n\nThat I’ll stick onto the graph directly\n\np + geom_text( aes(x=7.25, y=4.25, label=cor.text))\n\n\n\n\nAlternatively, we may want to label specific points. Here I find the mean values for each species.\n\nmean_Length &lt;- by( iris$Sepal.Length, iris$Species, mean, simplify = TRUE)\nmean_Width &lt;- by( iris$Sepal.Width, iris$Species, mean, simplify = TRUE)\nmean_Values &lt;- data.frame(  Species = levels( iris$Species), \n                            Sepal.Length = as.numeric( mean_Length ), \n                            Sepal.Width = as.numeric( mean_Width ) ) \nmean_Values\n\n     Species Sepal.Length Sepal.Width\n1     setosa        5.006       3.428\n2 versicolor        5.936       2.770\n3  virginica        6.588       2.974\n\n\nTo plot and label these mean values, I’m going to use two steps. First, since I named the columns of the new data.frame the same as before, we can just inherit the aes() but substitute in this new data.frame and add label=Species to the the aesthetics.\n\np + geom_text( data=mean_Values, aes(label=Species) )\n\n\n\n\nBut that is a bit messy. Here is a slick helper library for that that will try to minimize the overlap.\n\nlibrary( ggrepel ) \np + geom_label_repel( data=mean_Values, aes(label=Species) )\n\n\n\n\nSlick."
  },
  {
    "objectID": "ggplot_graphics.html#questions",
    "href": "ggplot_graphics.html#questions",
    "title": "10  ggplot2 Graphics",
    "section": "10.9 Questions",
    "text": "10.9 Questions\nIf you have any questions for me specifically on this topic, please post as an Issue in your repository, otherwise consider posting to the discussion board on Canvas."
  },
  {
    "objectID": "ggplot_graphics.html#footnotes",
    "href": "ggplot_graphics.html#footnotes",
    "title": "10  ggplot2 Graphics",
    "section": "",
    "text": "Literally, we add these toghter using the plus ‘+’ sign just like we were going to develop an equation.↩︎\nThe shapes are the same as the pch offerings covered in the lecture on graphing using Base R routines here.↩︎"
  },
  {
    "objectID": "tidyverse.html#the-tidyverse-approach",
    "href": "tidyverse.html#the-tidyverse-approach",
    "title": "11  Tidyverse",
    "section": "11.1 The Tidyverse Approach",
    "text": "11.1 The Tidyverse Approach\nThis is the first introduction to tidyverse and is the key skill necessary to become proficient at data analysis.\n\nlibrary( tidyverse )\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary( lubridate )\n\n\n11.1.1 The Datat\nFor this topic we will use some example data from the Rice Rivers Center. These data represent both atmospheric and water data collected from instrumentation on-site. I have stored these data in a spreadsheet that is shared on Google Drive as a CSV file.\nYou can look at it here.\n\n\n11.1.2 The Data in R\nSo let’s load it into memory and take a look at it.\n\nurl &lt;- \"https://docs.google.com/spreadsheets/d/1Mk1YGH9LqjF7drJE-td1G_JkdADOU0eMlrP01WFBT8s/pub?gid=0&single=true&output=csv\"\nrice &lt;- read_csv( url )\n\nRows: 8199 Columns: 23\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): DateTime\ndbl (22): RecordID, PAR, WindSpeed_mph, WindDir, AirTempF, RelHumidity, BP_H...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsummary( rice )\n\n   DateTime            RecordID          PAR           WindSpeed_mph   \n Length:8199        Min.   :43816   Min.   :   0.000   Min.   : 0.000  \n Class :character   1st Qu.:45866   1st Qu.:   0.000   1st Qu.: 2.467  \n Mode  :character   Median :47915   Median :   0.046   Median : 4.090  \n                    Mean   :47915   Mean   : 241.984   Mean   : 5.446  \n                    3rd Qu.:49964   3rd Qu.: 337.900   3rd Qu.: 7.292  \n                    Max.   :52014   Max.   :1957.000   Max.   :30.650  \n                                                                       \n    WindDir          AirTempF       RelHumidity        BP_HG      \n Min.   :  0.00   Min.   : 3.749   Min.   :15.37   Min.   :29.11  \n 1st Qu.: 37.31   1st Qu.:31.545   1st Qu.:42.25   1st Qu.:29.87  \n Median :137.30   Median :37.440   Median :56.40   Median :30.01  \n Mean   :146.20   Mean   :38.795   Mean   :58.37   Mean   :30.02  \n 3rd Qu.:249.95   3rd Qu.:46.410   3rd Qu.:76.59   3rd Qu.:30.21  \n Max.   :360.00   Max.   :74.870   Max.   :93.00   Max.   :30.58  \n                                                                  \n    Rain_in            H2O_TempC       SpCond_mScm      Salinity_ppt   \n Min.   :0.0000000   Min.   :-0.140   Min.   :0.0110   Min.   :0.0000  \n 1st Qu.:0.0000000   1st Qu.: 3.930   1st Qu.:0.1430   1st Qu.:0.0700  \n Median :0.0000000   Median : 5.450   Median :0.1650   Median :0.0800  \n Mean   :0.0008412   Mean   : 5.529   Mean   :0.1611   Mean   :0.0759  \n 3rd Qu.:0.0000000   3rd Qu.: 7.410   3rd Qu.:0.1760   3rd Qu.:0.0800  \n Max.   :0.3470000   Max.   :13.300   Max.   :0.2110   Max.   :0.1000  \n                     NA's   :1        NA's   :1        NA's   :1       \n       PH           PH_mv        Turbidity_ntu       Chla_ugl    \n Min.   :6.43   Min.   :-113.8   Min.   :  6.20   Min.   :  1.3  \n 1st Qu.:7.50   1st Qu.: -47.8   1st Qu.: 15.50   1st Qu.:  3.7  \n Median :7.58   Median : -43.8   Median : 21.80   Median :  6.7  \n Mean   :7.60   Mean   : -44.5   Mean   : 24.54   Mean   :137.3  \n 3rd Qu.:7.69   3rd Qu.: -38.9   3rd Qu.: 30.30   3rd Qu.:302.6  \n Max.   :9.00   Max.   :  28.5   Max.   :187.70   Max.   :330.1  \n NA's   :1      NA's   :1        NA's   :1        NA's   :1      \n   BGAPC_CML        BGAPC_rfu         ODO_sat         ODO_mgl     \n Min.   :   188   Min.   :  0.10   Min.   : 87.5   Min.   :10.34  \n 1st Qu.:   971   1st Qu.:  0.50   1st Qu.: 99.2   1st Qu.:12.34  \n Median :  1369   Median :  0.70   Median :101.8   Median :12.88  \n Mean   :153571   Mean   : 72.91   Mean   :102.0   Mean   :12.88  \n 3rd Qu.:345211   3rd Qu.:163.60   3rd Qu.:104.1   3rd Qu.:13.34  \n Max.   :345471   Max.   :163.70   Max.   :120.8   Max.   :14.99  \n NA's   :1        NA's   :1        NA's   :1       NA's   :1      \n    Depth_ft        Depth_m      SurfaceWaterElev_m_levelNad83m\n Min.   :12.15   Min.   :3.705   Min.   :-32.53                \n 1st Qu.:14.60   1st Qu.:4.451   1st Qu.:-31.78                \n Median :15.37   Median :4.684   Median :-31.55                \n Mean   :15.34   Mean   :4.677   Mean   :-31.55                \n 3rd Qu.:16.12   3rd Qu.:4.913   3rd Qu.:-31.32                \n Max.   :17.89   Max.   :5.454   Max.   :-30.78                \n                                                               \n\n\nThese data represent measurements taken every 15 minutes, 24 hours a day, 7 days a week, 365 days a year. For brevity, this file contains measurements starting at 1/1/2014 12:00:00 AM and ending at 3/27/2014 9:30:00 AM (only 8199 records here…).\nIf you look at the summary of the data above, you will see several things, including:\n\nDate and time objects are character\nSome measurements are in Standard and some in Imperial with units in the same file include both °F and °C, as well as measurements in meters, feet, and inches. In fact, there are duplication of data columns in different units (guess what kind of correlation they might have…)"
  },
  {
    "objectID": "tidyverse.html#verbs-of-analysis",
    "href": "tidyverse.html#verbs-of-analysis",
    "title": "11  Tidyverse",
    "section": "11.2 Verbs of Analysis",
    "text": "11.2 Verbs of Analysis\nWhen we perform any type of data manipulation, we use specific verbs. There is a limited lexicon for us to use, but the key here is how we perform these actions, and in which order they are deployed for a huge diversity in outcomes. For now, these basic verbs include:\n\nSelect: Used to grab or reorder columns of data.\nFilter: Used to grab subsets of records (rows) based upon some criteria.\nMutate: Create new columns of data based upon manipulations of existing columns.\nArrange: Order the records (rows) based upon some criteria.\nGroup: Gather records together to perform operations on chunks of them similar to by().\nSummarize: Extract summaries of data (or grouped data) based upon some defined criteria.\n\nIn the following examples, we’ll be using the rice data above. For each verb, I’m going to use the pipe operator (%&gt;%) to send the data into the example functions and then assign the result to a dummy data.frame named df. The arguments passed to each of the verbs are where the magic happens.\n\n11.2.1 The Output\nThe key to these activities is that every one of these functions takes a data.frame as input, does its operations on it, then return a data.frame object as output. The data.frame is the core data container for all of these actions.\n\n\n11.2.2 Select Operator\nThe select() function allows you to choose which columns of data to work with.\n\nrice %&gt;%\n  select( DateTime, AirTempF ) -&gt; df \nhead(df)\n\n# A tibble: 6 × 2\n  DateTime             AirTempF\n  &lt;chr&gt;                   &lt;dbl&gt;\n1 1/1/2014 12:00:00 AM     31.0\n2 1/1/2014 12:15:00 AM     30.7\n3 1/1/2014 12:30:00 AM     31.2\n4 1/1/2014 12:45:00 AM     30.5\n5 1/1/2014 1:00:00 AM      30.9\n6 1/1/2014 1:15:00 AM      30.6\n\n\nSelect can also be used to reorder the columns in a data.frame object. Here are the names of the data columns as initially loaded.\n\nnames( rice )\n\n [1] \"DateTime\"                       \"RecordID\"                      \n [3] \"PAR\"                            \"WindSpeed_mph\"                 \n [5] \"WindDir\"                        \"AirTempF\"                      \n [7] \"RelHumidity\"                    \"BP_HG\"                         \n [9] \"Rain_in\"                        \"H2O_TempC\"                     \n[11] \"SpCond_mScm\"                    \"Salinity_ppt\"                  \n[13] \"PH\"                             \"PH_mv\"                         \n[15] \"Turbidity_ntu\"                  \"Chla_ugl\"                      \n[17] \"BGAPC_CML\"                      \"BGAPC_rfu\"                     \n[19] \"ODO_sat\"                        \"ODO_mgl\"                       \n[21] \"Depth_ft\"                       \"Depth_m\"                       \n[23] \"SurfaceWaterElev_m_levelNad83m\"\n\n\nLet’s say that you wanted to reorder the columns as RecordID, ODO_mgl and PH as the first three columns and leave everything else as is. There is this cool function everthying() that helps out.\n\nrice %&gt;%\n  select( RecordID, ODO_mgl, PH, everything() ) -&gt; df\nnames( df )\n\n [1] \"RecordID\"                       \"ODO_mgl\"                       \n [3] \"PH\"                             \"DateTime\"                      \n [5] \"PAR\"                            \"WindSpeed_mph\"                 \n [7] \"WindDir\"                        \"AirTempF\"                      \n [9] \"RelHumidity\"                    \"BP_HG\"                         \n[11] \"Rain_in\"                        \"H2O_TempC\"                     \n[13] \"SpCond_mScm\"                    \"Salinity_ppt\"                  \n[15] \"PH_mv\"                          \"Turbidity_ntu\"                 \n[17] \"Chla_ugl\"                       \"BGAPC_CML\"                     \n[19] \"BGAPC_rfu\"                      \"ODO_sat\"                       \n[21] \"Depth_ft\"                       \"Depth_m\"                       \n[23] \"SurfaceWaterElev_m_levelNad83m\"\n\n\n\n\n11.2.3 Filter\nThe function filter() works to select records (rows) based upon some criteria. So for example, if I am interested in just records when the airtemp was freezing (and the raw data are in °F). The range of values in the original data was:\n\nrange( rice$AirTempF )\n\n[1]  3.749 74.870\n\n\nbut after filtering using the name of the variable and a logical operator.\n\nrice %&gt;%\n  filter( AirTempF &lt; 32 ) -&gt; df\nrange( df$AirTempF )\n\n[1]  3.749 31.990\n\n\nJust like select(), it is possible to have several conditions, that are compounded (using a logical AND operator) by adding them to the filter() function. Here I also split the conditionals requiring the data to be above freezing air temperatures, not missing data from the PH meter, and water turbidity &lt; 15 ntu’s. I also put each of these onto their own lines and auto-indent does a great job of making it reasonably readable.\n\nrice %&gt;%\n  filter( AirTempF &gt; 32, \n          !is.na(PH), \n          Turbidity_ntu &lt; 15) -&gt; df\nnrow(df)\n\n[1] 1449\n\n\n\n\n11.2.4 Mutate\nThe mutate() function changes values in the table and is quite versatile. Here I will jump back to our old friend mdy_hms() from lubridate and convert the DateTime column, which is\n\nclass( rice$DateTime )\n\n[1] \"character\"\n\n\nand convert it into a real date and time object\n\nrice %&gt;%\n  mutate( Date = mdy_hms(DateTime, tz = \"EST\") ) -&gt; df\nclass( df$Date )\n\n[1] \"POSIXct\" \"POSIXt\" \n\nsummary( df$Date )\n\n                 Min.               1st Qu.                Median \n\"2014-01-01 00:00:00\" \"2014-01-22 08:22:30\" \"2014-02-12 16:45:00\" \n                 Mean               3rd Qu.                  Max. \n\"2014-02-12 16:45:00\" \"2014-03-06 01:07:30\" \"2014-03-27 09:30:00\" \n\n\nYou can also create several mutations in one mutation step.\n\nrice %&gt;%\n  mutate( Date = mdy_hms(DateTime, tz = \"EST\"), \n          Month = month(Date, label = TRUE) ) -&gt; df\nsummary( df$Month )\n\n Jan  Feb  Mar  Apr  May  Jun  Jul  Aug  Sep  Oct  Nov  Dec \n2976 2688 2535    0    0    0    0    0    0    0    0    0 \n\n\n\n\n11.2.5 Arrange\nWe can sort entire data.frame objects based upon the values in one or more of the columns using the arrange() function.\n\nrice %&gt;%\n  arrange( WindSpeed_mph ) -&gt; df \ndf$WindSpeed_mph[1]\n\n[1] 0\n\n\nBy default, it is in ascending order, to reverse it, use the negative operator on the column name object in the function.\n\nrice %&gt;%\n  arrange( -WindSpeed_mph ) -&gt; df \ndf$WindSpeed_mph[1]\n\n[1] 30.65\n\n\nAs above, it is possible to combine many columns of data as criteria for sorting by adding more arguments to the function call.\n\nrice %&gt;%\n  arrange( -WindSpeed_mph, WindDir ) -&gt; df\n\n\n\n11.2.6 Summarise\nThis function is the first one that does not return some version of the original data that was passed to it. Rather, this performs operations on the data and makes a brand new data.frame object.\nEach argument you give to the function performs one or more operations on the data and returns a brand new data.frame object with only the the values specified.\nHere is an example where I am taking the mean air and water temperature (n.b., one is in °F and the other is in °C). Notice the result is a new data.frame object with one row and two new columns defined by how I asked for the summary in the first place. I used single tick notation so I can have a space in the column names.\n\nrice %&gt;%\n  summarize( `Air Temp` = mean( AirTempF), \n             `Water Temp` = mean(H2O_TempC, na.rm=TRUE))\n\n# A tibble: 1 × 2\n  `Air Temp` `Water Temp`\n       &lt;dbl&gt;        &lt;dbl&gt;\n1       38.8         5.53\n\n\n\n\n11.2.7 Group & Summarize\nTo get more than one row in the resulting data.frame from summary(), we need to group the data in some way. The function group_by() does this and is used prior to summary(). Let’s take a look at how we can get the average air and water temp by month. To do this, I’m going to have to do several steps. I’m just going to chain them together using the %&gt;% operator.\n\nrice %&gt;%\n  mutate( Date = mdy_hms( DateTime, \n                          tz=\"EST\"),\n          Month = month( Date, \n                         abbr = FALSE, \n                         label=TRUE) ) %&gt;%\n  group_by( Month ) %&gt;%\n  summarize( `Air Temp` = mean( AirTempF), \n             `Water Temp` = mean( H2O_TempC, \n                                  na.rm=TRUE) )\n\n# A tibble: 3 × 3\n  Month    `Air Temp` `Water Temp`\n  &lt;ord&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1 January        34.7         3.68\n2 February       39.7         5.29\n3 March          42.6         7.96\n\n\nAs you read the code, notice how easy it is to understand what is going on because of both the pipes and because of the way I am formatting the code itself."
  },
  {
    "objectID": "tidyverse.html#flows",
    "href": "tidyverse.html#flows",
    "title": "11  Tidyverse",
    "section": "11.3 Flows",
    "text": "11.3 Flows\nThis last part really showed off the process of multi-step data manipulations using the pipe operator and the several verbs we introduced. These are both efficient in terms of typing as well as efficient in the way of producing research that makes sense to look at.\nHere are some strategies that I use when building up these manipulation workflows.\n\nDo not think that you have to do the whole thing at once. I typically build up the workflow, one line at a time. Make sure the output from the previous line is what you think it should be then add the next one.\nKeep your code open and airy, it makes it easier to read and to catch any logical errors that may arrise.\nYou can pipe into a lot of different functions. In fact, any function that takes a data frame can be the recipient of a pipe. While developing a workflow, I will often pipe into things like head(), summary(), or View() to take a look at what is coming out of my workflow to make sure it resembles what I think it should look like."
  },
  {
    "objectID": "tidyverse.html#questions",
    "href": "tidyverse.html#questions",
    "title": "11  Tidyverse",
    "section": "11.4 Questions",
    "text": "11.4 Questions\nIf you have any questions for me specifically on this topic, please post as an Issue in your repository, otherwise consider posting to the discussion board on Canvas."
  },
  {
    "objectID": "functions.html",
    "href": "functions.html",
    "title": "12  Functions",
    "section": "",
    "text": "13 Creating Functions\nYou can create functions for small things to be used in a single document or they can be larger more general functions that can be used all the time.\nIf you are going to be using a function in a single markdown document, define it in its own code chunk and then from that point down the document, it will be available to use (like we’ve done in this document).\nHowever, if you are going to be calling a function from more than one sole Markdown document, it is probably good practice to put it in its own file. R script files contain ONLY code and this is where you should put it.\nMake a new R Script file by selecting File -&gt; New File -&gt; R Script.\nAs an example, I made entered the code shown below into this script file and then saved it as summarize_levels.R in the same directory as my project (this last part is important).\nThis code has a few sections to it. The top 9 rows are comments. These kinds of comments are denoted by a hashtag and a single quote. You do not need to have these comments in the file but when you start making a lot of function, each in their file, if you follow these instructions you can autogenerate the R help files so you (and others who may be using your code) can look at the help file.\nSince the function is located in another file, we need to ask R to load in the source of the code. This is done using the source() function.\nsource(\"summarize_levels.R\")\nDo this once and it will load the function in the current Global Environment.\nsummarize_levels( iris, \"Sepal.Length\", \"Species\", mean )\n\n     Species Sepal.Length\n1     setosa        5.006\n2 versicolor        5.936\n3  virginica        6.588\nOnce you a few functions together, you can put them together into a library (left as an advanced topic)."
  },
  {
    "objectID": "functions.html#a-basic-function",
    "href": "functions.html#a-basic-function",
    "title": "12  Functions",
    "section": "12.1 A Basic Function",
    "text": "12.1 A Basic Function\nA function is just a chunck of code, which is wrapped up in a block and given a variable name.\n\nfoo &lt;- function() { \n  cat(\"bar\")\n}\n\nfoo()\n\nbar\n\n\nThe amount of code within a function can be simple like the one above or quite complex. The boundaries of the code are defined by the curly brackets."
  },
  {
    "objectID": "functions.html#variable-scope",
    "href": "functions.html#variable-scope",
    "title": "12  Functions",
    "section": "12.2 Variable Scope",
    "text": "12.2 Variable Scope\nWhen we make a function, there is a notion of a scope for variables, which defines where variables are visible from. By default, when you start R you are given a Global Environment Scope that has all the variables and functions you’ve defined thus far. The image below is the one for this document at this stage of development.\n\n\n\nFigure 1: Main Environment in RStudio\n\n\nWhen we work with functions, we encapsulate code within curly-brackets. This protects their scope. Her is an example. In this function, we:\n\nPrint out the value of a variable x\n\nAssign values to the variables x and z\nPrint out the value of the variables x and z.\n\n\nfoo &lt;- function( ) {\n  x &lt;- 12\n  z &lt;- \"bob\"\n  cat(\"x =\", x, \"& z =\", z ,\"inside function.\\n\")\n}\n\nOK, so now let’s call this function.\n\nfoo()\n\nx = 12 & z = bob inside function.\n\n\n\nx &lt;- 42\ncat(\"x =\", x, \"before function.\\n\")\n\nx = 42 before function.\n\nfoo()\n\nx = 12 & z = bob inside function.\n\ncat(\"x =\", x, \"after running function.\\n\")\n\nx = 42 after running function.\n\n\nNOTE: The value of x was changed within the function but those changes were not reflected OUTSIDE of that function. The scope of the variable x inside foo() is local to that function and anything that follows its declaration within the curly brackets of the function. However, it is invisible outside the scope of that function. This is a ‘good thing©’ because if we had visibility of all the variables in all the functions then we would either a) quickly run out of variable names to keep them unique, or b) clobber all of our existing variables by writing over them and changing their values.\nAlso, notice that the variable z that is assigned bob in the function is also not visible in the global environment. What happens in the function, stays in the function.\n\nls()\n\n[1] \"foo\"             \"has_annotations\" \"x\""
  },
  {
    "objectID": "functions.html#passing-variables.",
    "href": "functions.html#passing-variables.",
    "title": "12  Functions",
    "section": "12.3 Passing Variables.",
    "text": "12.3 Passing Variables.\nWhile some functions do not take any input, most require some kind of data to work with or values to start using. These variables can are passed into the function code by including them within the function parentheses.\nAny required variables are added within the function definition parentheses. These translate into the names of the variables used within the chunk.\nHere is an example with one required variable, x.\n\nfoo &lt;- function( x ) {\n  print(x)\n}\n\nAnd it can be called by either naming the variable explicity or not.\n\nfoo( x = 23 )\n\n[1] 23\n\nfoo( 42 )\n\n[1] 42\n\n\nHowever, if you require a variable to be passed and it is not given, it will result in an error.\n\nfoo()\n\nError in foo(): argument \"x\" is missing, with no default\n\n\nYou can get around this by making a default value for the variable, which is specified in the function definition as follows:\n\nfoo &lt;- function( x = \"Dr Dyer is my favorite professor\" ) {\n  print(x)\n}\n\nThen if the individual does not fill in\n\nfoo()\n\n[1] \"Dr Dyer is my favorite professor\""
  },
  {
    "objectID": "functions.html#retrieving-results-from-functions",
    "href": "functions.html#retrieving-results-from-functions",
    "title": "12  Functions",
    "section": "12.4 Retrieving Results from Functions",
    "text": "12.4 Retrieving Results from Functions\nSimilarly, many functions we write will return something to the user who is calling it. By default, a function that just does something like print some message or make some plot will return NULL\n\nfoo &lt;- function( name = \"Alice\") {\n  cat(name, \"is in the house.\")\n}\nfoo()\n\nAlice is in the house.\n\n\nBut if I try to assign a variable the results of the function, I get NULL as the value returned.\n\nx &lt;- foo()\n\nAlice is in the house.\n\nclass(x)\n\n[1] \"NULL\"\n\nx\n\nNULL\n\n\nIf you want to return something to the user, you need to be explicit and use the return() function to pass back the variable.\n\nfoo &lt;- function( name = \"Alice\") {\n  response &lt;- paste( name, \"is in the house.\")\n  return( response )\n}\n\n\nwho_is_in_the_house &lt;- foo()\nwho_is_in_the_house\n\n[1] \"Alice is in the house.\"\n\n\nYou can only return one item but it can be a list a data.frame or any other R object."
  },
  {
    "objectID": "joins.html",
    "href": "joins.html",
    "title": "13  Joins",
    "section": "",
    "text": "14 Joins\nWe can also use joins to filter values within one data.frame. Here the semi_join() keeps everything in the left data that has a key in the right one, but importantly it does not import the right data columns into the result.\ndf.X %&gt;%\n  semi_join( df.Y )\n\nJoining with `by = join_by(Key)`\n\n\n  Key X\n1   B 2\n2   C 3\nThe opposite of this is the anti_join() which drops everything in the left table that has a key in the right one, leaving only the ones that are unique.\ndf.X %&gt;%\n  anti_join( df.Y )\n\nJoining with `by = join_by(Key)`\n\n\n  Key X\n1   A 1"
  },
  {
    "objectID": "joins.html#keys",
    "href": "joins.html#keys",
    "title": "13  Joins",
    "section": "13.1 Keys",
    "text": "13.1 Keys\nAn important component of relational data are the keys. These are unique identifiers for a particular datum from a table. In each of these examples the variable (obviously named) Key is what is called a Primary Key because it uniquely identifies each row. You can verify this by counting the number of entries then filtering only for ones with 2 or more instances.\n\nlibrary( tidyverse )\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndf.X %&gt;%\n  count( Key ) %&gt;%\n  filter( n &gt; 1 )\n\n[1] Key n  \n&lt;0 rows&gt; (or 0-length row.names)\n\n\nNotice there is nothing here as each is unique.\n\nThe column Key is a Primary Key for the df.X data because it identifies a unique row in that table. In addition to a Primary Key we can have a Foreign Key when it is used to indicate data within a separate table. For example, if I am interested to see if the smallest value in df.X$X corresponds with the smallest value in df.Y$Y, then I will be using the Key form df.X representing max(X) to find the value of Y in df.Y and evaluate if it is max(Y). This means that df.X$Key is a Foreign Key as it points to a row in the df.Y data frame.\n\nThe keys are used to link together different tables."
  },
  {
    "objectID": "joins.html#left-join",
    "href": "joins.html#left-join",
    "title": "13  Joins",
    "section": "14.1 Left Join",
    "text": "14.1 Left Join\nA left join is one where all the data from the left data frame is in the result and the data whose keys in the right data frame are present in the left one are also included. Graphically, this leads to:\n\n\n\nleft join\n\n\nWhere in R we do this using the left_join() function.\n\ndf.X %&gt;%\n  left_join( df.Y, by=\"Key\")\n\n  Key X  Y\n1   A 1 NA\n2   B 2 10\n3   C 3 11"
  },
  {
    "objectID": "joins.html#right-join",
    "href": "joins.html#right-join",
    "title": "13  Joins",
    "section": "14.2 Right Join",
    "text": "14.2 Right Join\nThe right join does the same thing but keeps all the keys in the right data table and has missing data where the key in the left one is not in the right one.\n\n\n\nRight Join\n\n\nThis is accomplished using the right_join() function.\n\ndf.X %&gt;%\n  right_join( df.Y, by=\"Key\")\n\n  Key  X  Y\n1   B  2 10\n2   C  3 11\n3   D NA 12"
  },
  {
    "objectID": "joins.html#full-or-outer-join",
    "href": "joins.html#full-or-outer-join",
    "title": "13  Joins",
    "section": "14.3 Full (or Outer) Join",
    "text": "14.3 Full (or Outer) Join\nThis join is one where all the keys are retained adding missing data as necessary.\n\n\n\nOuter Join\n\n\n\ndf.X %&gt;%\n  full_join( df.Y, by=\"Key\")\n\n  Key  X  Y\n1   A  1 NA\n2   B  2 10\n3   C  3 11\n4   D NA 12"
  },
  {
    "objectID": "joins.html#inner-join",
    "href": "joins.html#inner-join",
    "title": "13  Joins",
    "section": "14.4 Inner Join",
    "text": "14.4 Inner Join\nThe last one retains only those keys that are common in both.\n\n\n\nInner Join\n\n\n\ndf.X %&gt;%\n  inner_join( df.Y, by=\"Key\")\n\n  Key X  Y\n1   B 2 10\n2   C 3 11"
  },
  {
    "objectID": "joins.html#questions",
    "href": "joins.html#questions",
    "title": "13  Joins",
    "section": "15.1 Questions",
    "text": "15.1 Questions\nIf you have any questions for me specifically on this topic, please post as an Issue in your repository, otherwise consider posting to the discussion board on Canvas."
  },
  {
    "objectID": "statistical_inferences.html",
    "href": "statistical_inferences.html",
    "title": "14  Statistical Inferences",
    "section": "",
    "text": "This presentation was entirely online and made in Keynote, not markdown. Here are each of the slides from that presentation as individual images."
  },
  {
    "objectID": "discrete_data.html#discrete-data",
    "href": "discrete_data.html#discrete-data",
    "title": "15  Contingency Analysisq",
    "section": "15.1 Discrete Data",
    "text": "15.1 Discrete Data\n\nFactors allow us to represent a type of data that exclusive and categorical. These data may, or may not, be ordered and in most cases, we can think of these kinds of data to represent things like, treatment levels, sampling locations, etc.\n\nWhile we’ve seen this already, perhaps we hadn’t know how it was working under the hood, so I’m going to start by using days of the week—generated by hand and not from the lubridate library as they are a good example of a data type that is discrete and exclusive (e.g., you cannot be in both Monday and Wednesday at the same time).\nIn R, character data can be turned into a new data type called a Factor making it something that in statistical analyses we can use to represent things like treatment levels. Initially, they start as a character data type.\n\nweekdays &lt;- c(\"Monday\",\"Tuesday\",\"Wednesday\",\n              \"Thursday\",\"Friday\",\"Saturday\", \n              \"Sunday\")\nclass( weekdays )\n\n[1] \"character\"\n\n\nLet’s assume I have sampled a random set of days (40 days in this case) as a data set I’ll use an example. In R, I’m just going to use the sample() function (to grab with replacement)\n\ndata &lt;- sample( weekdays, size=40, replace=TRUE)\ndata\n\n [1] \"Sunday\"    \"Thursday\"  \"Wednesday\" \"Wednesday\" \"Monday\"    \"Wednesday\"\n [7] \"Friday\"    \"Wednesday\" \"Saturday\"  \"Wednesday\" \"Saturday\"  \"Thursday\" \n[13] \"Thursday\"  \"Sunday\"    \"Sunday\"    \"Sunday\"    \"Saturday\"  \"Monday\"   \n[19] \"Monday\"    \"Wednesday\" \"Saturday\"  \"Sunday\"    \"Wednesday\" \"Thursday\" \n[25] \"Tuesday\"   \"Saturday\"  \"Wednesday\" \"Friday\"    \"Thursday\"  \"Sunday\"   \n[31] \"Wednesday\" \"Monday\"    \"Saturday\"  \"Thursday\"  \"Wednesday\" \"Wednesday\"\n[37] \"Tuesday\"   \"Thursday\"  \"Thursday\"  \"Wednesday\"\n\n\nThese data are still\n\nclass( data )\n\n[1] \"character\"\n\n\nIf I’d like to turn them into a factor, we use… factor()\n\ndays &lt;- factor( data )\nis.factor( days )\n\n[1] TRUE\n\nclass( days )\n\n[1] \"factor\"\n\n\nNow when we look at the data, it looks a lot like it did before except for the last line which shows you the unique levels for elements in the vector.\n\nsummary( days )\n\n   Friday    Monday  Saturday    Sunday  Thursday   Tuesday Wednesday \n        2         4         6         6         8         2        12 \n\n\nWe can put them into data frames and they know how to summarize themselves properly by counting the number of occurances of each level.\n\ndf &lt;- data.frame( ID = 1:40, Weekdays = days )\nsummary( df )\n\n       ID             Weekdays \n Min.   : 1.00   Friday   : 2  \n 1st Qu.:10.75   Monday   : 4  \n Median :20.50   Saturday : 6  \n Mean   :20.50   Sunday   : 6  \n 3rd Qu.:30.25   Thursday : 8  \n Max.   :40.00   Tuesday  : 2  \n                 Wednesday:12  \n\n\nAnd we can directly access the unique levels within a factor as:\n\nlevels( days )\n\n[1] \"Friday\"    \"Monday\"    \"Saturday\"  \"Sunday\"    \"Thursday\"  \"Tuesday\"  \n[7] \"Wednesday\"\n\n\nSo factors can be categorical (e.g., one is just different than the next) and compared via == and != values. They can also be ordinal such that &gt; and &lt; make sense.\nBy default, a factor is not ordered unless we specify it to be so.\n\nis.ordered( days )\n\n[1] FALSE\n\ndays[1] &lt; days[2]\n\nWarning in Ops.factor(days[1], days[2]): '&lt;' not meaningful for factors\n\n\n[1] NA\n\n\n\ndata &lt;- factor( days, ordered=TRUE )\ndata \n\n [1] Sunday    Thursday  Wednesday Wednesday Monday    Wednesday Friday   \n [8] Wednesday Saturday  Wednesday Saturday  Thursday  Thursday  Sunday   \n[15] Sunday    Sunday    Saturday  Monday    Monday    Wednesday Saturday \n[22] Sunday    Wednesday Thursday  Tuesday   Saturday  Wednesday Friday   \n[29] Thursday  Sunday    Wednesday Monday    Saturday  Thursday  Wednesday\n[36] Wednesday Tuesday   Thursday  Thursday  Wednesday\n7 Levels: Friday &lt; Monday &lt; Saturday &lt; Sunday &lt; Thursday &lt; ... &lt; Wednesday\n\n\nSo that if we go and try to order them, the only way they can be sorted is alphabetically (which is what R does internally when we run a summary() on the weekday stuff, it presentes the level counts in alphabetical order).\n\nsort( data )\n\n [1] Friday    Friday    Monday    Monday    Monday    Monday    Saturday \n [8] Saturday  Saturday  Saturday  Saturday  Saturday  Sunday    Sunday   \n[15] Sunday    Sunday    Sunday    Sunday    Thursday  Thursday  Thursday \n[22] Thursday  Thursday  Thursday  Thursday  Thursday  Tuesday   Tuesday  \n[29] Wednesday Wednesday Wednesday Wednesday Wednesday Wednesday Wednesday\n[36] Wednesday Wednesday Wednesday Wednesday Wednesday\n7 Levels: Friday &lt; Monday &lt; Saturday &lt; Sunday &lt; Thursday &lt; ... &lt; Wednesday\n\n\nHowever, this does not make sense. Who in their right mind would like to have Friday followed immediately by Monday? That is just not right!\n\n15.1.1 Ordering Factors\nTo establish an ordinal variable with a specified sequence of values that are not alphabetical we need to pass along the levels themselves as well as an indication that the data are supposed to be ordered:\n\ndata &lt;- factor( days, ordered=TRUE, levels = weekdays )\ndata\n\n [1] Sunday    Thursday  Wednesday Wednesday Monday    Wednesday Friday   \n [8] Wednesday Saturday  Wednesday Saturday  Thursday  Thursday  Sunday   \n[15] Sunday    Sunday    Saturday  Monday    Monday    Wednesday Saturday \n[22] Sunday    Wednesday Thursday  Tuesday   Saturday  Wednesday Friday   \n[29] Thursday  Sunday    Wednesday Monday    Saturday  Thursday  Wednesday\n[36] Wednesday Tuesday   Thursday  Thursday  Wednesday\n7 Levels: Monday &lt; Tuesday &lt; Wednesday &lt; Thursday &lt; Friday &lt; ... &lt; Sunday\n\n\nNow they’ll sort properly.\n\nsort( data )\n\n [1] Monday    Monday    Monday    Monday    Tuesday   Tuesday   Wednesday\n [8] Wednesday Wednesday Wednesday Wednesday Wednesday Wednesday Wednesday\n[15] Wednesday Wednesday Wednesday Wednesday Thursday  Thursday  Thursday \n[22] Thursday  Thursday  Thursday  Thursday  Thursday  Friday    Friday   \n[29] Saturday  Saturday  Saturday  Saturday  Saturday  Saturday  Sunday   \n[36] Sunday    Sunday    Sunday    Sunday    Sunday   \n7 Levels: Monday &lt; Tuesday &lt; Wednesday &lt; Thursday &lt; Friday &lt; ... &lt; Sunday\n\n\nNotice one important thing here. The arrangement of the values in the vector weekdays that we give it is the de facto ordering that is imposed. For example, if you were to reverse the weekday vector, it would ordinate the results in the opposite fashion.\n\nfactor( days, ordered = TRUE, levels = rev(weekdays )) -&gt; bkwrds\nsummary( bkwrds)\n\n   Sunday  Saturday    Friday  Thursday Wednesday   Tuesday    Monday \n        6         6         2         8        12         2         4 \n\n\nSo be very explicit when you define these.\n\n\n15.1.2 Exclusivity of Factor Levels\nOnce you establish a factor, you cannot set the values to anyting that is outside of the pre-defined levels. If you do, it will just put in missing data NA.\n\ndays[3] &lt;- \"Bob\"\n\nWarning in `[&lt;-.factor`(`*tmp*`, 3, value = \"Bob\"): invalid factor level, NA\ngenerated\n\ndays\n\n [1] Sunday    Thursday  &lt;NA&gt;      Wednesday Monday    Wednesday Friday   \n [8] Wednesday Saturday  Wednesday Saturday  Thursday  Thursday  Sunday   \n[15] Sunday    Sunday    Saturday  Monday    Monday    Wednesday Saturday \n[22] Sunday    Wednesday Thursday  Tuesday   Saturday  Wednesday Friday   \n[29] Thursday  Sunday    Wednesday Monday    Saturday  Thursday  Wednesday\n[36] Wednesday Tuesday   Thursday  Thursday  Wednesday\nLevels: Friday Monday Saturday Sunday Thursday Tuesday Wednesday\n\n\nThat being said, we can have more levels in the factor than observed in the data. Here is an example of just grabbing the work days from the week but making the levels equal to all the potential weekdays.\n\nworkdays &lt;- sample( weekdays[1:5], size=40, replace = TRUE )\nworkdays &lt;- factor( workdays, ordered=TRUE, levels = weekdays )\n\nAnd when we summarize it, we see that while it is possible that days may be named Saturday and Sunday, they are not recoreded in the data we have for workdays.\n\nsummary( workdays )\n\n   Monday   Tuesday Wednesday  Thursday    Friday  Saturday    Sunday \n       15         4         7        10         4         0         0 \n\n\nWe can drop the levels that have no representation in the sample (this is helpful if you are making output or subsequent analyses based upon subsets of the data and do not want to consider the levels you have zero observations for).\n\nworkdays &lt;- droplevels( workdays ) \nsummary( workdays )\n\n   Monday   Tuesday Wednesday  Thursday    Friday \n       15         4         7        10         4"
  },
  {
    "objectID": "discrete_data.html#the-forcats-library",
    "href": "discrete_data.html#the-forcats-library",
    "title": "15  Contingency Analysisq",
    "section": "15.2 The forcats Library",
    "text": "15.2 The forcats Library\nThe forcats library has a bunch of helper functions for working with factors. This is a relatively small library in tidyverse but a powerful one. I would recommend looking at the cheatsheet for it to get a more broad understanding of what functions in this library can do. In what follows, I’m just going to give you a taste of some of the functionality with a few examples of when it may be helpful.\n\nlibrary( forcats )\n\nJust like stringr had the str_ prefix, all the functions here have the fct_ prefix. Here are some examples.\nCounting how many of each factor\nOK, this is realy just a way to get a data.frame output from a summary() like analysis… But helpful.\n\nfct_count( data )\n\n# A tibble: 7 × 2\n  f             n\n  &lt;ord&gt;     &lt;int&gt;\n1 Monday        4\n2 Tuesday       2\n3 Wednesday    12\n4 Thursday      8\n5 Friday        2\n6 Saturday      6\n7 Sunday        6\n\n\nLumping Rare Factors\nThere are times when you only have a few observations of some levels and would need to lump these together so that you can get enough samples to make sense. By default, it combines the lowest frequency ones into an Other category.\n\nlumped &lt;- fct_lump_min( data, min = 5 )\nfct_count( lumped )\n\n# A tibble: 5 × 2\n  f             n\n  &lt;ord&gt;     &lt;int&gt;\n1 Wednesday    12\n2 Thursday      8\n3 Saturday      6\n4 Sunday        6\n5 Other         8\n\n\nReordering Factor Levels by Frequency\nThis reorders the factors by the frequency of observation.\n\nfreq &lt;- fct_infreq( data )\nlevels( freq )\n\n[1] \"Wednesday\" \"Thursday\"  \"Saturday\"  \"Sunday\"    \"Monday\"    \"Tuesday\"  \n[7] \"Friday\"   \n\n\nReordering by Order of Appearance’\nThis one reorders the factors by the order in which they are encountered in the dataset.\n\nordered &lt;- fct_inorder( data )\nlevels( ordered )\n\n[1] \"Sunday\"    \"Thursday\"  \"Wednesday\" \"Monday\"    \"Friday\"    \"Saturday\" \n[7] \"Tuesday\"  \n\n\nReordering Specific Levels\nThis allows you to reorder specific elements by passing it the ones you want to extract and they will be put in the first position. The example I use here is that in my own calendar, I prefer the layout of Monday in the first position and the weekends at the end of the week. Howver, it is also common to have the week start on Sunday and that can be done using the fct_relevel() and telling it that the first one is Sunday.\n\nlevels(data)\n\n[1] \"Monday\"    \"Tuesday\"   \"Wednesday\" \"Thursday\"  \"Friday\"    \"Saturday\" \n[7] \"Sunday\"   \n\nnewWeek &lt;- fct_relevel( data, \"Sunday\")\nlevels( newWeek )\n\n[1] \"Sunday\"    \"Monday\"    \"Tuesday\"   \"Wednesday\" \"Thursday\"  \"Friday\"   \n[7] \"Saturday\" \n\n\nDropping Unobserved Levels\nThis is just like droplevels() and there are literally 2 fewer keystrokes int he name…\n\ndropped &lt;- fct_drop( workdays )\nsummary( dropped )\n\n   Monday   Tuesday Wednesday  Thursday    Friday \n       15         4         7        10         4"
  },
  {
    "objectID": "discrete_data.html#using-factors",
    "href": "discrete_data.html#using-factors",
    "title": "15  Contingency Analysisq",
    "section": "15.3 Using Factors",
    "text": "15.3 Using Factors\nIt is common to use factors as an organizing princple in our data. For example, let’s say we went out and sampled three different species of plants and measured characteristics of their flower size. The iris data set from R.A. Fisher is a classic data set that is include in R and it looks like this (the functions head() and tail() show the top or bottom parts of a data frame).\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nBy default it is a data.frame object.\n\nclass( iris )\n\n[1] \"data.frame\"\n\n\n\n15.3.1 By the by\nOne helpful function in base R is the by() function. It has the following form.\nby( data, index, function)\nThe data is the raw data you are using, the index is a vector that we are using to differentiate among the species (the factor), and the function is what function we want to use. In Tidyverse, this is really similar to the group_by() %&gt;% summarize() pattern of analysis that we’ve used thus far. I include it here because you’ll probably see it in other code examples (or via AI) and should at least be familiar with it. I prefer the group_by() |&gt; summarize() approach myself but it is a free country…\nSo for example, if I were interested in the mean length of the Sepal for each species, I could write.\n\nmeanSepalLength &lt;- by( iris$Sepal.Length, iris$Species, mean )\n\nAnd this would:\n\nTake the sepal length\n\nGroup it by Species\n\nApply the mean function to it.\n\nAnd give you the output as a specific kind of output. It is a data types that is by which is just a kind of fancy list-like object (like analyses outputs are).\n\nclass( meanSepalLength )\n\n[1] \"by\"\n\nnames( meanSepalLength ) \n\n[1] \"setosa\"     \"versicolor\" \"virginica\" \n\n\nAnd the output of the data look like this\n\nmeanSepalLength\n\niris$Species: setosa\n[1] 5.006\n------------------------------------------------------------ \niris$Species: versicolor\n[1] 5.936\n------------------------------------------------------------ \niris$Species: virginica\n[1] 6.588\n\n\nI could also do the same thing with the variance in sepal length (here using the square bracket notation instead of the $ one).\n\nby( iris[,2], iris[,5], var ) -&gt; varSepalLength\nvarSepalLength \n\niris[, 5]: setosa\n[1] 0.1436898\n------------------------------------------------------------ \niris[, 5]: versicolor\n[1] 0.09846939\n------------------------------------------------------------ \niris[, 5]: virginica\n[1] 0.1040041\n\n\nAnd then if I were doing something fancy with it, I could make it into a data frame.\n\ndf &lt;- tibble( Species = levels( iris$Species), \n              Average = meanSepalLength,\n              Variance = varSepalLength\n)\ndf\n\n# A tibble: 3 × 3\n  Species    Average  Variance  \n  &lt;chr&gt;      &lt;by[1d]&gt; &lt;by[1d]&gt;  \n1 setosa     5.006    0.14368980\n2 versicolor 5.936    0.09846939\n3 virginica  6.588    0.10400408\n\n\nUsing normal tidyverse approaches, we could get the same output a bit easier as:\n\niris %&gt;%\n  group_by( Species ) %&gt;%\n  summarize( Average = mean(Sepal.Length), \n             Variance = var( Sepal.Length ))\n\n# A tibble: 3 × 3\n  Species    Average Variance\n  &lt;fct&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 setosa        5.01    0.124\n2 versicolor    5.94    0.266\n3 virginica     6.59    0.404\n\n\nBut if you like the by() appraoch, go for it!\n\n\n15.3.2 Missing Data\nMissing data is a fact of life and R is very opinionated about how it handles missing values. In general, missing data is encoded as NA and is a valid entry for any data type (character, numeric, logical, factor, etc.). Where this becomes tricky is when we are doing operations on data that has missing values. R could take two routes:\n\nIt could ignore the data and give you the answer directly as if the data were not missing, or\n\nIt could let you know that there is missing data and make you do something about it.\n\nFortunately, R took the second route.\nAn example from the iris data, I’m going to add some missing data to it.\n\nmissingIris &lt;- iris[, 4:5]\nmissingIris$Petal.Width[ c(2,6,12) ] &lt;- NA\nsummary( missingIris )\n\n  Petal.Width          Species  \n Min.   :0.100   setosa    :50  \n 1st Qu.:0.300   versicolor:50  \n Median :1.300   virginica :50  \n Mean   :1.218                  \n 3rd Qu.:1.800                  \n Max.   :2.500                  \n NA's   :3                      \n\n\nNotice how the missing data is denoted in the summary.\n\n15.3.2.1 Indications of Missing Data\nWhen we perform a mathematical or statistical operation on data that has missing elements R will always return NA as the result.\n\nmean( missingIris$Petal.Width )\n\n[1] NA\n\n\nThis warns you that at least one of the observations in the data is missing.\nSame output for using by(), it will put NA into each level that has at least one missing value.\n\nby( missingIris$Petal.Width, missingIris$Species, mean )\n\nmissingIris$Species: setosa\n[1] NA\n------------------------------------------------------------ \nmissingIris$Species: versicolor\n[1] 1.326\n------------------------------------------------------------ \nmissingIris$Species: virginica\n[1] 2.026\n\n\n\n\n15.3.2.2 Working with Missing Data\nTo acknowledge that there are missing data and you still want the values, you need to tell the function you are using that data is missing and you are OK with that using the optional argument na.rm=TRUE (na = missing data & rm is remove).\n\nmean( missingIris$Petal.Width, na.rm=TRUE)\n\n[1] 1.218367\n\n\nTo pass this to the by() function, we add the optional argument na.rm=TRUE and by() passes it along to the mean function as “…”\n\nby( missingIris$Petal.Width, missingIris$Species, mean, na.rm=TRUE )\n\nmissingIris$Species: setosa\n[1] 0.2446809\n------------------------------------------------------------ \nmissingIris$Species: versicolor\n[1] 1.326\n------------------------------------------------------------ \nmissingIris$Species: virginica\n[1] 2.026"
  },
  {
    "objectID": "discrete_data.html#univariate-data",
    "href": "discrete_data.html#univariate-data",
    "title": "15  Contingency Analysisq",
    "section": "15.4 Univariate Data",
    "text": "15.4 Univariate Data\nSo the simplest kind of discrete data we may have will consist of just a vector of observations. These observations are factors and we can summarize the number of occuance of each type. In some sense, there is no predictor here, just a set of discrete responses. For data like this, I’m just going to demonstrate a simple binomial test example, drawing on the awesome catfish examples from the previous lecture and then introduce a quick extension of it as a Multinomial. I’m not going to spend much time on it so we can jump right into a more generalized approach using contingency tables that can be applied to both univariate data like the fish example as well as more complicated data sets with actual discrete predictors and responses.\n\n15.4.1 The Binomial\nOK, now that we understand a little about factors, we can start doing some kinds of analyses. In the previous lecture, we discussed the Binomial as a generative model for data that has two discrete states using both Catfish/Non-Catfish as a data set as well as sex ratios in the desert beetle data set.\nFor the binomial, the probability of observing \\(K\\) items in a collection of \\(N\\) samples (where \\(N &gt; K\\) of course) is given as:\n\\[\nP(K|N,p) = \\frac{N!}{K!(N-K!)}p^K(1-p)^{N-K}\n\\]\nassuming that the likelihood of observing a single instance of \\(K\\) is given by the probability \\(p\\) (and not \\(K\\) is \\(1-p\\)). While this is the probability function associated with the binomial distribution, we also have a similar statistical test of the data under the binomial.\nTo apply a binomial test, we are essentially testing the hypothesis \\(H_O: p = \\hat{p}\\), that is that the hypothesized \\(p\\) is equal to the specified value.\nFrom the catfish example, remember that we found an estimate of \\(p\\) to be roughly \\(0.74\\). If we were to go out and do another sampling session and this time, from a sample of 50 fish, get the following (made up but relevant data)\n\nfish_sample\n\n [1] Catfish Other   Other   Catfish Catfish Catfish Catfish Catfish Other  \n[10] Catfish Other   Catfish Catfish Other   Other   Other   Catfish Catfish\n[19] Catfish Catfish Catfish Catfish Catfish Catfish Other   Catfish Catfish\n[28] Catfish Other   Catfish Catfish Catfish Other   Other   Catfish Catfish\n[37] Catfish Other   Catfish Catfish Catfish Catfish Catfish Catfish Other  \n[46] Other   Other   Catfish Other   Other  \nLevels: Catfish Other\n\n\nWhich summarizes to:\n\nfct_count( fish_sample)\n\n# A tibble: 2 × 2\n  f           n\n  &lt;fct&gt;   &lt;int&gt;\n1 Catfish    33\n2 Other      17\n\n\nWe could test the hypothesis that The frequency of catfish in this sampling effort is not significantly different our previously observed frequency of 74%.\nFormally, we would specify.\n\\(H_O: p = 0.74\\)\nand test it against the alternative hypothesis\n\\(H_A: p \\ne 0.74\\)\nIn R, we do the statistical test by designating the number of observations in for catfish, total sample size, and the associated probability using the binom.test function.\n\nfit &lt;- binom.test(x = 33, n = 50 , p = 0.74)\n\nThe test returns a results object that I assigned to the variable fit (which is a list like object as you remember). However, we can print it out and it displays all the relevant information.\n\nfit\n\n\n    Exact binomial test\n\ndata:  33 and 50\nnumber of successes = 33, number of trials = 50, p-value = 0.1991\nalternative hypothesis: true probability of success is not equal to 0.74\n95 percent confidence interval:\n 0.5123475 0.7879453\nsample estimates:\nprobability of success \n                  0.66 \n\n\nFrom this, we can see the following components:\n\nThe data consist of 33 catfish from a sample of size 50 (double check with data to make sure you didn’t make a typo…),\nThe Probability associated with observing 33/50 if TRUE frequency is \\(0.74\\),\nA 95% confidence interval on the probability itself,\nThe original frequency of success in the new data.\n\nFrom these data, we can conclude that the likelihood of observing 33/50 as catfish is about 20% of the time. This is not so rare that we should reject the null hypothesis (unless you think 20% is a rare occurance). So we fail to reject the null \\(H_O: p = 0.74\\).\n\n\n15.4.2 The Multinomial\nThe multinomial test is a simple extension of to the Binomial, when there are more than two discrete states (e.g., 3 or more). The data still have to be discrete factors (and unordered). The associated probability function is a bit messier. However, for \\(K\\) different categories of things (each level denoted as \\(X_1\\), \\(X_2\\), , \\(X_K\\)), which are expected to occur at indiviudual frequencies of \\(p_1\\), \\(p_2\\), , \\(p_K\\) and that all the probabilities sum to one (e.g., \\(\\sum_{i=1}^K p_i = 1.0\\) ) the multinomial expansion:\n\\[\nP(X_1 = p_1, X_2 = p_2, \\ldots, X_K = p_k|N) = \\frac{N!}{\\prod_{i=1}^{K}x_i!}\\prod_{i=1}^Kp^{x_i}\n\\]\nWe are not going to really play around with this expansion much as it requires a bunch of additional assumptions and sometimes it is just easier to run it as a contingency table analysis (in the next section). I’m just going to leave this here for prosperity sake."
  },
  {
    "objectID": "discrete_data.html#contingency-tables",
    "href": "discrete_data.html#contingency-tables",
    "title": "15  Contingency Analysisq",
    "section": "15.5 Contingency Tables",
    "text": "15.5 Contingency Tables\nThis is a much more commonly used approach for when we have either univariate data or have predicted and response variables that are both discrete categorical data. Just like in the binomial multinomial examples, we can use a contingency table analysis to determine if the proportions of observations in each category are the same. Let’s assume, using the data we collected in the catfish where we found 37 catfish in a sample of 50 fishes.\nInstead of just talking about the probability of catfish, lets talk about the probability of both catfish and not catfish as a short vector of individual probabilities.\n\\[\np = \\left( \\frac{37}{50}, \\frac{13}{50} \\right)  = (0.74,0.26)\n\\]\nNow, if we go out and sample 172 fish, we would expect to see \\(0.74*172=127.28\\) catfish and \\(0.26*172=44.72\\) non-catfish. This is our EXPECTED values for each catgory and can be denoted as:\n\\[\nE = [E_1, E_2]\n\\]\nIf we had \\(K\\) different species of fishes, it could similarily be expanded to something like:\n\\[\nE = [E_1, E_2, \\ldots, E_K]\n\\]\nwith each entry being the total number of observations times the expected frequency of it.\nNow, let’s say that we did go out and of those 172, the OBSERVED number of catfish and non-catfish were:\n\\[\nO = [108, 64]\n\\]\nIs the fraction of observations in the observed set distributed as expected given the proportions in the expectation? The NULL hypothesis for this test is that the probability of fish is catfish is equal to the expected fraction of catfish (or more stodgely as:)\n\\(H_O: P(X\\;in\\;j) = p_j\\)\nThe test statistic then becomes a measure of how close the values in \\(O\\) are to those in \\(E\\) (with a standardization by the magnitude of the expectation).\n\\[\nT = \\sum_{i=1}^c \\frac{(O_i - E_i)^2}{E_i}\n\\]\nThis \\(T\\) test statistic just happens to be distributed (with sufficient sample sizes) like the \\(\\chi^2\\) distribution.\nIn R, we can us the chisq.test() function. If you go take a look at the help file ?chisq.test file for it, it shows the function as:\nchisq.test(x, y = NULL, correct = TRUE,\n           p = rep(1/length(x), length(x)), rescale.p = FALSE,\n           simulate.p.value = FALSE, B = 2000)\nThe salient points are:\n\nThe value of \\(x\\), which is our observed data as a vector of counts.\n\nThe expected freqeuncies (the \\(p_i\\) values). If you look at he function above, if you do not specify values for \\(p\\) it will assume the categories are expected in equal proportions (e.g., \\(p_1 = p_2 = \\ldots = p_k\\)). This is not what we want, so we’ll have to supply those expected proportions to the function directly.\n\n\nFishes &lt;- c(108,64)\np &lt;- c(0.74, 0.26)\nfit &lt;- chisq.test(x=Fishes, p = p )\n\nThe function returns the result as a ‘list-like’ object which we can look at by printing it out.\n\nfit\n\n\n    Chi-squared test for given probabilities\n\ndata:  Fishes\nX-squared = 11.233, df = 1, p-value = 0.0008037\n\n\nGiven the results, the P-value is quite small. This suggests that the likelihood of the condition stated in the NULL hypothesis (e.g., that the observed counts of fish are coincident with those given by \\(p\\)) is unlikely. In this case, we would reject the null hypothesis.\nThis approach can be generalized to any arbitrary number of categories using the same rationale and code. So if we were sampling catfish, gizzard shad, and crappy, we’d have vectors with 3 elements in them instead of just two.\n\n15.5.1 Rows & Columns\nIn general, we have two set of data here (I sometimes call them predictor and response but they can also be just two categories of data that have discrete types). I’m going to use some built-in data with hair and eye colors in R that was sampled from the University of Delaware in 1974 (hey, just like the motor trends data, it must have been a classic year for data sets to be put in stats packages…).\nBy default, the data are split by reported sex. The raw data look like this:\n\nHairEyeColor\n\n, , Sex = Male\n\n       Eye\nHair    Brown Blue Hazel Green\n  Black    32   11    10     3\n  Brown    53   50    25    15\n  Red      10   10     7     7\n  Blond     3   30     5     8\n\n, , Sex = Female\n\n       Eye\nHair    Brown Blue Hazel Green\n  Black    36    9     5     2\n  Brown    66   34    29    14\n  Red      16    7     7     7\n  Blond     4   64     5     8\n\n\nI’m going to combine them together into a single data set. They are both as matrices and kept as a 3-dimensional matrix so I’m just going to add the two matrices together (rows and columns in each) as:\n\nhairAndEye &lt;- HairEyeColor[,,1] + HairEyeColor[,,2]\nhairAndEye\n\n       Eye\nHair    Brown Blue Hazel Green\n  Black    68   20    15     5\n  Brown   119   84    54    29\n  Red      26   17    14    14\n  Blond     7   94    10    16\n\n\nThe rationale for the underlying statatical test is that the two traits are independent. This means that:\n\nEach row can be considered a random sample of eye colors from the background population that is independent of the hair color.\nEach column of the data should be a random sample of hair colors from the background population which is independent of the eye color.\n\nTaking eye color as an example, this would assume that for a single row of the data, say the black hair row, the data look like:\n\nhairAndEye[1,] -&gt; x\nx\n\nBrown  Blue Hazel Green \n   68    20    15     5 \n\n\nThe proportions of individuals in each eye color in this hair category are given as:\n\nx / sum(x)\n\n    Brown      Blue     Hazel     Green \n0.6296296 0.1851852 0.1388889 0.0462963 \n\n\nAnd if hair and eye color are truly independent of each other then we should expect that these proportions should roughly be equal to that if we look at the eye colors for all rows (there is a rowSums and colSums function that does just what you think it should do). In this case, if individuals in the first row of the data are from the same population as everyone else, then the total counts in the data\n\ncolSums( hairAndEye ) -&gt; tot\ntot\n\nBrown  Blue Hazel Green \n  220   215    93    64 \n\n\nshould have roughly the same set of proportions.\n\ntot / sum(tot)\n\n    Brown      Blue     Hazel     Green \n0.3716216 0.3631757 0.1570946 0.1081081 \n\n\nLet’s plot these to take a look. If they are the same then they should fall on the line starting at the origin with a slope of 1.0.\n\ndata.frame( Color = c(\"Brown\",\"Blue\",\"Hazel\",\"Green\"),\n            Total = tot/sum(tot),\n            Row1 = x/sum(x) ) %&gt;%\n  ggplot( aes(Total,Row1) ) + \n  geom_point() + \n  geom_abline(slope=1,intercept=0,col=\"red\", lty=2) + \n    geom_label_repel( aes(label=Color) ) +   \n  xlab(\"Fraction from Total Population\") + \n  ylab(\"Fraction from Row 1\") +\n  theme_minimal() + \n  xlim(c(0,1)) + \n  ylim(c(0,1))\n\n\n\n\nSome are off a bit. But across all hair colors, we could do the same thing and ask the same question.\nLet’s take the general case where the data look like this:\n\\[\n\\begin{bmatrix}\nO_{11} & O_{12} & \\ldots & O_{1c} \\\\\nO_{21} & O_{22} & \\ldots & \\vdots \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nO_{r1} & O_{r2} & \\ldots & O_{rc}\n\\end{bmatrix}\n\\]\nWhose rows can be summed to produce the row totals\n\\[\n\\begin{bmatrix}\nR_{1} \\\\\nR_{2} \\\\\n\\vdots \\\\\nR_{r}\n\\end{bmatrix}\n\\]\nAnd the columns can be summed to produce the column totals\n\\[\n\\begin{bmatrix}\nC_{1} & C_{2} & \\ldots & C_{c}\n\\end{bmatrix}\n\\]\nBoth of which sum to \\(N\\), the number of total observations across all rows and columns. To simplify this whole thing a bit, let’s just talk about the independence of a student having a specific hair and eye color. The strict hypothesis we are testing here in a contingency table analysis is thus (in a very stuffy fashion):\n\\(H_O:\\) The event ‘an observation in row i’ is independent of the event ‘the same observation in column j’ for all i and j.\nOr if we want to shorten it\n\\(H_O:\\) Hair and eye colors are independent traits.\nThe test statistic here depends upon the observed and expected values for each of the levels in the table. The observed data are from the table above and we can represent the count of individuals with black hair and brown eyes \\(O_{black,brown} = 68\\). The expected number of individuals with black hair and brown eyes is estimated as the number of individuals with black hair times the number of individuals with brown eyes divided by the total number of individuals.\n\\[\nE_{i,j} = R_i * C_j / N\n\\]\nWe define the test statistic as the the square of the differences in observed and expected \\((O_{ij} - E_{ij})^2\\), standardized by the expectation (\\(E_{ij}\\) as the fraction of the expected values). Summed across all row and columns, we get the statistic.\n\\[\nT = \\sum_{i=1}^r\\sum_{j=1}^c\\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\n\\]\nNow, if \\(T\\) is small then this suggests that the observed values in each row/column combination were relatively close to what was expected (favoring the NULL hypothesis) and if \\(T\\) is large, they the observed values are relatively far away from the expectations (favoring us to reject the NULL). How big is “big enough” depends upon the number of categories in the rows and columns… the degrees of freedom.\n\\(df = (r-1)(c-1)\\)\nAs it turns out, the \\(T\\) statistic is an observation that is derived from the \\(\\chi^2\\) distribution with \\((r-1)(c-1)\\) degrees of freedom. That was fun, no?\nAs it turns out, in R, once we have the raw data in a matrix format like this:\n\nhairAndEye\n\n       Eye\nHair    Brown Blue Hazel Green\n  Black    68   20    15     5\n  Brown   119   84    54    29\n  Red      26   17    14    14\n  Blond     7   94    10    16\n\n\nTo test it using a \\(\\chi^2\\)-test for independence (also called a ‘goodness of fit’ test) we just use the single function:\n\nfit &lt;- chisq.test(hairAndEye)\n\nwhich produces a result that is very ‘list-like’\n\nfit\n\n\n    Pearson's Chi-squared test\n\ndata:  hairAndEye\nX-squared = 138.29, df = 9, p-value &lt; 2.2e-16\n\n\nAnd from this, we have the statistic, the degrees of freedom, and the p-value associated with it. In this case, the null hypothesis as above is most likely quite uncommon as the P-value is denoted in scientific notation (moving the decimal place over to the left 16 times no less!)."
  },
  {
    "objectID": "correlation.html",
    "href": "correlation.html",
    "title": "16  Corelation",
    "section": "",
    "text": "17 Correlation Tests\nThe following types of correlation statistics are a sample of the most common approaches."
  },
  {
    "objectID": "correlation.html#some-new-data",
    "href": "correlation.html#some-new-data",
    "title": "16  Corelation",
    "section": "16.1 Some New Data",
    "text": "16.1 Some New Data\nFor this topic, I thought I would turn to a bit of a more digestible set of data—data describing beer styles! There is a new CSV data set on the GitHub site located at the following URL.\n\nbeer_url &lt;- \"https://raw.githubusercontent.com/dyerlab/ENVS-Lectures/master/data/Beer_Styles.csv\"\nbeer &lt;- read_csv( beer_url )\nsummary( beer )\n\n    Styles             Yeast              ABV_Min         ABV_Max      \n Length:100         Length:100         Min.   :2.400   Min.   : 3.200  \n Class :character   Class :character   1st Qu.:4.200   1st Qu.: 5.475  \n Mode  :character   Mode  :character   Median :4.600   Median : 6.000  \n                                       Mean   :4.947   Mean   : 6.768  \n                                       3rd Qu.:5.500   3rd Qu.: 8.000  \n                                       Max.   :9.000   Max.   :14.000  \n    IBU_Min         IBU_Max          SRM_Min         SRM_Max     \n Min.   : 0.00   Min.   :  8.00   Min.   : 2.00   Min.   : 3.00  \n 1st Qu.:15.00   1st Qu.: 25.00   1st Qu.: 3.50   1st Qu.: 7.00  \n Median :20.00   Median : 35.00   Median : 8.00   Median :17.00  \n Mean   :21.97   Mean   : 38.98   Mean   : 9.82   Mean   :17.76  \n 3rd Qu.:25.00   3rd Qu.: 45.00   3rd Qu.:14.00   3rd Qu.:22.00  \n Max.   :60.00   Max.   :120.00   Max.   :30.00   Max.   :40.00  \n     OG_Min          OG_Max          FG_Min          FG_Max     \n Min.   :1.026   Min.   :1.032   Min.   :0.998   Min.   :1.006  \n 1st Qu.:1.040   1st Qu.:1.052   1st Qu.:1.008   1st Qu.:1.012  \n Median :1.046   Median :1.060   Median :1.010   Median :1.015  \n Mean   :1.049   Mean   :1.065   Mean   :1.009   Mean   :1.016  \n 3rd Qu.:1.056   3rd Qu.:1.075   3rd Qu.:1.010   3rd Qu.:1.018  \n Max.   :1.080   Max.   :1.130   Max.   :1.020   Max.   :1.040  \n\n\nThe data consist of the following categories of data. For all but he first two columns of data, a range is given for the appropriate values for each style with Min and Max values.\n\nStyles - The official name of the beer style. Yes, there is an international standard that is officiated by the Beer Judge Certification Program.\nYeast Type - The species of yeast most commonly used for fermenation, consists of top fermenting Ale yeasts and bottom fermenting Lager yeasts.\n\nABV - The amount of alcohol in the finished beer as a percentage of the volume. This is a non-negative numerical value.\nIBU - The ‘International Bitterness Unit’ which roughly measures the amont of \\(\\alpha\\)-acids (asymptotically) added to the beer by the hops. This is a non-negative numerical value, with higher values indicating more bitter beer, though human ability to taste increasingly bitter beer is asymptotic.\nSRM - The Standard Reference Method calibration measuring the color of the finished beer. This is a non-negative integer going from 1 - 40 (light straw color - dark opaque).\nOG - The amount of dissolved sugars in the wort (the pre-beer liquid prior to putting in yeast and the initiation of fermentation), relative to pure water. This is a measurement ‘relative’ to water, which is 1.0. Values less than 1.0 have lower liquid densities than pure water and those greater than 1.0 have more dissolved sugars than pure water.\nFG - The amount of dissolved sugars in the beer after fermentation has been completed. Same as above but the difference in OG and FG can tell us what the ABV should be. Hihger FG beers are more sweet and have more body than lower OG beers (which may appear to have a cleaner, drier, mouth feel—yes that is a real term as well).\n\nAs we talk about correlations, we will use these as examples."
  },
  {
    "objectID": "correlation.html#parameters-estimates",
    "href": "correlation.html#parameters-estimates",
    "title": "16  Corelation",
    "section": "16.2 Parameters & Estimates",
    "text": "16.2 Parameters & Estimates\nIn statistics, we have two kinds of entities, parameters and estimates, which are dualities of each other. The TRUE mean of a set of data is referred to by \\(\\mu\\) whereas the mean of the data we measured is referred to as \\(\\bar{x}\\). The greek version is the idealized value for the parameter, something that we are striving to find the real estimate of. However, as a Frequentist, we can never actually get to that parameter (remember the actual population of data is infinite but we can only sample a small amount of it) and when we talk about the data associated with what we collect, we refer to it as a estimate and use normal variable names."
  },
  {
    "objectID": "correlation.html#parametric-assumptions",
    "href": "correlation.html#parametric-assumptions",
    "title": "16  Corelation",
    "section": "16.3 Parametric Assumptions",
    "text": "16.3 Parametric Assumptions\nFor much of the statistics we use, there are underlying assumptions about the form of the data that we shold look at.\n\n16.3.1 Testing for Normality.\n\nThe data can be estimated by a normal density function, or at least can be transformed into data that is reasonably normal in distribution.\n\nThe normal distribution function is defined as:\n[ f(x) = e^{-()} ]\nwhere \\(\\mu\\) and \\(\\sigma\\) are the true value of the underlying mean and standard deviation. This distribution is denoted as \\(N(\\mu,\\sigma)\\) and the differences in the mean value (\\(\\mu\\)) and the variation measured by the standard deviation (\\(\\sigma\\)) are shown below for \\(N(0,1)\\), \\(N(0,5)\\), and \\(N(10,1)\\).\n\nN &lt;- 1000\ndata.frame( Distribution = rep(c(\"N(0,1)\",\"N(10,1)\", \"N(0,5)\"), each=N ),\n            Data = c( rnorm(N,0,1),\n                      rnorm(N,10,1),\n                      rnorm(N,0,5) ) ) %&gt;%\n  ggplot( aes( Data ) ) + \n  geom_histogram( alpha=0.75, \n                  bins = 50) + \n  facet_grid(Distribution ~.)\n\n\n\n\nThere are a couple of ways to look at our data to see if they can be considered as normal. First, visually we can plot the theoretical (parameter) quantiles of the data against the sample quantiles using the qqnorm() plot. What this does is sort the data by expectation and observation and plot them and if the data are normal, then they should roughly be in a straight line. The qqline() function shows the expected line (n.b., this is another one of those things where you have to run the whole chunk to get both points and lines on the same graph if you are working in Markdown).\n\nqqnorm( beer$ABV_Min )\nqqline( beer$ABV_Min, col=\"red\")\n\n\n\n\nSo, what we commonly see is most of the data falling along the line throughout the middle portion of the distribution and then deviating around the edges. What this does not do is give you a statistic to test to see if we can reject the hypothesis \\(H_O: Data\\;is\\;normal\\). For this, we can use the Shapiro-Wilkes Normality test which produces the statistic:\n[ W = ]\nwhere \\(N\\) is the number of samples, \\(a_i\\) is a standardizing coeeficient, \\(x_i\\) is the \\(i^{th}\\) value of \\(x\\), \\(\\bar{x}\\) is the mean of the observed values, and \\(R_{x_i}\\) is the rank of the \\(x_i^{th}\\) observation.\n\nshapiro.test( beer$ABV_Min )\n\n\n    Shapiro-Wilk normality test\n\ndata:  beer$ABV_Min\nW = 0.94595, p-value = 0.0004532\n\n\nRejection of the null hypothesis (e.g., a small p-value from the test) indicates that the data are not to be considered as coming from a normal distribution. So, for the ABV_Min data above, it appears that it is not actually normally distributed. So what do we do?\n\n\n16.3.2 Transformations\nIf the data are not normal, we can look towards trying to see if we can transform it to a normally distributed variable. There are a lot of\nStudentized Data - One way to standardize the data is to make it have a mean of 0.0 and a standard deviation of 1.0. To do this, we subtract the mean() and divide by the sd().\n\nx &lt;- beer$ABV_Min \nx.std &lt;- (x - mean(x)) / sd( x )\n\nThere are times when this can be a nice way to compare the\nBox Cox - In 1964, Box & Cox defined a family of transformations known as the Box/Cox. This family is defined by a single parameter, \\(\\lambda\\), whose value may vary depending upon the data. The original data, \\(x\\), is then transformed using the following relationship\n[ = ]\nAs long as \\(\\lambda \\ne 0\\) (else we would be dividing by zero, which is not a good thing)!\nOne way to use this transformation is to look at a range of values for \\(\\lambda\\) and determine if the transformation\n\ntest_boxcox &lt;- function( x, lambdas = seq(-1.1, 1.1, by = 0.015) ) {\n  ret &lt;- data.frame( Lambda = lambdas,\n                     W = NA,\n                     P = NA)\n  \n  for( lambda in lambdas ) {\n    x.tilde &lt;- (x^lambda - 1) / lambda   \n    w &lt;- shapiro.test( x.tilde )\n    ret$W[ ret$Lambda == lambda ] &lt;- w$statistic\n    ret$P[ ret$Lambda == lambda ] &lt;- w$p.value\n  }\n  \n  return( ret )\n}\n\nvals &lt;- test_boxcox( beer$ABV_Min ) \n\n\nvals %&gt;%\n  ggplot( aes(Lambda, P) ) + \n  geom_line() + \n  ylab(\"P-Value\")\n\n\n\n\nSo if you look at this plot, it shows the P-value of the Shapiro-Wilkes test across a range of values. Depending upon the level of rigor, this approaches the \\(\\alpha = 0.05\\) value closest at:\n\nvals[ which(vals$P == max( vals$P)),]\n\n   Lambda        W          P\n82  0.115 0.973805 0.04351988\n\n\nwith \\(\\lambda = 0.115\\) and a \\(P = 0.044\\).\nArc-Sine Square Root When dealing with fractions, it is common that they do not behave very well when they are very close to 0.0 or 1.0. One of the common transformations to use with these kinds of data is the arc-sin square root transformation. For us, the ABV columns in the data is a percentage (but listed in numerical form as percent not as fraction). So to transform it we can do the following.\n\nabv &lt;- beer$ABV_Min / 100.0\nasin( sqrt( abv ) ) -&gt; abv.1\nshapiro.test( abv.1)\n\n\n    Shapiro-Wilk normality test\n\ndata:  abv.1\nW = 0.96746, p-value = 0.01418"
  },
  {
    "objectID": "correlation.html#equal-variance",
    "href": "correlation.html#equal-variance",
    "title": "16  Corelation",
    "section": "16.4 Equal Variance",
    "text": "16.4 Equal Variance\nAnother parametric assumption is the equality of variance across a range of the data. This means, for example, that the variance from one part of the experiment should not be different than the variance in samples from another portion of data. We will return to this when we evaluate regression models."
  },
  {
    "objectID": "correlation.html#independence-of-data",
    "href": "correlation.html#independence-of-data",
    "title": "16  Corelation",
    "section": "16.5 Independence of Data",
    "text": "16.5 Independence of Data\nThe samples you collect, and the way that you design your experiments are most important to ensure that your data are individually independent. You need to think about this very carefully as you design your experiments."
  },
  {
    "objectID": "correlation.html#parametric-test-pearson-product-moment-correlations",
    "href": "correlation.html#parametric-test-pearson-product-moment-correlations",
    "title": "16  Corelation",
    "section": "17.1 Parametric Test: Pearson Product Moment Correlations",
    "text": "17.1 Parametric Test: Pearson Product Moment Correlations\nBy far, the most common correlation statistic we see is the Pearson Product Moment Correlation, denoted as \\(\\rho\\). For two variables, \\(x\\) and \\(y\\), the correlation parameter is estimated as:\n[ = ]\nThe values of these data fall wihtin the range of: \\(-1 \\le \\rho \\le +1\\) with negative values indicating that when one variable goes up, the other goes down. Positive values of a correlation indicate that both variable change systematically in the same direction (e.g., both up or both down).\nHere are some examples of the distribution of two variables and their associated correlation coefficient.\n\n\n\nFigure 1: Data and associated correlation statistics.\n\n\nSignificance testing for a correlation such as \\(\\rho\\) determine the extent to which we thing the value of is deviant from zero. The Null Hypothesis is \\(H_O: \\rho \\ne 0\\) and can be evaluated using the Student’s t.test. With large enough sample sizes, it can be approximated by:\n[ t = r ]\nHowever, we should probably rely upon R to look up the critical values of the statistic.\nThe default value for cor.test() is the Pearson. Here is an example of its use and the output that we’ve seen before.\n\ncor.test( beer$OG_Max, beer$FG_Max ) -&gt; OG.FG.pearson\nOG.FG.pearson\n\n\n    Pearson's product-moment correlation\n\ndata:  beer$OG_Max and beer$FG_Max\nt = 15.168, df = 98, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.7671910 0.8878064\nsample estimates:\n      cor \n0.8374184 \n\n\nOf particular note are the components associated with the results object that allows you to gain access to specifics for any analysis.\n\nnames( OG.FG.pearson )\n\n[1] \"statistic\"   \"parameter\"   \"p.value\"     \"estimate\"    \"null.value\" \n[6] \"alternative\" \"method\"      \"data.name\"   \"conf.int\""
  },
  {
    "objectID": "correlation.html#non-parametric-test-spearmans-rho",
    "href": "correlation.html#non-parametric-test-spearmans-rho",
    "title": "16  Corelation",
    "section": "17.2 Non-Parametric Test: Spearman’s Rho",
    "text": "17.2 Non-Parametric Test: Spearman’s Rho\nAnother way to de a correlation test that does not rely upon parametric assumptions is to use non-parametric approaches. Most non-parametric tests are based upon ranks of the data rather than the assumption of normality of the data that is necessary for the Pearson Product Moment statistic. One of the constraints for non-parametric statistics is that they are often evaluated for probability based upon permutations.\nThe form of the estimator for this is almost identical to that of the Pearson statistic except that instead of the raw data, we are replacing values with the ranks of each value instead. In doing so, there is a loss of the breadth of the raw data since we are just using ranks, and if the underlying data are poorly behaved because of outliers or other issues, this takes care of it.\n\\[\n\\rho_{Spearman} = \\frac{ \\sum_{i=1}^N(R_{x_i} - \\bar{R_{x}})(R_{y_i} - \\bar{R_{y}})}{\\sqrt{\\sum_{i=1}^N(R_{x_i} - \\bar{R_{x}})^2}\\sqrt{\\sum_{i=1}^N(R_{y_i} - \\bar{R_{y}})^2}}\n\\]\nWith the same data, it does provide potentially different estimates of the amount of correlation between the variables.\n\nOG.FG.spearman &lt;- cor.test( beer$OG_Max, beer$FG_Max, \n                            method = \"spearman\" )\n\nWarning in cor.test.default(beer$OG_Max, beer$FG_Max, method = \"spearman\"):\nCannot compute exact p-value with ties\n\nOG.FG.spearman\n\n\n    Spearman's rank correlation rho\n\ndata:  beer$OG_Max and beer$FG_Max\nS = 39257, p-value &lt; 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.7644328"
  },
  {
    "objectID": "correlation.html#permutation-testing-for-significance",
    "href": "correlation.html#permutation-testing-for-significance",
    "title": "16  Corelation",
    "section": "17.3 Permutation Testing for Significance",
    "text": "17.3 Permutation Testing for Significance\nIn both of the previous methods, we used specific approaches to evaluate the significance of the statistic. For Pearson, we approximated using the \\(t\\). For the Spearman test with small numbers of samples, an approximation of the \\(t\\) test is used, based upon counting ranks and the number of ways we can get different combinations of ranks. For larger sample size tests using Spearman, an approximation using the \\(t\\) test can be used.\nAnother way of doing this is based upon permutation and this approach can be applied to a wide array of questions. For correlation’s, if we consider the null hypothesis \\(H_O: \\rho = 0\\) we can make a few inferences. If this hypothesis is true then we are, essentially, saying that the current relationship between \\(x_i\\) and \\(y_i\\) has no intrinsic relationship as there is no correlation. This is, by default, what the null hypothesis says.\nIf that is true, however, that means that any permutation of one of the variables, say \\(y\\), should produce a correlation statistic that is just as large as any other permutation of the data. This is key.\nSo, if we assume the \\(H_O\\) is true then we should be able to shuffle one of the data and estimate a correlation statistic a large number of times. We can then create a permuted distribution of values for the correlation, Assuming the NULL Hypothesis is true. To this distribution, we can evaluate the magnitude of the original correlation. Here is an example using the data from above.\n\nx &lt;- beer$OG_Max\ny &lt;- beer$FG_Max\ndf &lt;- data.frame( Estimate = factor( c( \"Original\",\n                                        rep(\"Permuted\", 999))), \n                  rho =  c( cor.test( x, y )$estimate,\n                            rep(NA, 999)) )\n\nsummary( df )\n\n     Estimate        rho        \n Original:  1   Min.   :0.8374  \n Permuted:999   1st Qu.:0.8374  \n                Median :0.8374  \n                Mean   :0.8374  \n                3rd Qu.:0.8374  \n                Max.   :0.8374  \n                NA's   :999     \n\n\nNow, we can go through the 999 NA values we put into that data frame and:\n1. Permute one of the variables 2. Run the analysis\n3. Store the statistic.\n\nfor( i in 2:1000) {\n  yhat &lt;- sample( y,   # this shuffles the data in y\n                  size = length(y), \n                  replace = FALSE)\n  model &lt;- cor.test( x, yhat )\n  df$rho[i] &lt;- model$estimate \n}\n\nNow we can look at the distribution of permuted values and the original one and see the relationship. If:\n\nThe observed value is within the body of the permuted values, then it is not too rare—given \\(H_O\\), or\nIf the observed value is way outside those permuted values, then it appears to be somewhat rare.\n\n\nggplot( df ) + \n  geom_histogram( aes(rho, fill=Estimate ), bins=50 )\n\n\n\n\nIf you look at the graph above, you see that the original value is way bigger than the values that would be found if and only if \\(H_O\\) were true. This suggests that the correlation is not zero and in fact it is the largest observation of the 1000 observations (a P estimate of \\(\\frac{1}{1000}\\)…)."
  },
  {
    "objectID": "regression.html#least-squares-fitting",
    "href": "regression.html#least-squares-fitting",
    "title": "17  Regression",
    "section": "17.1 Least Squares Fitting",
    "text": "17.1 Least Squares Fitting\nSo how do we figure this out? One of the most common ways is to uses a methods called Least Squared Distance fitting. To describe this, consider a set of hypothetical random models with random values estimated for both the intercept (\\(\\beta_0\\)) and slope (\\(\\beta_1\\)) coefficients. These could be close to a good models or not.\n\nmodels &lt;- data.frame( beta0 = runif(250,-20,40),\n                      beta1 = runif(250, -5, 5))\nsummary( models )\n\n     beta0             beta1        \n Min.   :-19.995   Min.   :-4.9815  \n 1st Qu.: -4.507   1st Qu.:-2.7747  \n Median : 13.686   Median :-0.5897  \n Mean   : 11.556   Mean   :-0.2112  \n 3rd Qu.: 26.956   3rd Qu.: 2.3349  \n Max.   : 39.893   Max.   : 4.9967  \n\n\nWe can plot these and the original data and all these randomly defined models.\n\nggplot() + \n  geom_abline( aes(intercept = beta0, \n                   slope = beta1), \n               data = models,\n               alpha = 0.1) + \n  geom_point( aes(x,y), \n              data=df )\n\n\n\n\nA least squares fit is one that minimizes the distances of each point (in the y-axis) from the line created by the model. In the graph below, we can see this would be the distances (squared so we do not have positive and negative values) along the y-axis, between each point and the fitted line.\nThe “best model” here is one that minimizes the sum of squared distances distances.\n\n\n\n\n\nLet’s look at those hypothetical models. I’m going to make a few little functions to help make the code look easy.\nFirst, here is a function that returns the distances between the original points and a hypothesized regression line defined by an interscept and slope from the original points.\n\nmodel_distance &lt;- function( interscept, slope, X, Y ) {\n  yhat &lt;- interscept + slope * X\n  diff &lt;- Y - yhat\n  return( sqrt( mean( diff ^ 2 ) ) )\n}\n\nNow, let’s go through all the models and estimate the mean squared distances between the proposed line (from intercept and slope) and the original data.\n\nmodels$dist &lt;- NA\nfor( i in 1:nrow(models) ) {\n  models$dist[i] &lt;- model_distance( models$beta0[i],\n                                    models$beta1[i],\n                                    df$x,\n                                    df$y )\n}\nhead( models )\n\n       beta0     beta1      dist\n1  13.224649 -3.863088 44.206855\n2  30.897552  4.279254 23.789143\n3   6.626146  4.963379  8.798447\n4  32.551256 -3.190101 24.204268\n5  25.785952  4.362171 19.453256\n6 -10.009928 -3.811105 65.569903\n\n\nIf we look through these models, we can see which are better than others by sorting in increasing squared distance.\n\nggplot()  + \n  geom_abline( aes(intercept = beta0,\n                   slope = beta1, \n                   color = -dist),\n               data = filter( models, rank(dist) &lt;= 10 ),\n               alpha = 0.5) + \n  geom_point( aes(x,y),\n              data=df) \n\n\n\n\nThese models in the parameter space of intercepts and slopes can be visualized as this. These red-circles are close to where the best models are located.\n\nggplot( models, aes(x = beta0, \n                    y = beta1,\n                    color = -dist)) + \n  geom_point( data = filter( models, rank(dist) &lt;= 10), \n              color = \"red\",\n              size = 4) +\n    geom_point()\n\n\n\n\nIn addition to a random search, we can be a bit more systematic about it and make a grid of interscept and slope values, using a grid search.\n\ngrid &lt;- expand.grid( beta0 = seq(15,20, length = 25),\n                     beta1 = seq(2, 3.5, length = 25))\ngrid$dist &lt;- NA\nfor( i in 1:nrow(grid) ) {\n  grid$dist[i] &lt;- model_distance( grid$beta0[i],\n                                  grid$beta1[i],\n                                  df$x,\n                                  df$y )\n}\n\nggplot( grid, aes(x = beta0, \n                  y = beta1,\n                  color = -dist)) + \n  geom_point( data = filter( grid, rank(dist) &lt;= 10), \n              color = \"red\",\n              size = 4) +\n  geom_point()\n\n\n\n\nYou could imagine that we could iteratively soom in this grid and find the best fit combination of \\(\\beta_0\\) and \\(\\beta_1\\) values until we converged on a really well fit set.\nThere is a more direct way to get to these results (though is much less pretty to look at) using the lm() linear models function."
  },
  {
    "objectID": "regression.html#our-friend-lm",
    "href": "regression.html#our-friend-lm",
    "title": "17  Regression",
    "section": "17.2 Our Friend lm()",
    "text": "17.2 Our Friend lm()\nTo specify a potential model, we need to get the function the form we are interested in using.\n\nfit &lt;- lm( y ~ x, data = df )\nfit\n\n\nCall:\nlm(formula = y ~ x, data = df)\n\nCoefficients:\n(Intercept)            x  \n     17.280        2.625  \n\n\nWe can see that for the values of the coefficients (labeled Interscept and x), it has a model_distance() of\n\nmodel_distance( -1.76, 3.385, df$x, df$y )\n\n[1] 15.90948\n\n\nwhich we can see is pretty close in terms of the coefficients and has a smaller model distance than those examined in the grid.\n\ngrid %&gt;%\n  arrange( dist ) %&gt;%\n  head( n = 1) \n\n     beta0 beta1     dist\n1 17.29167 2.625 5.240071\n\n\nFortunately, we have a lot of additional information available to us because we used the lm() function."
  },
  {
    "objectID": "regression.html#model-fit",
    "href": "regression.html#model-fit",
    "title": "17  Regression",
    "section": "17.3 Model Fit",
    "text": "17.3 Model Fit\nWe can estimate a bunch of different models but before we look to see if it well behaved. There are several interesting plots that we can examine from the model object such as:\n\nplot( fit, which = 1 )\n\n\n\n\n\nplot( fit, which = 2 )\n\n\n\n\n\nplot( fit, which = 5 )"
  },
  {
    "objectID": "regression.html#analysis-of-variance-tables---decomposing-variation",
    "href": "regression.html#analysis-of-variance-tables---decomposing-variation",
    "title": "17  Regression",
    "section": "17.4 Analysis of Variance Tables - Decomposing Variation",
    "text": "17.4 Analysis of Variance Tables - Decomposing Variation\nThus far, we’ve been able to estiamte a model, but is it one that explains a significant amount of variation? To determine this, we use the analysis of variance table.\n\nanova( fit )\n\nAnalysis of Variance Table\n\nResponse: y\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx          1 568.67  568.67  16.568 0.003581 **\nResiduals  8 274.58   34.32                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe terms in this table are:\n\nDegrees of Freedom (df): representing 1 degree of freedom for the model, and N-1 for the residuals.\nSums of Squared Deviations:\n\n\\(SS_{Total} = \\sum_{i=1}^N (y_i - \\bar{y})^2\\)\n\\(SS_{Model} = \\sum_{i=1}^N (\\hat{y}_i - \\bar{y})^2\\), and\n\\(SS_{Residual} = SS_{Total} - SS_{Model}\\)\n\nMean Squares (Standardization of the Sums of Squares for the degrees of freedom)\n\n\\(MS_{Model} = \\frac{SS_{Model}}{df_{Model}}\\)\n\\(MS_{Residual} = \\frac{SS_{Residual}}{df_{Residual}}\\)\n\nThe \\(F\\)-statistic is from a known distribution and is defined by the ratio of Mean Squared values.\nPr(&gt;F) is the probability associated the value of the \\(F\\)-statistic and is dependent upon the degrees of freedom for the model and residuals."
  },
  {
    "objectID": "regression.html#variance-explained",
    "href": "regression.html#variance-explained",
    "title": "17  Regression",
    "section": "17.5 Variance Explained",
    "text": "17.5 Variance Explained\nThere is a correlative measurement in regression models to the Pearson Product Moment Coefficient, (\\(\\rho\\)) in a statistic called \\(R^2\\). This parameter tells you, How much of the observed variation in y is explained by the model?\nThe equation for R^2 is:\n\\[\nR^2 = \\frac{SS_{Model}}{SS_{Total}}\n\\]\nThe value of this parameter is bound by 0 (the model explains no variation) and 1.0 (the model explains all the variation in the data). We can get to this and a few other parameters in the regression model by taking its summary.\n\nsummary( fit )\n\n\nCall:\nlm(formula = y ~ x, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.9836 -4.0182 -0.8709  5.3064  6.9909 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   17.280      4.002   4.318  0.00255 **\nx              2.626      0.645   4.070  0.00358 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.859 on 8 degrees of freedom\nMultiple R-squared:  0.6744,    Adjusted R-squared:  0.6337 \nF-statistic: 16.57 on 1 and 8 DF,  p-value: 0.003581\n\n\nJust like the model itself, the summary.lm object also has all these data contained within it in case you need to access them in textual format or to annotate graphical output.\n\nnames( summary( fit ) )\n\n [1] \"call\"          \"terms\"         \"residuals\"     \"coefficients\" \n [5] \"aliased\"       \"sigma\"         \"df\"            \"r.squared\"    \n [9] \"adj.r.squared\" \"fstatistic\"    \"cov.unscaled\" \n\n\nNotice that the p-value is not in this list… It is estimable from the fstatistic and df values and here is a quick function that returns the raw p-value by looking up the are under the curve equal to or greater than the observed fstatistic with those degrees of freedom.\n\nget_pval &lt;- function( model ) {\n  f &lt;- summary( model )$fstatistic[1]\n  df1 &lt;- summary( model )$fstatistic[2]\n  df2 &lt;- summary( model )$fstatistic[3]\n  p &lt;- as.numeric( 1.0 - pf( f, df1, df2 ) )\n  return( p  )\n}\n\nget_pval( fit )\n\n[1] 0.0035813\n\n\nAs an often-overlooked side effect, the \\(R^2\\) from a simple one predictor regression model and the correlation coefficient \\(r\\) from cor.test(method='pearson') are related as follows:\n\nc( `Regression R^2` = summary( fit )$r.squared,\n   `Squared Correlation` = as.numeric( cor.test( df$x, df$y )$estimate^2 ) )\n\n     Regression R^2 Squared Correlation \n          0.6743782           0.6743782 \n\n\n(e.g., the square of the correlation estimate \\(r\\) is equal to \\(R^2\\))."
  },
  {
    "objectID": "regression.html#extensions-of-the-model",
    "href": "regression.html#extensions-of-the-model",
    "title": "17  Regression",
    "section": "17.6 Extensions of the Model",
    "text": "17.6 Extensions of the Model\nThere are several helper functions for dealing with regression models such as finding the predicted values.\n\npredict( fit ) -&gt; yhat \nyhat \n\n       1        2        3        4        5        6        7        8 \n19.90545 22.53091 25.15636 27.78182 30.40727 33.03273 35.65818 38.28364 \n       9       10 \n40.90909 43.53455 \n\n\nAnd we can plot it as:\n\nplot( yhat ~ df$x, type='l', bty=\"n\", col=\"red\" )\n\n\n\n\nThe residual values (e.g., the distance between the original data on the y-axis and the fitted regression model).\n\nresiduals( fit ) -&gt; resids\nresids \n\n         1          2          3          4          5          6          7 \n-4.4054545  5.5690909 -2.8563636  4.5181818  0.6927273 -6.2327273  6.1418182 \n         8          9         10 \n-7.9836364  6.9909091 -2.4345455 \n\n\nWe almost always need to look at the residuals of a regression model to help diagnose any potential problems (as shown above in the plots of the raw model itself).\n\nplot( resids ~ yhat, bty=\"n\", xlab=\"Predicted Values\", ylab=\"Residuals (yhat - y)\", pch=16 )\nabline(0, 0, lty=2, col=\"red\")"
  },
  {
    "objectID": "regression.html#comparing-models",
    "href": "regression.html#comparing-models",
    "title": "17  Regression",
    "section": "17.7 Comparing Models",
    "text": "17.7 Comparing Models\nOK, so we have a model that appears to suggest that the predicted values in x can explain the variation observed in y. Great. But, is this the best model or only one that is sufficiently meh such that we can reject the null hypothesis. How can we tell?\nThere are two parameters that we have already looked at that may help. These are:\n\nThe P-value: Models with smaller probabilities could be considered more informative.\nThe \\(R^2\\): Models that explain more of the variation may be considered more informative.\n\nLet’s start by looking at some airquality data we have played with previously when working on data.frame objects.\n\nairquality %&gt;%\n  select( -Month, -Day ) -&gt; df.air\nsummary( df.air )\n\n     Ozone           Solar.R           Wind             Temp      \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  \n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  \n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  \n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n NA's   :37       NA's   :7                                       \n\n\nLet’s assume that we are interested in trying to explain the variation in Ozone (the response) by one or more of the other variables as predictors.\n\nfit.solar &lt;- lm( Ozone ~ Solar.R, data = df.air )\nanova( fit.solar )\n\nAnalysis of Variance Table\n\nResponse: Ozone\n           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nSolar.R     1  14780 14779.7  15.053 0.0001793 ***\nResiduals 109 107022   981.9                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLet’s look at all the predictors and take a look at both the p-value and R-squared.\n\nfit.temp &lt;- lm( Ozone ~ Temp, data = df.air )\nfit.wind &lt;- lm( Ozone ~ Wind, data = df.air )\n\ndata.frame( Model = c( \"Ozone ~ Solar\",\n                       \"Ozone ~ Temp\",\n                       \"Ozone ~ Wind\"), \n            R2 = c( summary( fit.solar )$r.squared,\n                    summary( fit.temp )$r.squared,\n                    summary( fit.wind )$r.squared ), \n            P = c( get_pval( fit.solar), \n                   get_pval( fit.temp ),\n                   get_pval( fit.wind ) ) ) -&gt; df.models\n\ndf.models %&gt;%\n  arrange( -R2 ) %&gt;%\n  mutate( P = format( P, scientific=TRUE, digits=3)) %&gt;%\n  kable( caption = \"Model parameters predicting mean ozone in parts per billion mresured in New York during the period of 1 May 2973 - 30 September 2973.\",\n         digits = 3) %&gt;%\n  kable_minimal()\n\n\nModel parameters predicting mean ozone in parts per billion mresured in New York during the period of 1 May 2973 - 30 September 2973.\n\n\nModel\nR2\nP\n\n\n\n\nOzone ~ Temp\n0.488\n0.00e+00\n\n\nOzone ~ Wind\n0.362\n9.27e-13\n\n\nOzone ~ Solar\n0.121\n1.79e-04\n\n\n\n\n\n\n\nSo if we look at these results, we see that in both \\(R^2\\) and \\(P\\), the model with Temp seems to be most explanatory as well as having the lowest probability. But is is significantly better?\nHow about if we start adding more than one variable to the equation so that we now have two variables (multiple regression) with the general model specified as:\n\\[\ny = \\beta_0 + \\beta_1 x_1 + beta_2 x_2 + \\epsilon\n\\]\nNow, we are estimating two regression coefficients and an interscept. For three predictors, this gives us 3 more models.\n\nfit.temp.wind &lt;- lm( Ozone ~ Temp + Wind, data = df.air )\nfit.temp.solar &lt;- lm( Ozone ~ Temp + Solar.R, data = df.air )\nfit.wind.solar &lt;- lm( Ozone ~ Wind + Solar.R, data = df.air )\n\nNow, we can add these output to the table.\n\ndf.models &lt;- rbind( df.models, \n                    data.frame( Model = c( \"Ozone ~ Temp + Wind\",\n                                           \"Ozone ~ Temp + Solar\",\n                                           \"Ozone ~ Wind + Solar\" ),\n                                R2 = c( summary( fit.temp.wind )$r.squared,\n                                        summary( fit.temp.solar )$r.squared,\n                                        summary( fit.wind.solar )$r.squared ),\n                                P = c( get_pval( fit.temp.wind),\n                                       get_pval( fit.temp.solar),\n                                       get_pval( fit.wind.solar) )\n                                ))\ndf.models %&gt;%\n  mutate( P = format( P, scientific=TRUE, digits=3)) %&gt;%\n  kable( caption = \"Model parameters predicting mean ozone in parts per billion mresured in New York during the period of 1 May 2973 - 30 September 2973.\",\n         digits = 3) %&gt;%\n  kable_minimal()\n\n\nModel parameters predicting mean ozone in parts per billion mresured in New York during the period of 1 May 2973 - 30 September 2973.\n\n\nModel\nR2\nP\n\n\n\n\nOzone ~ Solar\n0.121\n1.79e-04\n\n\nOzone ~ Temp\n0.488\n0.00e+00\n\n\nOzone ~ Wind\n0.362\n9.27e-13\n\n\nOzone ~ Temp + Wind\n0.569\n0.00e+00\n\n\nOzone ~ Temp + Solar\n0.510\n0.00e+00\n\n\nOzone ~ Wind + Solar\n0.449\n9.99e-15\n\n\n\n\n\n\n\nHmmmmmm.\nAnd for completeness, let’s just add the model that has all three predictors\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\epsilon\n\\]\n\nfit.all &lt;- lm( Ozone ~ Solar.R + Temp + Wind, data = df.air )\n\nNow let’s add that one\n\ndf.models &lt;- rbind( df.models, \n                    data.frame( Model = c( \"Ozone ~ Temp + Wind + Solar\"),\n                                R2 = c( summary( fit.all )$r.squared ),\n                                P = c( get_pval( fit.all)  )\n                                ))\n\n\ndf.models$P = cell_spec( format( df.models$P, \n                                 digits=3, \n                                 scientific=TRUE), \n                         color = ifelse( df.models$P == min(df.models$P), \n                                         \"red\",\n                                         \"black\"))\ndf.models$R2 = cell_spec( format( df.models$R2, \n                                  digits=3, \n                                  scientific=TRUE), \n                          color = ifelse( df.models$R2 == max( df.models$R2), \n                                          \"green\",\n                                          \"black\"))\n\ndf.models %&gt;%\n  mutate( P = format( P, digits=3, scientific = TRUE) ) %&gt;% \n  kable( caption = \"Model parameters predicting mean ozone in parts per billion mresured in New York during the period of 1 May 2973 - 30 September 2973.  Values in green indicate the model with the largest variance explained and those in red indicate models with the lowest probability.\",\n         escape = FALSE) %&gt;%\n  kable_paper( \"striped\", full_width = FALSE )\n\n\nModel parameters predicting mean ozone in parts per billion mresured in New York during the period of 1 May 2973 - 30 September 2973. Values in green indicate the model with the largest variance explained and those in red indicate models with the lowest probability.\n\n\nModel\nR2\nP\n\n\n\n\nOzone ~ Solar\n1.21e-01\n1.79e-04\n\n\nOzone ~ Temp\n4.88e-01\n0.00e+00\n\n\nOzone ~ Wind\n3.62e-01\n9.27e-13\n\n\nOzone ~ Temp + Wind\n5.69e-01\n0.00e+00\n\n\nOzone ~ Temp + Solar\n5.10e-01\n0.00e+00\n\n\nOzone ~ Wind + Solar\n4.49e-01\n9.99e-15\n\n\nOzone ~ Temp + Wind + Solar\n6.06e-01\n0.00e+00\n\n\n\n\n\n\n\nSo how do we figure out which one is best?\n\n17.7.1 Effects of Adding Parameters\nBefore we can answer this, we should be clear about one thing. We are getting more variance explained by adding more predictor variables. In fact, by adding any variable, whether they are informative or not, one can explain some amount of the Sums of Squares in a model. Taken to the extreme, this means that we could add an infinite number of explanatory variables to a model and explain all the variation there is!\nHere is an example using our small data set. I’m going to make several models, one of which is the original one and the remaining add one more predeictor varible that is made up of a random variables. We will then look at the \\(R^2\\) of each of these models.\n\nrandom.models  &lt;- list()\nrandom.models[[\"Ozone ~ Temp\"]] &lt;- fit.temp\nrandom.models[[\"Ozone ~ Wind\"]] &lt;- fit.wind\nrandom.models[[\"Ozone ~ Solar\"]] &lt;- fit.solar\nrandom.models[[\"Ozone ~ Temp + Wind\"]] &lt;- fit.temp.wind\nrandom.models[[\"Ozone ~ Temp + Solar\"]] &lt;- fit.temp.solar\nrandom.models[[\"Ozone ~ Wind + Solar\"]] &lt;- fit.wind.solar\nrandom.models[[ \"Ozone ~ Temp + Wind + Solar\" ]] &lt;- fit.all\n\ndf.tmp &lt;- df.air\n\nfor( i in 1:8 ) {\n  lbl &lt;- paste(\"Ozone ~ Temp + Wind + Solar + \", i, \" Random Variables\", sep=\"\")\n  df.tmp[[lbl]] &lt;- rnorm( nrow(df.tmp) )\n  random.models[[lbl]] &lt;- lm( Ozone ~ ., data = df.tmp ) \n}\n\ndata.frame( Models = names( random.models ),\n            R2 = sapply( random.models, \n                          FUN = function( x ) return( summary( x )$r.squared), \n                          simplify = TRUE ),\n            P = sapply( random.models, \n                        FUN = get_pval ) ) -&gt; df.random\n\ndf.random %&gt;%\n  kable( caption = \"Fraction of variation explained by original variable as well as models with incrementally more predictor variables made up of randomly derived data.\",\n         digits=4,\n         row.names = FALSE ) %&gt;%\n  kable_paper(\"striped\", full_width = FALSE )\n\n\nFraction of variation explained by original variable as well as models with incrementally more predictor variables made up of randomly derived data.\n\n\nModels\nR2\nP\n\n\n\n\nOzone ~ Temp\n0.4877\n0e+00\n\n\nOzone ~ Wind\n0.3619\n0e+00\n\n\nOzone ~ Solar\n0.1213\n2e-04\n\n\nOzone ~ Temp + Wind\n0.5687\n0e+00\n\n\nOzone ~ Temp + Solar\n0.5103\n0e+00\n\n\nOzone ~ Wind + Solar\n0.4495\n0e+00\n\n\nOzone ~ Temp + Wind + Solar\n0.6059\n0e+00\n\n\nOzone ~ Temp + Wind + Solar + 1 Random Variables\n0.6079\n0e+00\n\n\nOzone ~ Temp + Wind + Solar + 2 Random Variables\n0.6231\n0e+00\n\n\nOzone ~ Temp + Wind + Solar + 3 Random Variables\n0.6252\n0e+00\n\n\nOzone ~ Temp + Wind + Solar + 4 Random Variables\n0.6323\n0e+00\n\n\nOzone ~ Temp + Wind + Solar + 5 Random Variables\n0.6324\n0e+00\n\n\nOzone ~ Temp + Wind + Solar + 6 Random Variables\n0.6324\n0e+00\n\n\nOzone ~ Temp + Wind + Solar + 7 Random Variables\n0.6502\n0e+00\n\n\nOzone ~ Temp + Wind + Solar + 8 Random Variables\n0.6578\n0e+00\n\n\n\n\n\n\n\nSo if we just add random data to a model, we get a better fit!!!! Sounds great. That is easy! I can always get the best fit there is!\nThis is a well-known situation in statistics. An in fact, we must be very careful when we are examining the differences between models and attempting to decide which set of models are actually better than other sets of models."
  },
  {
    "objectID": "regression.html#model-fitting",
    "href": "regression.html#model-fitting",
    "title": "17  Regression",
    "section": "17.8 Model Fitting",
    "text": "17.8 Model Fitting\nTo get around this, we have a few tools at our disposal. The most common approach is to look at the information content in each model relative to the amount of pedictor variables. In essence, we must punish ourselves for adding more predictors so that we do not all run around and add random data to our models. The most common one is called Akaike Information Criterion (AIC), and provide a general framework for comparing several models.\n\\[\nAIC = -2 \\ln L + 2p\n\\]\nWhere \\(L\\) is the log likelihood estimate of the variance and \\(p\\) is the number of parameters. What this does is allow you to evaluate different models with different subsets of parameters. In general, the best model is the one with the smallest value for AIC.\nWe can also evaluate the relative values of all the models by looking in the difference between the “best” model and the rest by taking the difference\n\\[\n\\delta AIC = AIC - min(AIC)\n\\]\nThe prevailing notion is that models that have \\(\\delta AIC &lt; 2.0\\) should be considered as almost equally informative, where as those whose \\(\\delta AIC &gt; 5.0\\) are to be rejected as being informative. That \\(2.0 \\le \\delta AIC \\le 5.0\\) range is where it gets a bit fuzzy.\n\ndf.random$AIC &lt;- sapply( random.models, \n                         FUN = AIC, \n                         simplify = TRUE )\n\ndf.random$deltaAIC = df.random$AIC - min( df.random$A)\n\ndf.random %&gt;%\n  select( -P ) %&gt;%\n  kable( caption = \"Model parameters predicting mean ozone in parts per billion mresured in New York during the period of 1 May 2973 - 30 September 2973 with variance explained, AIC, and ∂AIC for alternative models.\",\n         escape = FALSE,\n         row.names = FALSE, \n         digits = 3) %&gt;%\n  kable_paper( \"striped\", full_width = FALSE )\n\n\nModel parameters predicting mean ozone in parts per billion mresured in New York during the period of 1 May 2973 - 30 September 2973 with variance explained, AIC, and ∂AIC for alternative models.\n\n\nModels\nR2\nAIC\ndeltaAIC\n\n\n\n\nOzone ~ Temp\n0.488\n1067.706\n69.940\n\n\nOzone ~ Wind\n0.362\n1093.187\n95.421\n\n\nOzone ~ Solar\n0.121\n1083.714\n85.948\n\n\nOzone ~ Temp + Wind\n0.569\n1049.741\n51.975\n\n\nOzone ~ Temp + Solar\n0.510\n1020.820\n23.053\n\n\nOzone ~ Wind + Solar\n0.449\n1033.816\n36.049\n\n\nOzone ~ Temp + Wind + Solar\n0.606\n998.717\n0.951\n\n\nOzone ~ Temp + Wind + Solar + 1 Random Variables\n0.608\n1000.141\n2.374\n\n\nOzone ~ Temp + Wind + Solar + 2 Random Variables\n0.623\n997.766\n0.000\n\n\nOzone ~ Temp + Wind + Solar + 3 Random Variables\n0.625\n999.136\n1.370\n\n\nOzone ~ Temp + Wind + Solar + 4 Random Variables\n0.632\n999.029\n1.263\n\n\nOzone ~ Temp + Wind + Solar + 5 Random Variables\n0.632\n1001.000\n3.233\n\n\nOzone ~ Temp + Wind + Solar + 6 Random Variables\n0.632\n1002.998\n5.232\n\n\nOzone ~ Temp + Wind + Solar + 7 Random Variables\n0.650\n999.475\n1.709\n\n\nOzone ~ Temp + Wind + Solar + 8 Random Variables\n0.658\n999.052\n1.286\n\n\n\n\n\n\n\nSo as we look at the data here, we see that the best fit model is the full model though others may be considered as informative and this is where we need to look at the biological importance of variables added to the models."
  },
  {
    "objectID": "anova.html#one-sample-hypotheses",
    "href": "anova.html#one-sample-hypotheses",
    "title": "18  Analysis of Variance",
    "section": "18.1 One Sample Hypotheses",
    "text": "18.1 One Sample Hypotheses\nAt the most basic level, we can take a set of data and test to see if the mean of those values are equated to some particular value, \\(H_O: \\mu = x\\) (or \\(H_O: \\mu = 0\\) in some cases). The idea here is to determine, by specifying a value for the null hypothesis, what we expect the mean value to be equal to. Going back to our idea of hypothesis testing, the null hypothesis is the thing we are trying to disprove (with some level of statistical confidence) and in doing so we need to define a test statistic that we have an idea about its behavior. In this case, we will define Student’s \\(t\\)-test statistic as:\n\\(t =\\frac{\\bar{x}-\\mu}{s_{\\bar{x}}}\\)\nwhere \\(\\bar{x}\\) is the observed mean of the data, \\(\\mu\\) is the mean value specified under the null hypothesis, and \\(s_{\\bar{x}}\\) is the standard deviation of the data. The value of the \\(t\\)-statistic can be defined based upon the sample size (e.g., the degrees of freedom, \\(df\\)). Here is what the probability density function looks like for \\(df = (1,3,\\infty)\\).\n\nlibrary( ggplot2 )\nx &lt;- seq(-5,5,by=0.02)\nd &lt;- data.frame( t=c(x,x,x),\n                  f=c(dt(x,df=1),\n                      dt(x,df=3),\n                      dt(x,df=Inf)),\n                  df=rep(c(\"1\",\"3\",\"Inf\"),each=length(x)))\nggplot( d, aes(x=t,y=f,color=df)) + geom_line() \n\n\n\n\n\n\n\n\nWhen \\(df=\\infty\\) then \\(PDF(t) = Normal\\). As such, we do not need to make corrections to understand the area under the curve, we can just use the normal probability density function. In fact, when \\(df=\\infty\\) then \\(t_{\\alpha,\\infty} = Z_{\\alpha} = \\sqrt{\\chi^2_{\\alpha,df=1}}\\)! The take home message here is that all your statistics become much easier when \\(N=\\infty\\), so go collect some more data!\nFor \\(df &lt; \\infty\\) (all the cases we will be dealing with), we will use the approximation defined by the \\(t\\) distribution. If you look at the distributions above, you see that as we increase the number of samples (e.g., as \\(df\\) increases), the distribution becomes more restricted. The actual function is defined (where \\(df = v\\) for simplicity in nomenclature) as:\n\\(P(t|x,v)= \\frac{ \\Gamma\\left( \\frac{v+1}{2}\\right)}{\\sqrt{v\\pi}\\Gamma\\left( \\frac{v}{2}\\right)} \\left( 1 + \\frac{x^2}{v}\\right)^{-\\frac{v+1}{2}}\\)\nwhere \\(\\Gamma\\) is the Gamma function. Not pretty! Fortunately, we have some built-in facilities in R that can make it easy for us.\nFor a single set of data, we can use the function above to estimate a value of the \\(t\\) statistic. The probability distribution, defined by the degrees of freedom, identifies regions within which we may suspect the statistic to be abnormally large. In our case, though it is quite arbitrary, we can define either one or two regions of the distribution whose values would be extreme enough such that we would consider a significant deviation. For a two-tailed test, the distribution below illustrates this concept. If the estimated value of the \\(t\\) statistic is in either of the shaded regions, we would reject the null hypothesis of \\(H_O: \\mu = 0\\) where \\(\\alpha=0.05\\).\n\nd1 &lt;- data.frame(t=c( seq(-5,-2.064, by=0.02), -2.064, -5), \n                 f=c( dt( seq(-5,-2.064, by=0.02),df=1), 0.01224269, 0.01224269))\nd2 &lt;- data.frame(t=c( seq(2.064,5,by=0.02), 5, 2.064),\n                 f=c( dt( seq( 2.064, 5, by=0.02),df=1), 0.01224269, 0.01224269))\nd3 &lt;- data.frame( x=c(2.5,-2.5), y=0.02719, label=\"2.5%\")\nggplot() + \n  geom_polygon(aes(t,f),data=d1, fill=\"#F8766D\",alpha=0.5,color=\"#F8766D\") + \n  geom_polygon(aes(t,f),data=d2, fill=\"#F8766D\",alpha=0.5,color=\"#F8766D\") + \n  geom_line( aes(t,f),data=d[d$df==1,], color=\"#F8766D\") + \n  geom_text( aes(x,y,label=label),data=d3)\n\n\n\n\n\n\n\n\nIn R, we can use the t.test() function. I’m going to go back to the Iris data set and use that as it has three categories (the species) and many measurements on sepals and pedals. Here I separate the species into their own data.frame objects.\n\ndf.se &lt;- iris[ iris$Species == \"setosa\",] \ndf.ve &lt;- iris[ iris$Species == \"versicolor\",] \ndf.vi &lt;- iris[ iris$Species == \"virginica\",]\n\nLets look at the Sepal.Length feature in these species and create some hypotheses about it.\n\nggplot( iris, aes(x=Sepal.Length, fill=Species)) + geom_density(alpha=0.75)\n\n\n\n\n\n\n\n\nWe could test the hypothesis, \\(H_O: mean(Sepal.Length)=6\\) for each of the species.\n\nfit.se &lt;- t.test(df.se$Sepal.Length, mu = 6.0)\nfit.se\n\n\n    One Sample t-test\n\ndata:  df.se$Sepal.Length\nt = -19.94, df = 49, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 6\n95 percent confidence interval:\n 4.905824 5.106176\nsample estimates:\nmean of x \n    5.006 \n\n\nFrom the output, it appears that we can reject that null hypothesis (\\(t =\\) -19.9; \\(df =\\) 49; \\(P =\\) 3.7e-25).\nFor I. versicolor, we see that the mean does appear to be equal to 6.0 (and thus fail to reject the null hypothesis):\n\nt.test( df.ve$Sepal.Length, mu=6.0 )\n\n\n    One Sample t-test\n\ndata:  df.ve$Sepal.Length\nt = -0.87674, df = 49, p-value = 0.3849\nalternative hypothesis: true mean is not equal to 6\n95 percent confidence interval:\n 5.789306 6.082694\nsample estimates:\nmean of x \n    5.936 \n\n\nand for I. virginica, we find that it is significantly larger than 6.0 and again reject the null hypothesis:\n\nt.test( df.vi$Sepal.Length, mu=6.0 )\n\n\n    One Sample t-test\n\ndata:  df.vi$Sepal.Length\nt = 6.5386, df = 49, p-value = 3.441e-08\nalternative hypothesis: true mean is not equal to 6\n95 percent confidence interval:\n 6.407285 6.768715\nsample estimates:\nmean of x \n    6.588 \n\n\nIn all the output, we are also given an estimate of the Confidence Interval around the mean. This confidence interval is determined as:\n\\(\\bar{x} - t_{\\alpha, df} s_{\\bar{x}} &lt; \\mu &lt; \\bar{x} + t_{\\alpha, df} s_{\\bar{x}}\\)\nor the mean plus or minus standard deviation of the data times the value of the \\(t\\)-statistic for a given level of \\(\\alpha\\) and \\(df\\).\n\n18.1.1 Data Variability\nThere are times when reporting some confidence around a parameter is important, particularly when using tabular data as output.\n\nSpecies &lt;- c(\"Iris setosa\",\"Iris versicolor\",\"Iris virginia\")\nSepal.Length &lt;- c(mean(df.se$Sepal.Length), mean(df.ve$Sepal.Length), mean( df.vi$Sepal.Length))\nSepal.Length.SE &lt;- c(sd(df.se$Sepal.Length), sd(df.ve$Sepal.Length), sd( df.vi$Sepal.Length))\nSepal.Length.SEM &lt;- Sepal.Length.SE / sqrt(50)\n\nThere are two ways we can talk about the data and it is important for you to think about what you are trying to communicate to your readers. These alternatives include:\n\nsd &lt;- paste( format(Sepal.Length,digits=2), \"+/-\", format(Sepal.Length.SE, digits=3))\nse &lt;- paste( format(Sepal.Length,digits=2), \"+/-\", format(Sepal.Length.SEM, digits=3))\ndf &lt;- data.frame( Species, sd, se )\nnames(df) &lt;- c(\"Species\",\"Mean +/- SD\", \"Sepal Length +/- SE\")\nknitr::kable(df,row.names = FALSE,digits = 3,align = \"lcc\")\n\n\n\n\nSpecies\nMean +/- SD\nSepal Length +/- SE\n\n\n\n\nIris setosa\n5.0 +/- 0.352\n5.0 +/- 0.0498\n\n\nIris versicolor\n5.9 +/- 0.516\n5.9 +/- 0.0730\n\n\nIris virginia\n6.6 +/- 0.636\n6.6 +/- 0.0899\n\n\n\n\n\nThe two columns of data tell us something different. The middle column tells us the mean and the standard deviation of the data. This tells us about the variability (and confidence) of the data itself. The last column is the Standard Error of the Mean (\\(\\frac{s}{\\sqrt{N}}\\)) and gives us an idea of the confidence we have about the mean estimate of the data (as opposed to the variation of the data itself). These are two different statements about the data and you need to make sure you are confident about which way you want to use to communicate to your audience."
  },
  {
    "objectID": "anova.html#two-sample-hypotheses",
    "href": "anova.html#two-sample-hypotheses",
    "title": "18  Analysis of Variance",
    "section": "18.2 Two Sample Hypotheses",
    "text": "18.2 Two Sample Hypotheses\nIn addition to a single sample test, evaluating if the mean of a set of data is equal to some specified value, we can test the equality of two different samples. It may be the case that the average sepal length for I. versicolor is not significantly different than 6.0 whereas I. virginia is. However, this does not mean that the mean of both of these species are significantly different from each other. This is a two-sampled hypothesis, stating that \\(H_O: \\mu_X = \\mu_Y\\).\nVisually, these data look like:\n\ndf &lt;- iris[ (iris$Species %in% c(\"versicolor\",\"virginica\")),]\nggplot( df, aes(x=Species, y=Sepal.Length)) + geom_boxplot(notch=TRUE)\n\n\n\n\n\n\n\n\nwhich clearly overlap in their distributions but are the mean values different? This sets up the null hypothesis:\n\\(H_O: \\mu_1 - \\mu_2 = 0\\)\nUnder this hypothesis, we can use a t-test like before but just rearranged as:\n\\(t = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_{\\bar{x}_1-\\bar{x}_2}}\\)\nAs before, if the difference in the numerator is small we would reject but here we need to standardize the differences in the means by a measure of the standard deviation that is based upon both sets of data. This is called a the standard error of the difference in two means (real catchy title, no?). This is defined as:\n\\(s_{\\bar{x}_1-\\bar{x}_2} = \\sqrt{ \\frac{s_1^2}{N_1}+\\frac{s_2^2}{N}}\\)\nTo test this, we use the same approach as before but instead of defining \\(\\mu = 6.0\\) in the t.test() function, we instead give it both data sets.\n\nt.test( x=df.vi$Sepal.Length, y = df.ve$Sepal.Length )\n\n\n    Welch Two Sample t-test\n\ndata:  df.vi$Sepal.Length and df.ve$Sepal.Length\nt = 5.6292, df = 94.025, p-value = 1.866e-07\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 0.4220269 0.8819731\nsample estimates:\nmean of x mean of y \n    6.588     5.936 \n\n\nHere we get a few bits of new information from the analysis. It is obvious that we would reject the null hypothesis given the magnitude of the estimated \\(P\\)-value. The output also provides us an estimate of the mean values for each group as well as the confidence around the difference in the mean values. This confidence interval does not overlap 0.0, as it shouldn’t if we reject \\(H_O: \\mu_X = \\mu_Y\\)."
  },
  {
    "objectID": "anova.html#many-sample-hypotheses",
    "href": "anova.html#many-sample-hypotheses",
    "title": "18  Analysis of Variance",
    "section": "18.3 Many Sample Hypotheses",
    "text": "18.3 Many Sample Hypotheses\nIf we have more than two samples, we could do a bunch of paired \\(t\\)-test statistics but this is not the best idea. In fact, if we do this to our data, each time testing at a confidence level of, say, \\(\\alpha = 0.05\\), then for each time we test at \\(0.05\\) but over all pairs, we test at an overall level of \\(0.05^k\\) (where \\(k\\) is the number of tests) value. We cannot do multiple tests without penalizing ourselves in terms of the level at which we consider something significant if we are going to do all these tests. You may have heard about a Bonferroni correction—this does exactly that, it allows us to modify the \\(\\alpha\\) level we use to take into consideration the number of tests we are going to use. While this may be an acceptable way to test for the equality of several means (and it may not actually be if you ask most statisticians), there is another way that is much easier.\nConsider the case where we have many categories (e.g., factors in R) that we are interested in determining if the mean of all are equal. The null hypothesis for this is, \\(H_O: \\mu_1 = \\mu_2 = \\ldots = \\mu_k\\), where there are \\(k\\) different treatment levels. This is essentially what we’d want to do by doing a bunch of \\(t\\)-tests but we can use another approach that we don’t have to penalize ourselves for multiple tests. Here is how it works.\nIn the Iris data, we can visualize the means and variation around them by using box plots. Here is an example.\n\nggplot( iris, aes(x=Species, y=Sepal.Length)) + \n  geom_boxplot(notch = TRUE) + \n  ylab(\"Sepal Length\")\n\n\n\n\n\n\n\n\nFor us to tell if there are statistical differences among the species, we need to look at both the location of the mean values as well as the variation around them. We do this by partitioning the variation in all the data into the components within each treatment (species) and among each treatment (species) using an approach derived from the sum of squared deviations (or Sums of Squares). Formally, we can estimate the sum of squares within each of the \\(K\\) groupings as:\n\\(SS_{Within} = \\sum_{i=1}^K\\left( \\sum_{j=1}^{N_i}(x_{ij}-\\bar{x}_i)^2 \\right)\\)\nwhose degrees of freedom are defined as:\n\\(df_{W} = \\sum_{i=1}^K \\left( N_i - 1 \\right) = N-K\\)\nThese parameters represent the deviation among samples within groups and the number of independent samples within these groups. We also need to partition out the variation among groups as a similarly defined Sums of Squares:\n\\(SS_{Among} = \\sum_{i=1}^K N_i\\left( \\bar{x}_i - \\bar{x} \\right)^2\\)\nor the deviation among the mean of each treatment compared to the overall mean of all the data. This parameter has degrees of freedom equal to\n\\(df_{A} = K - 1\\)\nThese two parameters describe all the data and as such \\(SS_{Total} = SS_{Within} + SS_{Among}\\). Formally, we see that\n\\(SS_{Total} = \\sum_{i=1}^K\\sum_{j=1}^{N_i} (x_{ij} - \\bar{x})^2\\)\nwhose degrees of freedom are\n\\(df_{T} = N - 1\\)\nFor each of these Sums of Squared deviations, we can standardize them using the degrees of freedom. The notion here is that with more samples, and more treatments, we will have greater \\(SS\\) values. However, if we standardize these parameters by the \\(df\\), we can come up with a standardized Mean Squared values (simplified as \\(MS = \\frac{SS}{df}\\) for each level).\nIf we look at all these values, we can create the venerable ANOVA table with Among, Within, and Total partitions of the variation.\n\n\n\n\n\n\n\n\n\nSource\ndf\nSS\nMS\n\n\n\n\nAmong\n\\(K-1\\)\n\\(\\sum_{i=1}^K N_i \\left( \\bar{x}_i - \\bar{x} \\right)^2\\)\n\\(\\frac{SS_A}{K-1}\\)\n\n\nWithin\n\\(N-K\\)\n\\(\\sum_{i=1}^Kn_i\\left( \\sum_{j=1}^{N_i}(x_{ij}-\\bar{x}_i)^2 \\right)\\)\n\\(\\frac{SS_W}{N-K}\\)\n\n\nTotal\n\\(N-1\\)\n\\(\\sum_{i=1}^K \\sum_{j=1}^{N_i} (x_{ij} - \\bar{x})^2\\)\n\n\n\n\nIn R, we can evaluate the equality of means by partitioning our data as depicted above. Essentially, if at least one of our treatments means deviate significantly, then the \\(MS_A\\) will be abnormally large relative to the variation within each treatment \\(MS_W\\). This gives us a statistic, defined by the American statistician Snedekor as:\n\\(F = \\frac{MS_A}{MS_W}\\)\nas an homage to Ronald Fisher (the F-statistic) has a pretty well understood distribution under a few conditions. This statistic has an expectation of:\n\\(f(x | df_A, df_W) = \\frac{\\sqrt{\\frac{(df_Ax)^{df_A}df_W^{df_W}}{(df_Ax + df_W)^{df_W+df_A}}}}{x\\mathbf{B}\\left( \\frac{df_A}{2}, \\frac{df_W}{2} \\right)}\\)\nwhich is even more of a mess than that for the \\(t\\)-test! Luckily, we have a bit of code to do this for us.\nHere is an example using the Iris data. Here we test the hypothesis that the Sepal Lengths are all the same (e.g., \\(H_O: \\mu_{se} = \\mu_{ve} = \\mu_{vi}\\))\n\nfit.aov &lt;- aov( Sepal.Length ~ Species, data=iris)\nfit.aov\n\nCall:\n   aov(formula = Sepal.Length ~ Species, data = iris)\n\nTerms:\n                 Species Residuals\nSum of Squares  63.21213  38.95620\nDeg. of Freedom        2       147\n\nResidual standard error: 0.5147894\nEstimated effects may be unbalanced\n\n\nThe function called here, aov() is the one that does the Analysis of Variance. It returns an object that has the necessary data we need. To estimate the ANOVA table as outlined above we ask for it as:\n\nanova(fit.aov)\n\nAnalysis of Variance Table\n\nResponse: Sepal.Length\n           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nSpecies     2 63.212  31.606  119.26 &lt; 2.2e-16 ***\nResiduals 147 38.956   0.265                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nwhich shows that the “Species” treatment are significantly different from each other, with an \\(F\\) statistic equal to \\(F = 119.3\\), which with 2 and 147 degrees of freedom is assigned a probability equal to \\(2e^{-16}\\), a very small value!\n\n18.3.1 Post-Hoc Tests\nWhat this analysis tells us is that at least one of the treatment means are different from the rest. What it does not tell us is which one or which subset. It could be that I. setosa is significantly smaller than both I. versitosa and I. virginia. It could be that I. virginia is significantly larger than the others, who are not different. It could also mean that they are all different. To address this, we can estimate a post hoc test, to evaluate the difference between treatment means within this model itself.\nOne of the most common ways to evaluate the equality of treatment mean values is that defined by Tukey. The so-called “Honest Significant Differences” post hoc test is given by\n\ntuk &lt;- TukeyHSD(fit.aov)\ntuk\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Sepal.Length ~ Species, data = iris)\n\n$Species\n                      diff       lwr       upr p adj\nversicolor-setosa    0.930 0.6862273 1.1737727     0\nvirginica-setosa     1.582 1.3382273 1.8257727     0\nvirginica-versicolor 0.652 0.4082273 0.8957727     0\n\n\nwhich breaks down the pair-wise differences in the mean of each treatment. Here we see the magnitude of the differences in mean values, the lower and upper confidence on the differences, and the probability associated with these differences. In this example, all three comparisons are highly unlikely (e.g., \\(P\\) is very small and in this case essentially zero). As a result, we can interpret these results as suggesting that each of the three species have significantly different. If we plot these results, we see which ones are larger and which are smaller.\n\nplot( tuk )\n\n\n\n\n\n\n\n\nWhich shows the difference between treatment mean between all pairs of treatments. Overall, we see that the Iris species are all significantly different."
  }
]